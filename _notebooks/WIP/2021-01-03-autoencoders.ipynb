{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbasecondab8a2a0b1ed86479d9f777b0a8cf633d1",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# (simple) Autoencoders â‰¡ PCA\n",
    "> and how Variational Autoencoders (VAEs) perform expectation maximization (EM).\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Background\n",
    "\n",
    "Shocker: The simplest autoencoder is actually the same as PCA. We will also see how VAEs perform expectation maximization. Autoencoders learn a meaningful representation of a signal. Using an encoder/decoder pair, autoencoders work by reconstructing a latent encoded representation of an original signal. By minimizing the loss between the original signal and the decoded latent representation, an encoder network can be trained to parse an instance of a dataset for its most meaningful features.\n",
    "\n",
    "Sound familiar? That is precisely the goal of computing the principal components analysis (PCA) of a matrix. It turns out that autoencoders, by construction are *exactly* PCA."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# The Simplest Autoencoder\n",
    "\n",
    "\n",
    "Let a neural network be defined with a single hidden unit $W^T\\sigma(f(W\\vec{X}))$, with a linear activation function $\\sigma$ (for this example). Let the weights on the encoder layer be denoted $W$.\n",
    "\n",
    "We thus define a decoder to use weights $W^T$, and final outputs of the network should converge to a reconstruction of the original features $\\vec{X}$ once properly trained. That is, our network should produce $\\hat{\\vec{X}}$ from $\\vec{X} \\to \\vec{y} = WX \\to z = \\sigma(f(y))$ and finally $\\hat{\\vec{X}} = W^Tz$.\n",
    "\n",
    "If we train by minimizing the $L_2$ divergence between $(\\vec{X},\\hat{\\vec{X}})$, we have an autoencoder, but we also learn the principle components of $\\vec{X}$:\n",
    "\n",
    "$$\\hat{x} = w^Twx;\\ div(\\hat{x},x) = \\|x-\\hat{x}\\|^2 = \\|x - w^TWx\\|^2 $$\n",
    "$$\\to via\\ backprop \\to \\hat{w} = \\arg\\min_w E\\left[ \\| x-w^TWx\\|^2 \\right]$$\n",
    "\n",
    "This is equivalent to discovering a basis that spans the principle components of the input, as we discover the directions of maximum energy, equal to the variance of $X$ if $X$ is a zero-mean random variable. In other words, we find a linear mapping of the original features to the principle axis of the data.\n",
    "\n",
    "Finally, if the learned decoder weight is *not* the same weight as the input weight (i.e., $U^T \\not = W^T$),  we still learn a hidden representation $z$ that lies along the major axis of the input $X$. The minimum error direction here is by definition the pinciple eigen vector of $X$.\n",
    "\n",
    "We could then find a useful component of $X$ (described perhaps by our training process or assumptions)by then projecting the eigen vector onto $X$. Again, if $U^T = W^T$, we arrive at the principle component(s) of $X$.\n",
    "\n",
    "Of course, this is a roundabout method of obtaining principle components, but I hope it shows the rigorous grounding and versatility of perceptrons in learning representations of data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}