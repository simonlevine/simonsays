{
  
    
        "post0": {
            "title": "Magic in Numpy",
            "content": "Numpy Basics . We will now go over some basic approaches on a seemingly simple matrix for illustrative purposes. Hopefully some of the efficient and useful properties of Numpy become apparent. . A favorite work by a favorite artist, Dürer&#39;s Melencolia I (1514) includes sophisticated use of mathematical allegory, particularly in the top-right corner. It turns out Dürer actually makes many interesting points through his art, something you wouldn&#39;t expect from his messiah complex. . . . The matrix is thus: . import numpy as np X = np.array([[16, 3, 2, 13], [5, 10, 11, 8], [9, 6, 7, 12], [4, 15, 14, 1]]) print(X) . [[16 3 2 13] [ 5 10 11 8] [ 9 6 7 12] [ 4 15 14 1]] . type(X) . numpy.ndarray . Magic Squares . This matrix is purported to be a magic square. We must fit the following constraints: . 1) Magic 2) Square . Simple enough. Starting with the second condition, Numpy provides a number of methods. Though magic cubes and tesseracts are possible, we can begin with a square. Here&#39;s a simple function to detect if an array is square. . def is_square(M: np.ndarray) -&gt; bool: &#39;&#39;&#39; Arguments: M, a 2-d matrix Returns: a boolean, True if square &#39;&#39;&#39; assert M.ndim == 2 return True if M.shape[0] == M.shape[1] else False is_square(X) . True . Vectorized Summation: Magic . Now, the more involved condition. If a square is &quot;magic&quot;, the array exhibits the following properties1: . i) Each of the $n$ elements are of the set of distinct positive integers $[1,2,3,...,n^2]$, such that $n$ is the &quot;order&quot; of the square. Dürer thus presents a $4^{th}$ order magic square. . ii) The sum of the $n$ numbers about any horizontal, vertical or main diagnonal is the same number – the &quot;magic&quot; constant. It is known that such magic constants can be given by $ mathcal{M}(X_n) = frac{1}{n} sum_{k=1}^{n^2}k = frac{1}{2}n(n^2+1)$ . . Aside: iii) The complement to a magic square is derived from subtracting every number in a given magic square by $n^2 + 1$. . Back to Dürer. We can check each condition as follows. . . there are others, see https://faculty.etsu.edu/stephen/matrix-magic-squares.pdf)&#8617; . | def is_magic(M, verbose = True)-&gt;bool: #By constraints i) &amp; ii) assert M.shape[0] == M.shape[1], &#39;Not a square.&#39; n = M.shape[0] assert np.array_equal(np.sort(M.flatten()), np.arange(n**2) + 1), &#39;Expected elements from [1,2,...,n^2]&#39; column_sums = np.sum(M,axis=0) #Note that summing across axis 0 actually returns column-wise sums, and vice-versa. row_sums = np.sum(M, axis=1) diagonal_sums = np.array([np.trace(M),np.trace(np.fliplr(M))]).astype(int) magic_num_sum = np.unique(np.concatenate( (column_sums,row_sums,diagonal_sums) )) if len(magic_num_sum) == 1: if verbose: print(f&#39;Magic number is {magic_num_sum} with order {n}.&#39;) return True . np.fliplr(X).diagonal().sum() == np.flipud(X).diagonal().sum() . True . np.trace(X) == np.diagonal(X).sum() . True . is_magic(X) . Magic number is [34] with order 4. . True . X . array([[16, 3, 2, 13], [ 5, 10, 11, 8], [ 9, 6, 7, 12], [ 4, 15, 14, 1]]) . Fast Indexing: Gnomon Magic . Dürer&#39;s square is actually a Gnomonic Magic Square – that is, each non-overlapping root subsquare bordering the four sides of the square ($2 times 2$ subsquare), as well as the central subsquare, sums to the magic constant of the overarching square. . The Gnomon is the portion of the sundial casting a shadow. In a way we also cast a magic projection on subarrays of our main magic square. . We can verify this easily – in Numpy, arrays can be efficiently split with minimal logic, rather than looping over each element and hard-indexing. . a,b,c,d = [quadrant for sub_x in np.split(X,2, axis = 0) for quadrant in np.split(sub_x,2, axis = 1)] n = X.shape[0] n_subsquare = np.sqrt(n).astype(int) start = n//2 - (n_subsquare // 2) end = n//2 + (n_subsquare // 2) e = X[start:end,start:end] sections = [a, b, c, d, e] sections . [array([[16, 3], [ 5, 10]]), array([[ 2, 13], [11, 8]]), array([[ 9, 6], [ 4, 15]]), array([[ 7, 12], [14, 1]]), array([[10, 11], [ 6, 7]])] . print(set([sum(s.flatten()) for s in sections])) . {34} . All quadrants sum to the magic number of 34. As such, we have verified the deliberate style of Dürer. . Linear Algebra . We will now move onto some essential linear algebra operations on the magic square. . X . array([[16, 3, 2, 13], [ 5, 10, 11, 8], [ 9, 6, 7, 12], [ 4, 15, 14, 1]]) . Rank . The rank of a matrix is the number of its linearly independent columns. That is, the dimensionality of the vector space spanned by a matrix&#39;s columns (or rows) is given by its rank, such that the span is the smallest linear subspace containing a set of vectors describing the matrix. . We can obtain the span of all linear combinations of some vectors $ vec{u}, vec{v}$ by computing $s vec{u} + t vec{v}$ for some scalar constants $s,t$. The dimensionality of the span of the row or column vectors of a matrix thus yields the row or column rank. . We will proceed using Numpy, which proceeds using singular value decompositon (SVD): . rank = np.linalg.matrix_rank(X) rank . 3 . Thus we have a rank-deficient matrix, since $3 &lt; 4$. 4 is the column-dimensionality of the Magic Square matrix but the columns only span 3 dimensions. Note that $rank(M) leq min (m,n)$ for an $m times n$ matrix $M$. . Note how Numpy uses the property that the rank is equal to the number of nonzero singular values as follows: . u,s,vh = np.linalg.svd(X) s . array([3.40000000e+01, 1.78885438e+01, 4.47213595e+00, 6.25921042e-16]) . We have 4 nonzero singular values, but the final value is extremely small. Numpy therefore considers this zero as the default tolerance is computed as M.max()*max(M.shape)*eps. . eps = np.finfo(float).eps tol = X.max()*max(X.shape)*eps tol . 1.4210854715202004e-14 . rank == len(s[s&gt;tol]) . True . rank . 3 . Determinant . The determinant is a useful encoding of the linear transformation described by a particular $n times n$ matrix. In geometric terms, it is analagous to the volume scaling factor of the linear transformation described by the matrix. . In other words, this is the volume of the parallelepiped (a rhomboid prism; a cube is to a square as a parallelepiped is to a parallelogram) spanned by the vectors (row or column) of a matrix. The sign of the determinant of a matrix denotes whether or not the orientation of a vector space is preserved by the transformation described by the matrix. . Two simple examples, then Dürer&#39;s: . A = np.array([[0,-1],[1,0]]) B = np.array([[-2,0],[0,2]]) . $A$ describes a 90-degree counterclockwise (↪️) rotation. . $B$ describes a scaling by a factor of $2$ as well as a reflection about the $y$ axis. . print(np.linalg.det(A), np.linalg.det(B), sep=&#39; n&#39;) . 1.0 -4.0 . A simple rotation does not change &quot;volume&quot; nor orientation. A scaling on $x,y$ and a reflection about the $y$ axis is encoded in the determinant, however: the &quot;volume&quot; is changed in total by a factor of $4$ and the sign is negative, indicating a change in the orientation of previous vector space. . These are simple enough to compute by hand, but even a $4 times 4$ dimensional matrix as provided by Dürer is more onerous. Thankfully, Numpy works well: . X = np.array([[16, 3, 2, 13], [5, 10, 11, 8], [9, 6, 7, 12], [4, 15, 14, 1]]) np.linalg.det(X) . 1.449507180950607e-12 . An interesting observation: Dürer does not provide a pandiagonal magic square, as the determinant of this order-4 magic square is near to, but $not$, zero. In other words, if the broken diagonals (for instance, $16,11,12,15$, or $3,8,7,4$) summed to the magic number, the determinant would be zero [^1]. . Eigenvectors and Eigenvalues . We solve the characteristic equation of a matrix $M$ to find eigenvalues $ vec{ lambda}$. That is, we solve $|M - lambda I| = 0$ where $I$ is the identity matrix ($I_{ij} = 1 s.t i=j, 0 s.t. i not = j$) of identical dimensionality to $M$. . In the case of Dürer&#39;s magic square, we simply subtract a value $ lambda$ from each element on the main diagonal and set the resulting matrix&#39;s determinant equal to zero. . We can quickly avoid rote work using Numpy: . eigenvals, eigenvects = np.linalg.eig(X) eigenvects . array([[ 5.00000000e-01, 8.16496581e-01, 2.23606798e-01, -4.08248290e-01], [ 5.00000000e-01, -4.08248290e-01, -6.70820393e-01, 0.00000000e+00], [ 5.00000000e-01, -1.76752662e-16, 6.70820393e-01, -4.08248290e-01], [ 5.00000000e-01, -4.08248290e-01, -2.23606798e-01, 8.16496581e-01]]) . eigenvals . array([ 3.40000000e+01, 8.00000000e+00, 4.84818517e-17, -8.00000000e+00]) . Note the interesting property of magic squares: the principal (largest) eigenvalue of a magic square composed of positive elements is its magic constant! Of further note, but not applicable here, is the observation that if a magic square has some negative elements, then its magic constant is one of its eigenvalues.[^1] . To show the first point, consider that $[1,1,...,1]^T$ is an eigenvector of a matrix $M$ if every row sums to the same value $k$. This can be shown by computing $|M- lambda I| = 0$ for a matrix $M$ where all rows sum to the same constant $k$. Substituting values and simplifying, $k$ is an eigenvalue. The same holds for columns that sum to the same constant. . Now, we want to show that the entries in the vector $Mv$ are equal to $kv$, where $k$ is both the magic number and an eigenvalue, and $v$ is an eigenvector of $M$. Recall that the sum of all elements in an $n times n$ magic square $M^*$ is, by construction, equal to $ frac{n(n^2+1)}{2}$. Thus, since a magic square $M^*$ indeed does have each row sum to $k$, we have that $Av = kv$ for $[1,1,...,1]^T$. . This gives us another way to find the magic number of a magic square. . def get_magic_number(M): if is_magic(M, verbose= False) and is_square(M): return np.linalg.eig(X)[0].round(1).astype(int)[0] get_magic_number(X) . 34 . Conclusion . This concludes discussion (for now) of magic squares, essential Numpy, and some linear-algebraic approaches to simple matrices. From here, I hope to move to much more complex topics involving far more abstract data types and approaches to manipulation. Nonetheless, foundations will always be important and most likely present under the hood. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/04/linear-algebra.html",
            "relUrl": "/jupyter/2021/01/04/linear-algebra.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Tensors via Bioimaging",
            "content": "Background . 1.3 billion humans are able to use Microsoft Excel. Microsoft Excel is the modern clay tablet, an intuitive but ultimately limited instrument for the computational professional. After all, why should we be limited to two dimensions? Why should we pay for the privelege? Often, data is best represented as $n$-dimensional. . For instance, let&#39;s say you&#39;d like become a billionaire without too much effort. One way would be to totally automate clinical bioimage analysis at human-level fidelity using machine learning. . . A Diabetic retinopathy slide (https://www.kaggle.com/c/diabetic-retinopathy-detection). The condition is estimated to effect over 93 million people. . Whereas a highly skilled human could potentially spot abnormalities in the above retinopathy slide, a machine can do it better and much faster. Partially, this is because a machine views the below image somewhat naïvely as 150,528 dots, as we have a square RGB image with with 224 pixels per dimension. . Viewing the image not as an image but as a tensor, we simply vector-valued matrices, or an $n$-dimensional generalization of a matrix, we can then move onto analyzing the image: segmentation, feature learning, classification (if labels are detected), and other interesting tasks that a human may or may not be able to do. . Numpy . We use Numpy for efficient vectorized tensor manipulation with the convenient abstraction of the Python programming language. Numpy fuency will carry a computational professional very far, and it will only begin to show limitations when deep learning and very large datasets are involved (though the syntax of major deep learning packages are very close to Numpy) . We can easily load the above png &quot;image&quot; into a numpy array using a number of packages. imageio is used below. . import imageio import numpy as np my_image = imageio.imread(&#39;10009_right.png&#39;) my_image . Array([[[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], ..., [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]]], dtype=uint8) . Above we see the image loaded into memory as an integer-valued 3-dimensional matrix. Basic Numpy / Python fluency is assumed (slicing, etc.). . my_image.shape #note memory persistence in Jupyter. . (224, 224, 3) . my_image.size . 150528 . my_image.ndim . 3 . The $224 times 224 times 3$, $150,528$-element, $3$-dimensional array can be now be subject to a multitude of useful manipulations. . For instance, we can sparsify the matrix using scipy.sparse for 2-d matrices and sparse (!pip install sparse) for $n$-dimensional arrays (tensors). This may be useful for quick compression or storage of many such images: note the large number of zero values corresponding to &quot;black&quot; portions of the image. . import sparse sparse.COO(my_image) . Formatcoo | . Data Typeuint8 | . Shape(224, 224, 3) | . nnz120885 | . Density0.8030731823979592 | . Read-onlyTrue | . Size2.9M | . Storage ratio20.1 | . The sparse matrix in general is less memory intensive. . Another example of manipulation: we can quickly invert and flip the image with numerical rigor. We will use matplotlib to display the numpy array. . np.invert is np.bitwise_not . True . import matplotlib.pyplot as plt plt.imshow(my_image) plt.axis(&#39;off&#39;) plt.title(&#39;All Axes&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; plt.imshow(np.invert(my_image)) plt.axis(&#39;off&#39;) plt.title(&#39;Inverted (Bitwise NOT; twos-complement)&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Said another way... . 255 - my_image[100,100,:] == np.invert(my_image)[100,100,:] #we are in 256-bit color. . Array([ True, True, True]) . plt.imshow(np.fliplr(my_image)) plt.axis(&#39;off&#39;) plt.title(&#39;Flipped (LR)&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; This leads to some intuitive operations. For instance, if we were looking to feature-engineer such image data for use in a statistical learning classifier, we may hypothesize the location of the fovea (bright central spot) as a useful aspect of the retinopathy image. . I&#39;m betting we can segment these features with a single line of numpy. For this and subsequent examples, let&#39;s take a sample of the first sheet (zeroth index) of the image for simplicity such that we have a 2-d array (pretend we read-in a greyscale image). . x = my_image[:,:,0] #take all elements of the first sheet/leaf of the array. plt.imshow(x) plt.axis(&#39;off&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Now for the single line. We will find the bright portion by the assumption that we can find it by taking $region = frac{ sum_i x_i}{N} * 2 * sigma( vec{x})$, or all pixels with intensity greater than equal to two standard deviations above the mean. . x = my_image[:,:,0] #take all elements of the first sheet/leaf of the array. mask = (x &gt;= x.mean() + 2*x.std()) . plt.imshow(mask) plt.axis(&#39;off&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; We can then get the coordinates in the array simply: . coords = np.argwhere(x == mask) coords . array([[ 0, 0], [ 0, 1], [ 0, 2], ..., [223, 221], [223, 222], [223, 223]]) . print(coords.size) . 20656 . If we knew a priori that a particular type of retinopathy was characterized by abnormal foveal locations, and we had a sufficient train/test/validation dataset, we could reduce the size of our dataset significantly with such an engineered feature. . print(f&#39;If we only require foveal coordinates, our dataset may be reduced by {round(100-(coords.size/my_image.size)*100,2)}% !&#39;) . If we only require foveal coordinates, our dataset may be reduced by 86.28% ! . In Closing... . I hope this example was helpful in showing how we can quickly prepare complex data types for computational purposes. I will return to abstract bioimaging processing in future posts. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/tensors.html",
            "relUrl": "/jupyter/2020/12/31/tensors.html",
            "date": " • Dec 31, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "It's better to be evil than boring",
            "content": "Background . Healthcare is, apart from university tuition, one of the more rapidly inflating services on the market. Inconvenient as it might be, much of the reason behind the crushing costs of paying for healthcare has little to do with what you might expect. The price of the cancer drug crizotinib was actually developed on a shoestring, judging by the name. And yeah, it stings to think about how our copayments trickle their way into the Mercedes-AMG option sheet between some surgeon&#39;s sweaty thumbs, but there aren&#39;t that many surgeons. In reality, a large reason behind cost inflation in healthcare has nothing to do with the evil stuff. It has to do with the boring stuff. . In this post, I will look at a major chapter of the administrative nightmare of healthcare: manual medical coding. We will quickly see how a large cost center in administration, medical coding, will likely be totally automated in the near future. My hope is that you will be left wondering how this is still possibly done largely by hand, and perhaps more optimistic about the future of healthcare. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/boring.html",
            "relUrl": "/jupyter/2020/12/31/boring.html",
            "date": " • Dec 31, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "(simple) Autoencoders ≡ PCA",
            "content": "Background . Shocker: The simplest autoencoder is actually the same as PCA. We will also see how VAEs perform expectation maximization. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/autoencoders.html",
            "relUrl": "/jupyter/2020/12/31/autoencoders.html",
            "date": " • Dec 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Originally from a small town in northern California, I completed my B.A. in biological sciences with a minor in mathematical finance at the University of Southern California. . I then spent about a year in consulting and briefly worked at a winery. I also devoted a considerable amount of time and effort on the law school admissions test (LSAT) but realized the profession was not for me. Instead, I chose to return toward technical topics and decided to pursue the master’s program in computational biology at Carnegie Mellon University. . Since then, I’ve worked toward mastering an understanding of statistical and deep learning methods as they relate to the biosciences. Some of my favorite coursework includes 02-710: Computational Genomics, and the infamous 11-785: Deep Learning. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://simonlevine.github.io/simonsays/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://simonlevine.github.io/simonsays/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}