{
  
    
        "post0": {
            "title": "Everything but the Model",
            "content": "Background . Most of the attention with regard to machine learning goes to the models, with good reason. We are at the point where we can reasonably model any $X to y$ relationship we wish. Difficult classification and clustering problems are increasingly tractable with clever approaches in statistical and rules-based learning. Even if we have no clue how to begin to go about developing such an approach (or such an approach would be intractable / biased), we can train deep neural networks – Universal Approximators – to theoretically represent almost any $f(x)= y = Wx + b$ relationship given properly trained weights $W$. . Still, humans are likely to be of use for the foreseeable future, and one of the more useful domains is the preparation of dat for such modeling. This includes topics such as feature selection and feature extraction, and how we might select the best model based on a rigorous, generalizable evaluation metric. . In this post, I will outline some of these useful topics with the hope that you will be able to tackle most everything about machine learning but the machine itself. . Example Dataset . I will use the COVID-19 Cell Atlas&#39; nasal immunodefiency swab dataset (https://www.sanger.ac.uk/group/vento-tormo-group/) for examples to follow. This is a recent dataset of real patients. I will use the scanpy package to load in the data and take a pandas dataframe for examples for feed into scikit-learn. . For the response variable, I will use Vasoactive agents required during hospitalization, a proxy for severity of symptoms and infection. . import scanpy dataset = scanpy.read_h5ad(&#39;../data/2021-01-09/Immunodeficiency_Nasal_swabs.h5ad&#39;) y = dataset.obs[&#39;Vasoactive agents required during hospitalization&#39;] X = dataset.obs.drop(columns=[&#39;Vasoactive agents required during hospitalization&#39;]) dataset.obs.head() . CellType log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Lab number Donor Id Age Sex Race ... Pre-existing Hypertension Pre-existing immunocompromised condition Smoking SARS-CoV-2 PCR SARS-CoV-2 Ab Symptomatic Admitted to hospital Highest level of respiratory support Vasoactive agents required during hospitalization 28-day death . GW1_AAACGGGAGCTAGTCT-1 Secretary epithelium | 14.909096 | 5687 | 0.042059 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AAAGTAGTCCTAGGGC-1 Secretary epithelium KRT5 | 13.611947 | 3967 | 0.097771 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AACACGTCAGCGTCCA-1 Ciliated epithelium | 9.366322 | 513 | 0.036419 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AACCATGAGAATCTCC-1 Secretary epithelium | 15.217731 | 6260 | 0.053720 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AACCATGCATCCTTGC-1 Ciliated epithelium | 9.134426 | 439 | 0.016043 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . 5 rows × 26 columns . X.columns . Index([&#39;CellType&#39;, &#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;, &#39;MT_fraction&#39;, &#39;Viral Molecules&#39;, &#39;Lab number&#39;, &#39;Donor Id&#39;, &#39;Age&#39;, &#39;Sex&#39;, &#39;Race&#39;, &#39;Ethnicity&#39;, &#39;BMI&#39;, &#39;Pre-existing heart disease&#39;, &#39;Pre-existing lung disease&#39;, &#39;Pre-existing kidney disease&#39;, &#39;Pre-existing diabetes&#39;, &#39;Pre-existing Hypertension&#39;, &#39;Pre-existing immunocompromised condition&#39;, &#39;Smoking&#39;, &#39;SARS-CoV-2 PCR&#39;, &#39;Symptomatic&#39;, &#39;Admitted to hospital&#39;, &#39;Highest level of respiratory support&#39;, &#39;28-day death&#39;], dtype=&#39;object&#39;) . This is a good toy dataset. We have a mixture of categorical, continuous-valued, integer-valued, string-valued, and others, as well as a clean binary Yes or No response variable. . Wrangling . A few short operations will make life easier later on. . Dealing with NaN (Missing Values) . You can see in the SARS-CoV-2 Ab column that we have NaN values. Although classifier implementations may have built-in accommodations, it may best best to deal with these values in a way we can fully control. . print(X[&#39;SARS-CoV-2 Ab&#39;]) . GW1_AAACGGGAGCTAGTCT-1 NaN GW1_AAAGTAGTCCTAGGGC-1 NaN GW1_AACACGTCAGCGTCCA-1 NaN GW1_AACCATGAGAATCTCC-1 NaN GW1_AACCATGCATCCTTGC-1 NaN ... GW13_TTTCCTCCAAGCCTAT-1 NaN GW13_TTTCCTCCAAGTCTGT-1 NaN GW13_TTTGTCAAGCCCAATT-1 NaN GW13_TTTGTCAGTAGGACAC-1 NaN GW13_TTTGTCATCGTGTAGT-1 NaN Name: SARS-CoV-2 Ab, Length: 4936, dtype: category Categories (3, object): [&#39;Not done&#39; &lt; &#39;Negative&#39; &lt; &#39;Positive&#39;] . First, though I suspect this is just an indicator variable to show whether or not the patient recieved an antibody test, we can look at unique values to be sure. . X[&#39;SARS-CoV-2 Ab&#39;].unique() . [NaN] Categories (0, object): [] . In this case, let&#39;s just drop the column. It is likely uninformative with regard to the reponse variable y = Vasoactive agents required during hospitalization. . X = X.drop(columns=[&#39;SARS-CoV-2 Ab&#39;]) print(X.shape,dataset.obs.shape, sep=&#39; n&#39;) . (4936, 24) (4936, 26) . As expected. . Dealing with Categorial Features . We will need to encode nominal and/or ordinal features to a one-hot representation. We can easily exclude numerical-valued columns from this process. We should also binarize the $y$ labels. . numer_cols = list(X._get_numeric_data().columns) cat_cols = list(set(X.columns) - set(numerical_cols)) print(f&#39;numerical columns: n{numer_cols} n ncategorical columns: n {cat_cols}&#39;) . numerical columns: [&#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;, &#39;MT_fraction&#39;, &#39;Viral Molecules&#39;] categorical columns: [&#39;Race&#39;, &#39;BMI&#39;, &#39;Pre-existing immunocompromised condition&#39;, &#39;Pre-existing Hypertension&#39;, &#39;Donor Id&#39;, &#39;Pre-existing lung disease&#39;, &#39;Pre-existing diabetes&#39;, &#39;Age&#39;, &#39;28-day death&#39;, &#39;Admitted to hospital&#39;, &#39;Lab number&#39;, &#39;Smoking&#39;, &#39;Ethnicity&#39;, &#39;Symptomatic&#39;, &#39;Pre-existing kidney disease&#39;, &#39;Sex&#39;, &#39;Highest level of respiratory support&#39;, &#39;SARS-CoV-2 PCR&#39;, &#39;CellType&#39;, &#39;Pre-existing heart disease&#39;] . X[cat_cols].head(3) . Race BMI Pre-existing immunocompromised condition Pre-existing Hypertension Donor Id Pre-existing lung disease Pre-existing diabetes Age 28-day death Admitted to hospital Lab number Smoking Ethnicity Symptomatic Pre-existing kidney disease Sex Highest level of respiratory support SARS-CoV-2 PCR CellType Pre-existing heart disease . GW1_AAACGGGAGCTAGTCT-1 White | 30.0-39.9 (obese) | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium | No | . GW1_AAAGTAGTCCTAGGGC-1 White | 30.0-39.9 (obese) | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium KRT5 | No | . GW1_AACACGTCAGCGTCCA-1 White | 30.0-39.9 (obese) | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Ciliated epithelium | No | . With the exception of BMI, this looks fine. Since we have a BMI range, we should use an ordinal encoder in this case. The rest are simply categorical. . ord_cols = [&#39;BMI&#39;] cat_cols = [i for i in cat_cols if i not in ord_cols] X[cat_cols].head(3) . Race Pre-existing immunocompromised condition Pre-existing Hypertension Donor Id Pre-existing lung disease Pre-existing diabetes Age 28-day death Admitted to hospital Lab number Smoking Ethnicity Symptomatic Pre-existing kidney disease Sex Highest level of respiratory support SARS-CoV-2 PCR CellType Pre-existing heart disease . GW1_AAACGGGAGCTAGTCT-1 White | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium | No | . GW1_AAAGTAGTCCTAGGGC-1 White | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium KRT5 | No | . GW1_AACACGTCAGCGTCCA-1 White | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Ciliated epithelium | No | . Now we can encode these properly using scikit-learn or a builtin pandas method (we will use the latter and comment out the former). While we could use an ordinal encoder, whereby one clas is mapped to an integer, we should actually use one-hot encoding as this is a continuous input, valid for scikit-learn estimators. Note that a NaN is treated as a distinct category. It&#39;s a good thing we dropped that NaN column! . import pandas as pd pd.get_dummies(X[cat_cols]).head() . Race_White Race_Black Race_Asian Race_Other Pre-existing immunocompromised condition_No Pre-existing immunocompromised condition_Yes Pre-existing Hypertension_No Pre-existing Hypertension_Yes Donor Id_GWAS_1 Donor Id_GWAS_10 ... CellType_Secretary epithelium KRT5 CellType_Squamous epithelium 1 CellType_Squamous epithelium 2 CellType_Ciliated epithelium CellType_Neutrophil CellType_Erythrocytes CellType_Low quality CellType_filtered cells and doublets Pre-existing heart disease_No Pre-existing heart disease_Yes . GW1_AAACGGGAGCTAGTCT-1 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . GW1_AAAGTAGTCCTAGGGC-1 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . GW1_AACACGTCAGCGTCCA-1 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . GW1_AACCATGAGAATCTCC-1 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . GW1_AACCATGCATCCTTGC-1 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 5 rows × 77 columns . from sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder() transformed_data = encoder.fit_transform(X[cat_cols]) encoded_data = pd.DataFrame(transformed_data, index=X[cat_cols].index) # now concatenate the original data and the encoded data concatenated_data = pd.concat([X[cat_cols], encoded_data], axis=1) concatenated_data . Race Pre-existing immunocompromised condition Pre-existing Hypertension Donor Id Pre-existing lung disease Pre-existing diabetes Age 28-day death Admitted to hospital Lab number Smoking Ethnicity Symptomatic Pre-existing kidney disease Sex Highest level of respiratory support SARS-CoV-2 PCR CellType Pre-existing heart disease 0 . GW1_AAACGGGAGCTAGTCT-1 White | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . GW1_AAAGTAGTCCTAGGGC-1 White | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium KRT5 | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . GW1_AACACGTCAGCGTCCA-1 White | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Ciliated epithelium | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . GW1_AACCATGAGAATCTCC-1 White | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . GW1_AACCATGCATCCTTGC-1 White | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Ciliated epithelium | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . GW13_TTTCCTCCAAGCCTAT-1 White | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Tcells CD8 | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . GW13_TTTCCTCCAAGTCTGT-1 White | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Tcells CD8 | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . GW13_TTTGTCAAGCCCAATT-1 White | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . GW13_TTTGTCAGTAGGACAC-1 White | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Squamous epithelium 1 | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . GW13_TTTGTCATCGTGTAGT-1 White | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Squamous epithelium 1 | No | (0, 1) t1.0 n (0, 2) t1.0 n (0, 3) t1.0 n ... | . 4936 rows × 20 columns . from sklearn.preprocessing import X[cat_cols]. . log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules . GW1_AAACGGGAGCTAGTCT-1 14.909096 | 5687 | 0.042059 | 0 | . GW1_AAAGTAGTCCTAGGGC-1 13.611947 | 3967 | 0.097771 | 0 | . GW1_AACACGTCAGCGTCCA-1 9.366322 | 513 | 0.036419 | 0 | . GW1_AACCATGAGAATCTCC-1 15.217731 | 6260 | 0.053720 | 0 | . GW1_AACCATGCATCCTTGC-1 9.134426 | 439 | 0.016043 | 0 | . ... ... | ... | ... | ... | . GW13_TTTCCTCCAAGCCTAT-1 10.832890 | 870 | 0.017005 | 0 | . GW13_TTTCCTCCAAGTCTGT-1 11.065416 | 1001 | 0.015406 | 0 | . GW13_TTTGTCAAGCCCAATT-1 13.670767 | 3477 | 0.032975 | 0 | . GW13_TTTGTCAGTAGGACAC-1 9.147205 | 313 | 0.044170 | 0 | . GW13_TTTGTCATCGTGTAGT-1 9.623881 | 537 | 0.029188 | 0 | . 4936 rows × 4 columns . y.head() . GW1_AAACGGGAGCTAGTCT-1 Yes GW1_AAAGTAGTCCTAGGGC-1 Yes GW1_AACACGTCAGCGTCCA-1 Yes GW1_AACCATGAGAATCTCC-1 Yes GW1_AACCATGCATCCTTGC-1 Yes Name: Vasoactive agents required during hospitalization, dtype: category Categories (2, object): [&#39;No&#39; &lt; &#39;Yes&#39;] . X[set(X.columns) - set(numerical_cols)] . Race BMI Pre-existing immunocompromised condition Pre-existing Hypertension Donor Id Pre-existing lung disease Pre-existing diabetes Age 28-day death Admitted to hospital Lab number Smoking Ethnicity Symptomatic Pre-existing kidney disease Sex Highest level of respiratory support SARS-CoV-2 PCR CellType Pre-existing heart disease . GW1_AAACGGGAGCTAGTCT-1 White | 30.0-39.9 (obese) | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium | No | . GW1_AAAGTAGTCCTAGGGC-1 White | 30.0-39.9 (obese) | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium KRT5 | No | . GW1_AACACGTCAGCGTCCA-1 White | 30.0-39.9 (obese) | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Ciliated epithelium | No | . GW1_AACCATGAGAATCTCC-1 White | 30.0-39.9 (obese) | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium | No | . GW1_AACCATGCATCCTTGC-1 White | 30.0-39.9 (obese) | Yes | No | GWAS_1 | No | No | 18 | No | Yes | CV19-1-S3.2A | Never or unknown | Not Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Ciliated epithelium | No | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . GW13_TTTCCTCCAAGCCTAT-1 White | Unknown | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Tcells CD8 | No | . GW13_TTTCCTCCAAGTCTGT-1 White | Unknown | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Tcells CD8 | No | . GW13_TTTGTCAAGCCCAATT-1 White | Unknown | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Secretary epithelium | No | . GW13_TTTGTCAGTAGGACAC-1 White | Unknown | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Squamous epithelium 1 | No | . GW13_TTTGTCATCGTGTAGT-1 White | Unknown | Yes | No | GWAS_11 | No | No | 50 | No | Yes | CV19-11-S3.2A | Never or unknown | Hispanic or Latino | Yes | No | F | Mechanical ventilation with intubation | Positive | Squamous epithelium 1 | No | . 4936 rows × 20 columns . Feature Rescaling . It may be prudent to rescale features, especially if each feature is not on the same given scale. This may be done to remove the bias of certain features given downstream tasks. For instance, take the following features from our $X$: log2p1_RNAcount and nFeature_RNA. . X[[&#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;]].head() . log2p1_RNAcount nFeature_RNA . GW1_AAACGGGAGCTAGTCT-1 14.909096 | 5687 | . GW1_AAAGTAGTCCTAGGGC-1 13.611947 | 3967 | . GW1_AACACGTCAGCGTCCA-1 9.366322 | 513 | . GW1_AACCATGAGAATCTCC-1 15.217731 | 6260 | . GW1_AACCATGCATCCTTGC-1 9.134426 | 439 | . In an extraction process (such as PCA, making use of covariance), or when using a classifier making use of Euclidean distance, the feature with the largest numerical range will be naturally more weighted. . x_1 = X[&#39;log2p1_RNAcount&#39;] x_2 = X[&#39;nFeature_RNA&#39;] range_1 = x_1.max()-x_1.min() range_2 = x_2.max()-x_2.min() print(range_1, range_2, sep=&#39; n&#39;) . 12.476429788752128 9651 . nFeature_RNA therefore has a much more significant bearing on an outcome contingent upon this range. . We therefore rescale in a few ways: . 1) with min/max rescaling $x_i&#39; = frac{x_i - min(x)}{max(x) - min(x)}$ | simple, preserves mean of dataset. Useful for image pixel intensity, for instance. | . | 2) with $z$-score normalization . $x_i&#39; = frac{x_i - bar{x}}{ sigma}$ = $ text{number of standard deviations from the mean}$ = $z text{-score}$ | standardizes features, typically with $ mu = 0, sigma^2 = 1$. This is a better choice than min/max for things like PCA since in that case we want to select components maximizing variance of the feature matrix, without getting caught up by the scale of that variance. | . | 3) with median and interquartile range rescaling (robust rescaling). . Remove the median value and scale according to interquartile range. | better choice if there are significant outliers. | . | . Let&#39;s go with 3) somewhat arbitrarily but also in the case of outliers. . from sklearn import preprocessing robust_scaler = preprocessing.RobustScaler() robust_scaler.fit_transform(X) . ValueError Traceback (most recent call last) &lt;ipython-input-62-da54db209324&gt; in &lt;module&gt; 2 3 robust_scaler = preprocessing.RobustScaler() -&gt; 4 robust_scaler.fit_transform(X) ~/opt/anaconda3/envs/pt_xmlc_transformer/lib/python3.8/site-packages/sklearn/base.py in fit_transform(self, X, y, **fit_params) 688 if y is None: 689 # fit method of arity 1 (unsupervised transformation) --&gt; 690 return self.fit(X, **fit_params).transform(X) 691 else: 692 # fit method of arity 2 (supervised transformation) ~/opt/anaconda3/envs/pt_xmlc_transformer/lib/python3.8/site-packages/sklearn/preprocessing/_data.py in fit(self, X, y) 1198 # at fit, convert sparse matrices to csc for optimized computation of 1199 # the quantiles -&gt; 1200 X = self._validate_data(X, accept_sparse=&#39;csc&#39;, estimator=self, 1201 dtype=FLOAT_DTYPES, 1202 force_all_finite=&#39;allow-nan&#39;) ~/opt/anaconda3/envs/pt_xmlc_transformer/lib/python3.8/site-packages/sklearn/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params) 418 f&#34;requires y to be passed, but the target y is None.&#34; 419 ) --&gt; 420 X = check_array(X, **check_params) 421 out = X 422 else: ~/opt/anaconda3/envs/pt_xmlc_transformer/lib/python3.8/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs) 71 FutureWarning) 72 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}) &gt; 73 return f(**kwargs) 74 return inner_f 75 ~/opt/anaconda3/envs/pt_xmlc_transformer/lib/python3.8/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator) 597 array = array.astype(dtype, casting=&#34;unsafe&#34;, copy=False) 598 else: --&gt; 599 array = np.asarray(array, order=order, dtype=dtype) 600 except ComplexWarning: 601 raise ValueError(&#34;Complex data not supported n&#34; ~/opt/anaconda3/envs/pt_xmlc_transformer/lib/python3.8/site-packages/numpy/core/_asarray.py in asarray(a, dtype, order) 83 84 &#34;&#34;&#34; &gt; 85 return array(a, dtype, copy=False, order=order) 86 87 ~/opt/anaconda3/envs/pt_xmlc_transformer/lib/python3.8/site-packages/pandas/core/generic.py in __array__(self, dtype) 1776 1777 def __array__(self, dtype=None) -&gt; np.ndarray: -&gt; 1778 return np.asarray(self._values, dtype=dtype) 1779 1780 def __array_wrap__(self, result, context=None): ~/opt/anaconda3/envs/pt_xmlc_transformer/lib/python3.8/site-packages/numpy/core/_asarray.py in asarray(a, dtype, order) 83 84 &#34;&#34;&#34; &gt; 85 return array(a, dtype, copy=False, order=order) 86 87 ValueError: could not convert string to float: &#39;Secretary epithelium&#39; . Feature Extraction . Feature extraction is the process of reducing dimensionality to find latent features in a given feature set. This can be done in a variety of ways for a variety of use cases. As for why, let&#39;s say we have a dataset with a massive number of features, such that training a network to make use of all the features somewhat equally. That is, let . .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/09/all-but-the-model.html",
            "relUrl": "/jupyter/2021/01/09/all-but-the-model.html",
            "date": " • Jan 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Probability for Computational Biologists",
            "content": "Introduction . This will be a comprehensive outline of topics in probability, including random variables, distributions, and rules of probability. My hope is that this will serve as a picture of (almost) everything you need may need to know regarding probability. . Topics such as statistical and deep learning leverage these concepts as assumed knowledge. It is therefore vital to have at least working knowledge of what is to come. . Counting . Multiplication Rule . Given an experiment with many potential outcomes, we can simply multiply the number of outcomes at each step for the overall number of possible outcomes. . $n_{total} = prod_i n_i$ . Sampling . Sampling $k$ items from a population of size $n$ results in the number of possibilities as follows: . Replacement (place each of the $k$ samples back into the population after choosing!) . Order-Preserving (we count each unique order in which we select the $k$ samples) $n^k$ | . | Order Doesn&#39;t Matter (we only care about the class membership of the $k samples) $ {n+k-1} choose{k} $ | . | . | No Replacement . Order-Preserving $ frac{n!}{(n-k)!}$ | A permutations problem. | . | Order Doesn&#39;t Matter $n choose k$ = $ frac{n!}{k!(n-k)!}$ | A combinations problem; use the binomial coefficient | . | . | . Na&#239;ve Probability . If all outcomes in a given event space are equally likely, $P_{naive}(X=x_i) = frac{ text{outcomes favorable to } x_i}{ text{number of outcomes}}$ . | This is intuitive. The probability of heads given a strictly fair coin (i.e., $X sim Bernoulli(p=.50)$) over two trials is $ frac{1}{2}$. . | . Conditional Probability . Independence . Independent Events . Two events $A,B$ are independent if the outcome of one event has no bearing on the other. In other words, knowing the outcome of $B$ gives no information about the potential outcome of $A$: . For independent events $A,B$, $P(A,B) = P(A)P(B)$ | $P(A|B) = P(A)$ | $P(B|A) = P(B)$ | . | . Conditional Independence . Two events $A,B$ are conditionally independent given another event outcome $C$ if: $$P(A,B|C) = P(A|C)P(B|C)$$ That is, we can tease apart the probability of a given event if they share a relevant background variable. . As an example, take the problem of three genetic mutations, $i,j$ and $k$. Let $i$ and $j$ be tightly correlated in a dataset and assume the probability distributions of each event is known. At first glance, we might think we could never model the two mutations independently. We may even assume $i$ causes $j$ in some fashion, or vice versa. . However, if we discovered that $k$ was a definitely a mutation appearing in an upstream promoter region impacting both sites corresponding to $i$ and $j$, we could then show conditional independence between $i$ and $j$ given the mutation $k$. Suddenly, our assumptions change and we may be more inclined to target $k$ as an event outcome worthy of attention. . Unions, Intersects, Complements . Set theory can be useful in the realm of probabilty. At the end of the day, we use probabilistic models to try and understand real events. This is the case with both discrete and continuous outcomes. . De Morgan&#39;s Laws . De Morgan&#39;s Laws offer versatility in logical reasoning, proofs, set theory, and other areas of intrigue. One thing to note is that generally, we frequently use $AND$ and $OR$ operators in probability. Note the following: . $ lnot(A lor B)= lnot A land lnot B $ | $ lnot(A land B) = lnot A lor lnot B $ | . Joint, Marginal, and Conditional Probability . Joint Probability . $P(A,B)$ . Note, $P(A,B) = P(A)P(B|A)$ | We can tease apart the distributions by conditioning on the portion of the event space occupied by $B$ where $A$ also has a bearing. . | Note also how $P(A,B)=P(B,A) = P(B)P(A|B)$. While consistency is important, this is just a matter of our choice of labels on the events. . | . | . Marginal (Unconditional / Prior) Probability . $P(A)$ | . Conditional Probability . $P(A|B) = frac{P(A,B)}{P(B)}$ | The Probability of A given B is equal to the Probability of A and B over the (prior) Probability of B . | Note: We can easily see how Bayes&#39; Rule follows. Given that we can &quot;flip&quot; the order of the joint probability expression, what is the right side equivalent to? . $P(A|B) = frac{P(A,B)}{P(B)} to P(A|B)P(B) = P(A,B) = P(B|A)P(A)$ | $ implies P(A,B) = frac{P(B|A)P(A)}{P(B)}$, or Bayes&#39; Rule! | . | . Conditional Probability is Probability . $P(A|B)$ is a probability function like any other for a fixed $B$. Any theorem applicable to probability is relevant for conditional probability. | . Chain Rule for Probability . Note we can disentangle a joint probability by use of the &quot;chain&quot; rule, an extension of operations on two-event probabilities. | $P(A,B,C) = P(A|B,C)P(B,C) = P(A|B,C)P(B|C)P(C)$. | This is exaclty the same as calling the joint event space $B,C = D$ and writing $P(A,D) = P(A|D)P(D)$. | . Law of Total Probability . For an event $A$ and disjoint sample partitions $B_1,...,B_n$, we can always marginalize out irrelevant event spaces. . $P(A) = P(A|B_1)P(B_1) + ... + P(A|B_n)P(P_n)$ | . Bayes&#39; Rule . Combining the definitions of conditional probility $P(A|B) = frac{P(A,B)}{P(B)}$ and joint probability $P(A,B) = P(A|B)P(B) = P(B|A)P(A)$, we can describe Bayes&#39; Rule: . $$P(A|B) = frac{P(B|A)P(A)}{P(B)}$$ . For 3 events $A,B,C$, we can write $$P(A|B,C) = frac{P(A,B,C)}{P(B,C)} = frac{P(B,C|A)P(A)}{P(B,C)}$$ . We can also use the chain rule to our liking: . $$P(A|B,C) = frac{P(A,B,C)}{P(B,C)} = frac{P(A|B,C)P(B|C)P(C)}{P(B|C)P(C)} = P(A|B,C)$$ . Note that it may be useful to commute these terms depending on the circumstance. . Random Variables and their Distributions . A random variable can take on a number of values according to a mathematical function. This may be thought of as the probability of a given outcome of an experiment in a global sense. . Probability Mass Functions (PMF) and Probability Distribution Functions (PDF) . Cumulative Distribution Functions (CDF) . Survival Functions . Independence of Random Variables . Expected Values and Indicators . Expected Values and Linearity . Expected Values . Mean, expectation, or first moment . Linearity . $ text{Distribution} implies text{Mean}$ . Conditional Expected Value . Indicator Random Variables . Indicator RVs . Distribution of an Indicator RV . Fundamental Bridge of an Indicator RV . Variance and Standard Deviation (w.r.t. Expectation) . Continuous Random Variables, Law of the Unconscious Statistician (LOTUS), and the Universaility of Uniform (UoU) . Continuous Random Variables . Probability of a CRV in a Given Interval . The Probability Density Function of a CRV . Expected Values of CRVs versus DRVs . Law of the Unconscious Statistician (LOTUS) . $g(RV_i) = RV_j$: The function of a random variable is itself a random variable. . I.e., one need only know the PMF/PDF of $X$ to find the PMF/PDF of $g(x)$. | . Universality of Uniform / Probability Integral Transform . Substitution of any $X_{cts}$ into its cumulative distribution function $F_X(x) = P(X leq x)$ yields $U(0,1)$. | Let $Y=F_X(X)$. Then, $F_Y(y) = P(Y leq y) = P(F_X(X) leq y) = P(X leq F^{-1}(y)) = F_X(F_X^{-1}(y)) = y$ for $Y sim U(0,1)$ and $X$ is some continous random variable with CDF $F_X$. . I.e. $F_X(X_{cts}) = int_{- infty}^{x}f(t)dt = int_{- infty}^{x}P(X leq t)dt = X sim U(0,1)$. | . | . Now, in Python. . Moments and Moment-Generating Functions . Moments . Moment-Generating Functions . Joint Probability Density Functions (PDFs) and Comulative Distribution Functions (CDFs) . Joint Distributions . Conditional Distributions . Bayes&#39; Rule for Discrete RVs . Bayes&#39; Rule for Continuous RVs . Marginal Distributions . Discrete Case: Marginal PMF from Joint PMF | Continous Case: Marginal PDF from Joint PDF | . Independence of Random Variables . Multivariate LOTUS . Further Topics . We will now take a look at topics that are more practical and/or obscure, such as relevant distributions and Markov Models . Covariance and Transformations . Covariance and Correlation . Covariance and Independence . Covariance and Variance . Properties of Covariance . Correlation: Location and Space-Invariant . Transformations . Single Variable . | Two Variables . | . Convolutions . Convolution Integral . Relevance to &quot;Convolutional&quot; Neural Networks . Poisson Processes . Law of Large Numbers . Central Limit Theorem . Markov Chains . Markov Property . States . Transition Matrix . Chain Properties . Stationary DIstributions . Some Continuous Distribitions . Normal . Exponential . Gamma . Beta . Chi-Square . Some Discrete Distribitions . Sampling: Varying Number of Trials and Replacement . Bernoulli . Binomial . Geometric . First-Success . Negative Binomial . Hypergeometric . Poisson . Some Multivariate Distributions . Multinomial . Multivariate Uniform . Multivariate Normal . A Note on Mixture Models . EM and Mixture Models . In Case You Missed It: Special Cases of Distributions . - . Important Inequalities . Cauchy-Shwarz . Markov . Chebyshev . Jensen . Background: Formulas . Geometric Series . Exponential Function . Gamma and Beta Integrals . Euler&#39;s Approximation for a Harmonic Sum . Stirling&#39;s Approximation for Factorials .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/05/probability.html",
            "relUrl": "/jupyter/2021/01/05/probability.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Magic in Numpy",
            "content": "Numpy Basics . We will now go over some basic approaches on a seemingly simple matrix for illustrative purposes. Hopefully some of the efficient and useful properties of Numpy become apparent. . A favorite work by a favorite artist, Dürer&#39;s Melencolia I (1514) includes sophisticated use of mathematical allegory, particularly in the top-right corner. It turns out Dürer actually makes many interesting points through his art, something you wouldn&#39;t expect from his messiah complex. . . . The matrix is thus: . import numpy as np X = np.array([[16, 3, 2, 13], [5, 10, 11, 8], [9, 6, 7, 12], [4, 15, 14, 1]]) print(X) . [[16 3 2 13] [ 5 10 11 8] [ 9 6 7 12] [ 4 15 14 1]] . type(X) . numpy.ndarray . Magic Squares . This matrix is purported to be a magic square. We must fit the following constraints: . 1) Magic 2) Square . Simple enough. Starting with the second condition, Numpy provides a number of methods. Though magic cubes and tesseracts are possible, we can begin with a square. Here&#39;s a simple function to detect if an array is square. . def is_square(M: np.ndarray) -&gt; bool: &#39;&#39;&#39; Arguments: M, a 2-d matrix Returns: a boolean, True if square &#39;&#39;&#39; assert M.ndim == 2 return True if M.shape[0] == M.shape[1] else False is_square(X) . True . Vectorized Summation: Magic . Now, the more involved condition. If a square is &quot;magic&quot;, the array exhibits the following properties1: . i) Each of the $n$ elements are of the set of distinct positive integers $[1,2,3,...,n^2]$, such that $n$ is the &quot;order&quot; of the square. Dürer thus presents a $4^{th}$ order magic square. . ii) The sum of the $n$ numbers about any horizontal, vertical or main diagnonal is the same number – the &quot;magic&quot; constant. It is known that such magic constants can be given by $ mathcal{M}(X_n) = frac{1}{n} sum_{k=1}^{n^2}k = frac{1}{2}n(n^2+1)$ . . Aside: iii) The complement to a magic square is derived from subtracting every number in a given magic square by $n^2 + 1$. . Back to Dürer. We can check each condition as follows. . . there are others, see https://faculty.etsu.edu/stephen/matrix-magic-squares.pdf)&#8617; . | def is_magic(M, verbose = True)-&gt;bool: #By constraints i) &amp; ii) assert M.shape[0] == M.shape[1], &#39;Not a square.&#39; n = M.shape[0] assert np.array_equal(np.sort(M.flatten()), np.arange(n**2) + 1), &#39;Expected elements from [1,2,...,n^2]&#39; column_sums = np.sum(M,axis=0) #Note that summing across axis 0 actually returns column-wise sums, and vice-versa. row_sums = np.sum(M, axis=1) diagonal_sums = np.array([np.trace(M),np.trace(np.fliplr(M))]).astype(int) magic_num_sum = np.unique(np.concatenate( (column_sums,row_sums,diagonal_sums) )) if len(magic_num_sum) == 1: if verbose: print(f&#39;Magic number is {magic_num_sum} with order {n}.&#39;) return True . np.fliplr(X).diagonal().sum() == np.flipud(X).diagonal().sum() . True . np.trace(X) == np.diagonal(X).sum() . True . is_magic(X) . Magic number is [34] with order 4. . True . X . array([[16, 3, 2, 13], [ 5, 10, 11, 8], [ 9, 6, 7, 12], [ 4, 15, 14, 1]]) . Fast Indexing: Gnomon Magic . Dürer&#39;s square is actually a Gnomonic Magic Square – that is, each non-overlapping root subsquare bordering the four sides of the square ($2 times 2$ subsquare), as well as the central subsquare, sums to the magic constant of the overarching square. . The Gnomon is the portion of the sundial casting a shadow. In a way we also cast a magic projection on subarrays of our main magic square. . We can verify this easily – in Numpy, arrays can be efficiently split with minimal logic, rather than looping over each element and hard-indexing. . a,b,c,d = [quadrant for sub_x in np.split(X,2, axis = 0) for quadrant in np.split(sub_x,2, axis = 1)] n = X.shape[0] n_subsquare = np.sqrt(n).astype(int) start = n//2 - (n_subsquare // 2) end = n//2 + (n_subsquare // 2) e = X[start:end,start:end] sections = [a, b, c, d, e] sections . [array([[16, 3], [ 5, 10]]), array([[ 2, 13], [11, 8]]), array([[ 9, 6], [ 4, 15]]), array([[ 7, 12], [14, 1]]), array([[10, 11], [ 6, 7]])] . print(set([sum(s.flatten()) for s in sections])) . {34} . All quadrants sum to the magic number of 34. As such, we have verified the deliberate style of Dürer. . Linear Algebra . We will now move onto some essential linear algebra operations on the magic square. . X . array([[16, 3, 2, 13], [ 5, 10, 11, 8], [ 9, 6, 7, 12], [ 4, 15, 14, 1]]) . Rank . The rank of a matrix is the number of its linearly independent columns. That is, the dimensionality of the vector space spanned by a matrix&#39;s columns (or rows) is given by its rank, such that the span is the smallest linear subspace containing a set of vectors describing the matrix. . We can obtain the span of all linear combinations of some vectors $ vec{u}, vec{v}$ by computing $s vec{u} + t vec{v}$ for some scalar constants $s,t$. The dimensionality of the span of the row or column vectors of a matrix thus yields the row or column rank. . We will proceed using Numpy, which proceeds using singular value decompositon (SVD): . rank = np.linalg.matrix_rank(X) rank . 3 . Thus we have a rank-deficient matrix, since $3 &lt; 4$. 4 is the column-dimensionality of the Magic Square matrix but the columns only span 3 dimensions. Note that $rank(M) leq min (m,n)$ for an $m times n$ matrix $M$. . Note how Numpy uses the property that the rank is equal to the number of nonzero singular values as follows: . u,s,vh = np.linalg.svd(X) s . array([3.40000000e+01, 1.78885438e+01, 4.47213595e+00, 6.25921042e-16]) . We have 4 nonzero singular values, but the final value is extremely small. Numpy therefore considers this zero as the default tolerance is computed as M.max()*max(M.shape)*eps. . eps = np.finfo(float).eps tol = X.max()*max(X.shape)*eps tol . 1.4210854715202004e-14 . rank == len(s[s&gt;tol]) . True . rank . 3 . Determinant . The determinant is a useful encoding of the linear transformation described by a particular $n times n$ matrix. In geometric terms, it is analagous to the volume scaling factor of the linear transformation described by the matrix. . In other words, this is the volume of the parallelepiped (a rhomboid prism; a cube is to a square as a parallelepiped is to a parallelogram) spanned by the vectors (row or column) of a matrix. The sign of the determinant of a matrix denotes whether or not the orientation of a vector space is preserved by the transformation described by the matrix. . Two simple examples, then Dürer&#39;s: . A = np.array([[0,-1],[1,0]]) B = np.array([[-2,0],[0,2]]) . $A$ describes a 90-degree counterclockwise (↪️) rotation. . $B$ describes a scaling by a factor of $2$ as well as a reflection about the $y$ axis. . print(np.linalg.det(A), np.linalg.det(B), sep=&#39; n&#39;) . 1.0 -4.0 . A simple rotation does not change &quot;volume&quot; nor orientation. A scaling on $x,y$ and a reflection about the $y$ axis is encoded in the determinant, however: the &quot;volume&quot; is changed in total by a factor of $4$ and the sign is negative, indicating a change in the orientation of previous vector space. . These are simple enough to compute by hand, but even a $4 times 4$ dimensional matrix as provided by Dürer is more onerous. Thankfully, Numpy works well: . X = np.array([[16, 3, 2, 13], [5, 10, 11, 8], [9, 6, 7, 12], [4, 15, 14, 1]]) np.linalg.det(X) . 1.449507180950607e-12 . An interesting observation: Dürer does not provide a pandiagonal magic square, as the determinant of this order-4 magic square is near to, but $not$, zero. In other words, if the broken diagonals (for instance, $16,11,12,15$, or $3,8,7,4$) summed to the magic number, the determinant would be zero [^1]. . Eigenvectors and Eigenvalues . We solve the characteristic equation of a matrix $M$ to find eigenvalues $ vec{ lambda}$. That is, we solve $|M - lambda I| = 0$ where $I$ is the identity matrix ($I_{ij} = 1 s.t i=j, 0 s.t. i not = j$) of identical dimensionality to $M$. . In the case of Dürer&#39;s magic square, we simply subtract a value $ lambda$ from each element on the main diagonal and set the resulting matrix&#39;s determinant equal to zero. . We can quickly avoid rote work using Numpy: . eigenvals, eigenvects = np.linalg.eig(X) eigenvects . array([[ 5.00000000e-01, 8.16496581e-01, 2.23606798e-01, -4.08248290e-01], [ 5.00000000e-01, -4.08248290e-01, -6.70820393e-01, 0.00000000e+00], [ 5.00000000e-01, -1.76752662e-16, 6.70820393e-01, -4.08248290e-01], [ 5.00000000e-01, -4.08248290e-01, -2.23606798e-01, 8.16496581e-01]]) . eigenvals . array([ 3.40000000e+01, 8.00000000e+00, 4.84818517e-17, -8.00000000e+00]) . Note the interesting property of magic squares: the principal (largest) eigenvalue of a magic square composed of positive elements is its magic constant! Of further note, but not applicable here, is the observation that if a magic square has some negative elements, then its magic constant is one of its eigenvalues.[^1] . To show the first point, consider that $[1,1,...,1]^T$ is an eigenvector of a matrix $M$ if every row sums to the same value $k$. This can be shown by computing $|M- lambda I| = 0$ for a matrix $M$ where all rows sum to the same constant $k$. Substituting values and simplifying, $k$ is an eigenvalue. The same holds for columns that sum to the same constant. . Now, we want to show that the entries in the vector $Mv$ are equal to $kv$, where $k$ is both the magic number and an eigenvalue, and $v$ is an eigenvector of $M$. Recall that the sum of all elements in an $n times n$ magic square $M^*$ is, by construction, equal to $ frac{n(n^2+1)}{2}$. Thus, since a magic square $M^*$ indeed does have each row sum to $k$, we have that $Av = kv$ for $[1,1,...,1]^T$. . This gives us another way to find the magic number of a magic square. . def get_magic_number(M): if is_magic(M, verbose= False) and is_square(M): return np.linalg.eig(X)[0].round(1).astype(int)[0] get_magic_number(X) . 34 . Conclusion . This concludes discussion (for now) of magic squares, essential Numpy, and some linear-algebraic approaches to simple matrices. From here, I hope to move to much more complex topics involving far more abstract data types and approaches to manipulation. Nonetheless, foundations will always be important and most likely present under the hood. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/04/linear-algebra.html",
            "relUrl": "/jupyter/2021/01/04/linear-algebra.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Tensors via Bioimaging",
            "content": "Background . 1.3 billion humans are able to use Microsoft Excel. Microsoft Excel is the modern clay tablet, an intuitive but ultimately limited instrument for the computational professional. After all, why should we be limited to two dimensions? Why should we pay for the privelege? Often, data is best represented as $n$-dimensional. . For instance, let&#39;s say you&#39;d like become a billionaire without too much effort. One way would be to totally automate clinical bioimage analysis at human-level fidelity using machine learning. . . A Diabetic retinopathy slide (https://www.kaggle.com/c/diabetic-retinopathy-detection). The condition is estimated to effect over 93 million people. . Whereas a highly skilled human could potentially spot abnormalities in the above retinopathy slide, a machine can do it better and much faster. Partially, this is because a machine views the below image somewhat naïvely as 150,528 dots, as we have a square RGB image with with 224 pixels per dimension. . Viewing the image not as an image but as a tensor (vector-valued matrices), or an $n$-dimensional generalization of a matrix, we can then move onto analyzing the image: segmentation, feature learning, classification (if labels are detected), and other interesting tasks that a human may or may not be able to do. . Numpy . We use Numpy for efficient vectorized tensor manipulation with the convenient abstraction of the Python programming language. Numpy fluency will carry a computational professional very far, and it will only begin to show limitations when deep learning and very large datasets are involved (though the syntax of major deep learning packages are very close to Numpy). . We can easily load the above png &quot;image&quot; into a numpy array using a number of packages. imageio is used below. . import imageio import numpy as np my_image = imageio.imread(&#39;10009_right.png&#39;) my_image . Array([[[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], ..., [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]]], dtype=uint8) . Above we see the image loaded into memory as an integer-valued 3-dimensional matrix. Basic Numpy / Python fluency is assumed (slicing, etc.). . my_image.shape #note memory persistence in Jupyter. . (224, 224, 3) . my_image.size . 150528 . my_image.ndim . 3 . The $224 times 224 times 3$, $150,528$-element, $3$-dimensional array can be now be subject to a multitude of useful manipulations. . For instance, we can sparsify the matrix using scipy.sparse for 2-d matrices and sparse (!pip install sparse) for $n$-dimensional arrays (tensors). This may be useful for quick compression or storage of many such images: note the large number of zero values corresponding to &quot;black&quot; portions of the image. . import sparse sparse.COO(my_image) . Formatcoo | . Data Typeuint8 | . Shape(224, 224, 3) | . nnz120885 | . Density0.8030731823979592 | . Read-onlyTrue | . Size2.9M | . Storage ratio20.1 | . The sparse matrix in general is less memory intensive. . Another example of manipulation: we can quickly invert and flip the image with numerical rigor. We will use matplotlib to display the numpy array. . np.invert is np.bitwise_not . True . import matplotlib.pyplot as plt plt.imshow(my_image) plt.axis(&#39;off&#39;) plt.title(&#39;All Axes&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; plt.imshow(np.invert(my_image)) plt.axis(&#39;off&#39;) plt.title(&#39;Inverted (Bitwise NOT; twos-complement)&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Said another way... . 255 - my_image[100,100,:] == np.invert(my_image)[100,100,:] #we are in 256-bit color. . Array([ True, True, True]) . plt.imshow(np.fliplr(my_image)) plt.axis(&#39;off&#39;) plt.title(&#39;Flipped (LR)&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; This leads to some intuitive operations. For instance, if we were looking to feature-engineer such image data for use in a statistical learning classifier, we may hypothesize the location of the fovea (bright central spot) as a useful aspect of the retinopathy image. . I&#39;m betting we can segment these features with a single line of numpy. For this and subsequent examples, let&#39;s take a sample of the first sheet (zeroth index) of the image for simplicity such that we have a 2-d array (pretend we read-in a greyscale image). . x = my_image[:,:,0] #take all elements of the first sheet/leaf of the array. plt.imshow(x) plt.axis(&#39;off&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Now for the single line. We will find the bright portion by the assumption that we can find it by taking $region = frac{ sum_i x_i}{N} * 2 * sigma( vec{x})$, or all pixels with intensity greater than equal to two standard deviations above the mean. . x = my_image[:,:,0] #take all elements of the first sheet/leaf of the array. mask = (x &gt;= x.mean() + 2*x.std()) . plt.imshow(mask) plt.axis(&#39;off&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; We can then get the coordinates in the array simply: . coords = np.argwhere(x == mask) coords . array([[ 0, 0], [ 0, 1], [ 0, 2], ..., [223, 221], [223, 222], [223, 223]]) . print(coords.size) . 20656 . If we knew a priori that a particular type of retinopathy was characterized by abnormal foveal locations, and we had a sufficient train/test/validation dataset, we could reduce the size of our dataset significantly with such an engineered feature. . print(f&#39;If we only require foveal coordinates, our dataset may be reduced by {round(100-(coords.size/my_image.size)*100,2)}% !&#39;) . If we only require foveal coordinates, our dataset may be reduced by 86.28% ! . In Closing... . I hope this example was helpful in showing how we can quickly prepare complex data types for computational purposes. I will return to abstract bioimaging processing in future posts. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/tensors.html",
            "relUrl": "/jupyter/2020/12/31/tensors.html",
            "date": " • Dec 31, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "It's better to be evil than boring",
            "content": "Background . Healthcare is, apart from university tuition, one of the more rapidly inflating services on the market. Inconvenient as it might be, much of the reason behind the crushing costs of paying for healthcare has little to do with what you might expect. The price of the cancer drug crizotinib was actually developed on a shoestring, judging by the name. And yeah, it stings to think about how our copayments trickle their way into the Mercedes-AMG option sheet between some surgeon&#39;s sweaty thumbs, but there aren&#39;t that many surgeons. In reality, a large reason behind cost inflation in healthcare has nothing to do with the evil stuff. It has to do with the boring stuff. . In this post, I will look at a major chapter of the administrative nightmare of healthcare: manual medical coding. We will quickly see how a large cost center in administration, medical coding, will likely be totally automated in the near future. My hope is that you will be left wondering how this is still possibly done largely by hand, and perhaps more optimistic about the future of healthcare. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/boring.html",
            "relUrl": "/jupyter/2020/12/31/boring.html",
            "date": " • Dec 31, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "(simple) Autoencoders ≡ PCA",
            "content": "Background . Shocker: The simplest autoencoder is actually the same as PCA. We will also see how VAEs perform expectation maximization. Autoencoders learn a meaningful representation of a signal. Using an encoder/decoder pair, autoencoders work by reconstructing a latent encoded representation of an original signal. By minimizing the loss between the original signal and the decoded latent representation, an encoder network can be trained to parse an instance of a dataset for its most meaningful features. . Sound familiar? That is precisely the goal of computing the principal components analysis (PCA) of a matrix. It turns out that autoencoders, by construction are exactly PCA. . The Simplest Autoencoder . Let a neural network be defined with a single hidden unit $W^T sigma(f(W vec{X}))$, with a linear activation function $ sigma$ (for this example). Let the weights on the encoder layer be denoted $W$. . We thus define a decoder to use weights $W^T$, and final outputs of the network should converge to a reconstruction of the original features $ vec{X}$ once properly trained. That is, our network should produce $ hat{ vec{X}}$ from $ vec{X} to vec{y} = WX to z = sigma(f(y))$ and finally $ hat{ vec{X}} = W^Tz$. . If we train by minimizing the $L_2$ divergence between $( vec{X}, hat{ vec{X}})$, we have an autoencoder, but we also learn the principle components of $ vec{X}$: . $$ hat{x} = w^Twx; div( hat{x},x) = |x- hat{x} |^2 = |x - w^TWx |^2 $$ $$ to via backprop to hat{w} = arg min_w E left[ | x-w^TWx |^2 right]$$ . This is equivalent to discovering a basis that spans the principle components of the input, as we discover the directions of maximum energy, equal to the variance of $X$ if $X$ is a zero-mean random variable. In other words, we find a linear mapping of the original features to the principle axis of the data. . Finally, if the learned decoder weight is not the same weight as the input weight (i.e., $U^T not = W^T$), we still learn a hidden representation $z$ that lies along the major axis of the input $X$. The minimum error direction here is by definition the pinciple eigen vector of $X$. . We could then find a useful component of $X$ (described perhaps by our training process or assumptions)by then projecting the eigen vector onto $X$. Again, if $U^T = W^T$, we arrive at the principle component(s) of $X$. . Of course, this is a roundabout method of obtaining principle components, but I hope it shows the rigorous grounding and versatility of perceptrons in learning representations of data. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/autoencoders.html",
            "relUrl": "/jupyter/2020/12/31/autoencoders.html",
            "date": " • Dec 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Originally from a small town in northern California, I completed my B.A. in biological sciences with a minor in mathematical finance at the University of Southern California. . I then spent about a year in consulting and briefly worked at a winery. I also devoted a considerable amount of time and effort on the law school admissions test (LSAT) but realized the profession was not for me. Instead, I chose to return toward technical topics and decided to pursue the master’s program in computational biology at Carnegie Mellon University. . Since then, I’ve worked toward mastering an understanding of statistical and deep learning methods as they relate to the biosciences. Some of my favorite coursework includes 02-710: Computational Genomics, and the infamous 11-785: Deep Learning. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://simonlevine.github.io/simonsays/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://simonlevine.github.io/simonsays/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}