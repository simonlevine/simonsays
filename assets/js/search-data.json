{
  
    
        "post0": {
            "title": "Active Learning via Membership Query Synthesis for Binary Classification",
            "content": "Background . This is an implementation of . Active Learning via Query Synthesis and Nearest Neighbour Search, by Liantao Wang, Xuelei Hu, Bo Yuan, Jianfeng Lu . http://dx.doi.org/10.1016/j.neucom.2014.06.042 . As you&#39;ll see, this fits the ModAL API fairly well. . from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split import numpy as np X, y = make_classification(n_samples=3000,n_features=20, n_redundant=2, n_informative=10,random_state=1, n_clusters_per_class=2) rng = np.random.RandomState(2) X += .5 * rng.uniform(size=X.shape) X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size=0.8, random_state=42) # X_unlabeled, X_labeled, y_unlabeled, y_labeled = train_test_split(X_train, y_train, test_size=0.15, random_state=42) # 0.25 x 0.8 = 0.2 clf = LogisticRegression(random_state=0).fit(X_train, y_train) clf.score(X_test,y_test) . 0.8670833333333333 . from sklearn.neighbors import NearestCentroid from sklearn.neighbors import NearestNeighbors from tqdm.auto import tqdm, trange import numpy as np from sklearn.linear_model import LogisticRegression . def initialize_opposite_pair(X_labeled:np.ndarray,y_labeled:np.ndarray,verbose=False): &#39;&#39;&#39; Initialize an opposite pair about the decision boundary based on binary class membership and nearest centroids. &#39;&#39;&#39; clf = NearestCentroid() #to get initial centroids for opposite pairing. clf.fit(X_labeled, y_labeled) opposite_pair = dict(zip(clf.classes_,clf.centroids_)) if verbose: print(f&#39;Initial opposite pair found as: n{opposite_pair}&#39;) return opposite_pair . def get_perp_query(opposite_pair, magnitude:float = 1.0) -&gt; np.ndarray : &#39;&#39;&#39; Synthesize the midperpendicular query given the opposite pair. Per the paper: &quot;This new synthesis can be obtained by adding an orthogonal vector to the mid-point. We can find the vector by orthogonalizing a random vector using Gram-Schmit process and normalize its magnitude to Œª.&quot; &#39;&#39;&#39; x_pos,x_neg = tuple(opposite_pair.values()) x_naught = x_pos - x_neg x_r = np.random.rand(*x_naught.shape) # Use Gram-Schmit process to make x_r orthogonal to x_o x_r = x_r - np.dot(x_r,x_naught) / np.dot(x_naught,x_naught)*x_naught # set magnitude x_r = magnitude/np.linalg.norm(x_r)*x_r x_s = x_r + (x_pos+x_neg)/2 return x_s . def query_instance(x_s,X_labeled,y_labeled,X_unlabeled,y_unlabeled=None): &#39;&#39;&#39; Query the oracle, or draw from the &quot;unlabeled&quot; label pool. &#39;&#39;&#39; nn = NearestNeighbors(n_neighbors=1) nn.fit(X_unlabeled) #want to find nearest neighbor of x_s within unlabeled pool _, idx = nn.kneighbors(x_s.reshape(1,-1)) idx=idx.squeeze() x_q = X_unlabeled[idx] #query x_q for y_q if not isinstance(y_unlabeled, np.ndarray): y_q = ask_oracle(x_q,possible_labels=np.unique(y_labeled),label_type=&#39;int&#39;) else: y_q=y_unlabeled[idx] y_unlabeled = np.delete(y_unlabeled,idx,0) #delete off the used value if matrix is present X_labeled = np.vstack((X_labeled,x_q)) try: y_labeled = np.vstack((y_labeled,y_q)) except: #if label shape cannot be efficiently stacked. y_labeled = np.append(y_labeled.flatten(),y_q) X_unlabeled= np.delete(X_unlabeled,idx,0) data = (X_labeled,y_labeled,X_unlabeled,y_unlabeled) return x_q,y_q,data . def find_first_pair(X_labeled:np.ndarray,y_labeled:np.ndarray,X_unlabeled:np.ndarray,y_unlabeled=None,b:int = 64, opposite_pair=None): &#39;&#39;&#39; Find a new opposite pair with ùëè bits of precision (unless not enough instances remain) via binary search about the hyperplane. If opposite_pair is not provided, initialize it with centroids of labeled set. &#39;&#39;&#39; if opposite_pair==None: opposite_pair=initialize_opposite_pair(X_labeled,y_labeled,verbose=False) # &quot;positive&quot; and &quot;negative&quot; in the paper just refers to first and second label data=(X_labeled,y_labeled,X_unlabeled,y_unlabeled) while b and X_unlabeled.shape[0]&gt;0: #synthesize a new instance x_pos,x_neg=tuple(opposite_pair.values()) x_s = (x_pos+x_neg)/2 x_q,y_q,data = query_instance(x_s, *data) opposite_pair[y_q]=x_q # &quot; if x_q is positive (y_q is 0) then x_+ ‚Üê x_q. else (y_q is 1), x_‚àí ‚Üê x_q b-=1 #decrement the remaining precision return opposite_pair, data . def ask_oracle(x_q,possible_labels,label_type:str=&#39;int&#39;): &#39;&#39;&#39; Ask an oracle (via query command line input; int or float) for a suitable label to x_q. &#39;&#39;&#39; y_q=None while not y_q: # Ask the user for a label. oracle_label = input(f&quot;The possible labels for this instance should be from n {possible_labels}. n Please provide a suitable label for {x_q}: &quot;) if label_type==&#39;int&#39;: return int(y_q) elif label_type==&#39;float&#39;: return float(y_q) else: return y_q . def active_learning(learner, X_labeled:np.ndarray, y_labeled:np.ndarray, X_unlabeled:np.ndarray, y_unlabeled: np.ndarray, n_queries:int = 200, b:int = 64, magnitude:float = 1.0, X_test=None, y_test=None): &#39;&#39;&#39; Perform an active learning loop. &#39;&#39;&#39; #get centroids of data set per class member initial_opposite_pair = initialize_opposite_pair(X_labeled,y_labeled,verbose=False) opposite_pair,data = find_first_pair( X_labeled=X_labeled,y_labeled=y_labeled,X_unlabeled=X_unlabeled,y_unlabeled=y_unlabeled,b=b,opposite_pair=initial_opposite_pair) history=[] n = n_queries - b #number instances minus the precision of the initialization. for _ in trange(n): if X_unlabeled.shape[0]==0: break # iterate for (number of desired points to use from pool) minus (bits used in initial search). x_s = get_perp_query(opposite_pair,magnitude) x_q,y_q,data= query_instance(x_s, *data) X_labeled,y_labeled,X_unlabeled,y_unlabeled = data learner.fit(X_labeled,y_labeled) history.append(learner.score(X_test,y_test)) return learner,history . from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification import numpy as np from sklearn.model_selection import train_test_split . %%time X, y = make_classification(n_samples=3000,n_features=20, n_redundant=2, n_informative=10, random_state=1, n_clusters_per_class=2) rng = np.random.RandomState(2) X += .5 * rng.uniform(size=X.shape) X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size=0.8, random_state=42) X_unlabeled, X_labeled, y_unlabeled, y_labeled = train_test_split(X_train, y_train, test_size=0.15, random_state=42) # 0.25 x 0.8 = 0.2 clf=RandomForestClassifier() learner,history_rf=active_learning(clf, X_labeled,y_labeled,X_unlabeled,y_unlabeled, n_queries=464, b=64,X_test=X_test,y_test=y_test) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [01:03&lt;00:00, 6.27it/s]CPU times: user 1min 3s, sys: 869 ms, total: 1min 3s Wall time: 1min 3s . history_rf[-1] . 0.9029166666666667 . %%time X, y = make_classification(n_samples=3000,n_features=20, n_redundant=2, n_informative=10,random_state=1, n_clusters_per_class=2) rng = np.random.RandomState(2) X += .5 * rng.uniform(size=X.shape) X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size=0.8, random_state=42) X_unlabeled, X_labeled, y_unlabeled, y_labeled = train_test_split(X_train, y_train, test_size=0.15, random_state=42) # 0.25 x 0.8 = 0.2 from sklearn.linear_model import LogisticRegression clf=LogisticRegression() learner,history_lr=active_learning(clf, X_labeled,y_labeled,X_unlabeled,y_unlabeled, n_queries=464, b=64,X_test=X_test,y_test=y_test) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:03&lt;00:00, 118.73it/s]CPU times: user 22.1 s, sys: 4.59 s, total: 26.7 s Wall time: 3.41 s . import copy X, y = make_classification(n_samples=3000,n_features=20, n_redundant=2, n_informative=10,random_state=1, n_clusters_per_class=2) rng = np.random.RandomState(2) X += .5 * rng.uniform(size=X.shape) X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size=0.8, random_state=42) n_initial = 100 initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False) X_initial, y_initial = X_train[initial_idx], y_train[initial_idx] X_pool=copy.deepcopy(X_train) y_pool=copy.deepcopy(y_train) X_pool=np.delete(X_pool,initial_idx,axis=0) y_pool=np.delete(y_pool,initial_idx,axis=0) def random_sampling(classifier, X_pool): n_samples = len(X_pool) query_idx = np.random.choice(range(n_samples)) return query_idx, X_pool[query_idx] . from modAL.models import ActiveLearner learner = ActiveLearner( estimator=RandomForestClassifier(), query_strategy=random_sampling, X_training=X_initial, y_training=y_initial ) n_call_to_oracle=400 history_baseline_rf = [] for _ in trange(n_call_to_oracle): query_idx, _ = learner.query(X_pool) learner.teach( X_pool[query_idx].reshape(1,-1), y_pool[query_idx].reshape(1,)) X_pool = np.delete(X_pool, query_idx, axis=0) y_pool = np.delete(y_pool, query_idx, axis=0) r2 = learner.score(X_test,y_test) history_baseline_rf.append(r2) . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:58&lt;00:00, 6.85it/s] . X, y = make_classification(n_samples=3000,n_features=20, n_redundant=2, n_informative=10,random_state=1, n_clusters_per_class=2) rng = np.random.RandomState(2) X += .5 * rng.uniform(size=X.shape) X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size=0.8, random_state=42) n_initial = 100 initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False) X_initial, y_initial = X_train[initial_idx], y_train[initial_idx] X_pool=copy.deepcopy(X_train) y_pool=copy.deepcopy(y_train) X_pool=np.delete(X_pool,initial_idx,axis=0) y_pool=np.delete(y_pool,initial_idx,axis=0) learner = ActiveLearner( estimator=LogisticRegression(), query_strategy=random_sampling, X_training=X_initial, y_training=y_initial ) n_call_to_oracle=400 history_baseline_lr = [] for _ in trange(n_call_to_oracle): query_idx, _ = learner.query(X_pool) learner.teach( X_pool[query_idx].reshape(1,-1), y_pool[query_idx].reshape(1,)) X_pool = np.delete(X_pool, query_idx, axis=0) y_pool = np.delete(y_pool, query_idx, axis=0) r2 = learner.score(X_test,y_test) history_baseline_lr.append(r2) . &lt;AxesSubplot:&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-04-26T20:03:42.352736 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ array([&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;], dtype=object) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-04-26T20:03:36.864049 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/04/21/mqs-clf.html",
            "relUrl": "/jupyter/2021/04/21/mqs-clf.html",
            "date": " ‚Ä¢ Apr 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Purines, Pyrimidines, and Convolutional Neural Networks",
            "content": "Background . This notebook is adapted from the CS-185, Spring 2021 course at Brown University (http://cs.brown.edu/courses/csci1850) . Biology . Transcription Factors (TFs) are an important class of protein that bind to sections of DNA. This binding process is mediated by their propensity to attach to particular sequence motifs, or patterns in the DNA sequence. Often, we humans refer to binding as important regulatory regions, as associated transcription factors play a large role in upregulating and/or downregulating sequence transcription. In other words, finding the location of such motifs on a given sequence allows us to glean a functional understanding from structure alone. If an algorithm can be trained to detect relevant motifs given a cluster with high fidelity, then the algorithm could obviate hand-tuned feature identification, and even predict likely TF binding sites from unknown sequences. In fact, this is the exact problem we will solve in this exercise. . Dataset . The small dataset provided actually is simulated homotypic motif density localization data. For more about how to generate such data, see the Kundaje Lab&#39;s DragoNN (http://kundajelab.github.io/dragonn/), and if you want, try to beat their results with, perhaps, the GAN from my previous post üòâ. The nice thing about this dataset is that it is already formatted for easy loading. Each &quot;sequence&quot; is actually a $1500-mer$, and, given their in silico provenance, we can assume üåü perfect coverage üåü and üå∏ no sequencing errors üå∏ for the purpose of this exercise. We can take a peek at the training split below. . #Get the data. . import numpy as np import pandas as pd data=np.load(&#39;./data_dna_cnn/hw1_data.npz&#39;) train_seq = data[&#39;train_seq&#39;] train_y = data[&#39;train_y&#39;] train_df = pd.DataFrame([train_seq,train_y]).T.rename(columns=dict(zip([0,1],[&#39;X&#39;,&#39;y&#39;]))) train_df.head(10) . X y . 0 TGTTACGCTATCGCGAACAGTATGTTCAAGTGGGCGTTCGCTAGGA... | [True] | . 1 CAAACTTCTTCAAAGGATCCCAGATGCGGTTAAATCATACCACCAA... | [True] | . 2 GACCATGCCTTGAGGCGTATACATTTGTTGTAGTTGAATTACTACG... | [True] | . 3 AGTATTACGATCATAGAGTATCAGAGTTGTTTTGTAACAGCGTCGG... | [False] | . 4 CTTCAGATACGTAATTACTCTATAATTTACAAACAACCACGTATCC... | [False] | . 5 AGGAGTTTCTATTTCAGAAGTGAACGGTGACAAAAAGTCCACTCAA... | [True] | . 6 GGGTCGAAAAGTATGACGATCCAATCATTAGAAGTGAGGTTCACCT... | [False] | . 7 AATATTTAATTTATCTCCGACCAGTCAAAGATACTGTCTTGAACTG... | [True] | . 8 GGCGCCTTTTTTCTCAAAAGAAAGTGATGTTGTTTTGCATTTCTAT... | [True] | . 9 TCGCCGGATGGACTGCTACCTTGTTGGACTAATGGTATATTTTTCC... | [True] | . np.unique([len(x) for x in train_df.X]) . array([1500]) . import matplotlib.pyplot as plt plt.style.use(&#39;dark_background&#39;) train_df.y.astype(int).plot(kind=&#39;hist&#39;,title=f&#39;{train_df.y.astype(bool).value_counts()}&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;True 2010 nFalse 1990 nName: y, dtype: int64&#39;}, ylabel=&#39;Frequency&#39;&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-03-04T22:10:30.203093 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ We see an equal distribution of positive (True) and negative (False) $1500$-mer training instances (a nice thing to have, but not something we can necessarily count on in the real world!). In fact, the same class proportion and sequence length holds true for the test and validation splits (the former being especially unlikely). . Machine Learning . We aim to build a classifer to predict functional motifs given sequences. Formally, we learn a function $f(X)= vec{y}$ that, given the nature of deep learning networks, fits with non-linearity a probability density function $f=P(y|X; theta)$. The task is to learn a weights matrix $W subset theta$ (a subset since our network may be more complicated) approximating the true class posterior distribution, such that when we call $f(x_{novel})$, we get $ hat{y} approx y_{true}$, where $y in {0,1 } = { text{motif,non-motif} }$. . Task: . We aim to train a CNN to classify the sequences as positive or negative for homotypic motif clusters. A CNN should be a good choice due to the natural shift-invariance of the architecture. When used on images, CNNs are feature-aware but, importantly, location-agnostic. We want the same approach to motif finding since such subsequences may appear anywhere in the $1500$-mer instances. . One-Hot Basic CNN: . - CNNs on static one-hot embedded nucleotides. - 1 Convolution layer (+ nonlinearity + pooling) and 1 dense (`linear` + activation) output layer. - Shift-invariance of the CNN is the main point here. - Train the model using stochastic gradient descent and binary cross-entropy loss. . Embedding with CNN: . - Use of a learnable embedding on integer-mapped DNA sequences together with the same downstream architecture. . Transformer: . - Akin to NLP, we can treat each nucelotide like a token in a sentence. - Train an XLNet (with self-attention and all the rest) model with a linear head to predict presence (0/1) of motifs. - This particular model is useful since we have somewhat lengthy instances for a transformer (1500-mers). . I will begin by defining the requisite PyTorch Dataset and Dataloader classes, as well as an extensible LightningModule for use across models. . . import os import torch from torch import nn from torch.nn import functional as F from torch.utils.data import Dataset, DataLoader, random_split from torchvision import transforms import pytorch_lightning as pl from pytorch_lightning.metrics.functional import accuracy from transformers import AutoConfig, AutoModelForSequenceClassification #the latter includes a linear head. . The Dataset class is now defined with associated preprocessing methods: . class DNADataset(Dataset): &quot;&quot;&quot;DNA / Pytorch dataset.&quot;&quot;&quot; def __init__(self, X, y, encoding=&#39;one-hot&#39;): &quot;&quot;&quot; Args: data (np.array): Path to the url .npz file with annotations. &quot;&quot;&quot; self.X = torch.FloatTensor(self.one_hot_encoding(X)) if encoding==&#39;one-hot&#39; else torch.LongTensor(self.embedding_encoding(X)) self.y = torch.FloatTensor(y) #NOTE: we need this to be a float to accommodate nn.BCEWithLogitsLoss() later to match logit input. def __len__(self): return len(self.X) def __getitem__(self, idx): sample = {&#39;X&#39;: self.X[idx], &#39;y&#39;: self.y[idx]} return sample def one_hot_encoding(self,seq_array:np.ndarray) -&gt; np.ndarray: &quot;&quot;&quot; :param seq_array: np array of DNA sequences :return: np array of one-hot encodings of input DNA sequences of shape N,S,M, where N is the number of instances, S is the cardinality of the corpus, and M is the length of each sequence. &quot;&quot;&quot; nuc2id = {&#39;A&#39; : 0, &#39;C&#39; : 1, &#39;T&#39; : 2, &#39;G&#39; : 3} N = len(seq_array) S = len(nuc2id.keys()) M = len(seq_array[0]) #since we know each sequence is the same length onehot_array = np.zeros((N, S, M)) print(&#39;Nucelotides -&gt; One-Hot&#39;) for seq_num, seq in enumerate(seq_array): for seq_idx, nucleotide in enumerate(seq): nuc_idx = nuc2id[nucleotide] onehot_array[seq_num, nuc_idx, seq_idx] = 1 return onehot_array def embedding_encoding(self, seq_array: np.ndarray) -&gt; np.ndarray: &quot;&quot;&quot; :param seq_array: np array of DNA sequences :return: np array of integer encodings of input DNA sequences &quot;&quot;&quot; nuc2id = {&#39;A&#39; : 0, &#39;C&#39; : 1, &#39;T&#39; : 2, &#39;G&#39; : 3} N = len(seq_array) M = len(seq_array[0]) #since we know each sequence is the same length int_array = np.full((N,M),0) print(&#39;Nucleotides -&gt; Integers&#39;) for seq_num, seq in enumerate(seq_array): for seq_idx, nucleotide in enumerate(seq): nuc_idx = nuc2id[nucleotide] int_array[seq_num, seq_idx] = nuc2id[nucleotide] return int_array class DNADataModule(pl.LightningDataModule): def __init__(self, data_dir: str, batch_size:int=64,encoding=&#39;one-hot&#39;): super().__init__() self.batch_size = batch_size self.data_fpath = data_dir self.encoding=encoding def setup(self, stage=None): all_data = np.load(self.data_fpath) self.train = DNADataset(all_data[&#39;train_seq&#39;],all_data[&#39;train_y&#39;],self.encoding) self.val = DNADataset(all_data[&#39;train_seq&#39;],all_data[&#39;train_y&#39;],self.encoding) self.test = DNADataset(all_data[&#39;test_seq&#39;], all_data[&#39;test_y&#39;], self.encoding) def train_dataloader(self): return DataLoader(self.train, batch_size=self.batch_size) def val_dataloader(self): return DataLoader(self.val, batch_size=self.batch_size) def test_dataloader(self): return DataLoader(self.test, batch_size=self.batch_size) . dm_onehot = DNADataModule(data_dir=&#39;./data_dna_cnn/hw1_data.npz&#39;,encoding=&#39;one-hot&#39;) dm_onehot.setup() dm_int = DNADataModule(data_dir=&#39;./data_dna_cnn/hw1_data.npz&#39;,encoding=&#39;int&#39;) #for use with 2nd CNN and Transformer dm_int.setup() . Nucelotides -&gt; One-Hot Nucelotides -&gt; One-Hot Nucelotides -&gt; One-Hot Nucleotides -&gt; Integers Nucleotides -&gt; Integers Nucleotides -&gt; Integers . Model Instantiation . We can now define models to be instantiated in the main Pytorch LightningModule. . Basic CNN . Again, we define a convolutional neural net on static, one-hot embedded nucleotides. For now, I will use a single 1-Dimensional Convolution layer (+ nonlinearity + flattening) and one dense (linear + activation) output layer. This model will make use of the dm_onehot data module defined and instantiated above. . As you might remember from above, we one-hot encoded each sequence using a dictionay of nucleotides to integers, and then encoded a 4-channel NumPy binary array using the indices of this dictionary. When placed into the dataset class with a batch-size parameter, we can finally see what the network sees per &quot;minibatch&quot; of sequences. I say &quot;minibatch&quot; because technically each shuffled pass during a stochastic gradient descent epoch is commonly referred to as a batch. . Please see https://colab.research.google.com/github/Avsecz/DL-genomics-exercise/blob/master/Simulated.ipynb#scrollTo=MYVEEl0zZwlr for the Keras model I&#39;ve based my implementation on. . At any rate, the model views a tensor from the dataloader of the following shape (note this is also a nice way to access a sample instance from the dataloader directly): . x_sample_onehot = next(iter(dm_onehot.test_dataloader()))[&#39;X&#39;] x_sample_onehot . tensor([[[1., 1., 1., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], [0., 0., 0., ..., 0., 0., 1.], [0., 0., 0., ..., 1., 0., 0.]], [[0., 0., 0., ..., 1., 0., 0.], [0., 0., 0., ..., 0., 0., 1.], [1., 1., 0., ..., 0., 1., 0.], [0., 0., 1., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 1., 1.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [1., 1., 1., ..., 1., 0., 0.]], ..., [[0., 0., 0., ..., 0., 1., 1.], [0., 0., 0., ..., 0., 0., 0.], [1., 1., 1., ..., 1., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[1., 0., 0., ..., 1., 1., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 1., ..., 0., 0., 1.]], [[0., 1., 0., ..., 0., 0., 1.], [0., 0., 0., ..., 0., 0., 0.], [1., 0., 1., ..., 1., 1., 0.], [0., 0., 0., ..., 0., 0., 0.]]]) . x_sample_onehot.shape #batch_size, channels, length . torch.Size([64, 4, 1500]) . So, we have a fixed one-hot tensor of shape $(N,C,L)$, just like the Pytorch nn.Conv1d will expect. Note that we use $1$-D here since we have sequential data. . class DNACNN(nn.Module): def __init__(self): super(DNACNN, self).__init__() self.net=nn.Sequential( nn.Conv1d(in_channels=4, out_channels=16, kernel_size=16), # -&gt; [64, 16, 1485] nn.ReLU(), nn.AdaptiveMaxPool1d(16), #-&gt; [64, 16, 16] nn.Flatten(), #-&gt; [64,256] nn.Linear(256,1), # -&gt; [64,1] , a single logit output per instance. # nn.Sigmoid() -&gt; no need for final activation since we will use nn.BCEWithLogitsLoss() ) def forward(self, x): x = self.net(x) return x . CNN with Trainable Embeddings . Now, for the embedding case, I will essentially &quot;recreate&quot; a learnable one-hot embedding. That is, we can directly compare the outcome of a learnable nn.embedding versus a fixed one-hot vector per sequence minibatch by being creative with dimensions. . x_sample_int = next(iter(dm_int.test_dataloader()))[&#39;X&#39;] x_sample_int.shape #batch_size, channels, length . torch.Size([64, 1500]) . x_sample_int_embedded = nn.Embedding(num_embeddings=64, embedding_dim=4)(x_sample_int).view(64,4,1500) x_sample_int_embedded.shape . torch.Size([64, 4, 1500]) . Note that this looks identical to the one-hot encoding. However, we can take a look at the values: . x_sample_int_embedded . tensor([[[ 0.4570, -2.0863, 1.1360, ..., -0.5568, 0.0075, -0.0272], [ 0.4570, -2.0863, 1.1360, ..., 1.5261, 0.5930, 1.0185], [ 0.4570, -2.0863, 1.1360, ..., -2.0863, 1.1360, -0.5638], [ 0.0159, -1.1691, 1.2557, ..., -0.5568, 0.0075, -0.0272]], [[ 0.1240, -0.5568, 0.0075, ..., -0.5568, 0.0075, -0.0272], [ 0.4570, -2.0863, 1.1360, ..., -0.5568, 0.0075, -0.0272], [-0.9929, 1.5261, 0.5930, ..., -2.0863, 1.1360, -0.5638], [-0.9929, 1.5261, 0.5930, ..., -1.1691, 1.2557, -0.1304]], [[-0.9929, 1.5261, 0.5930, ..., -1.1691, 1.2557, -0.1304], [ 0.4570, -2.0863, 1.1360, ..., -2.0863, 1.1360, -0.5638], [-0.9929, 1.5261, 0.5930, ..., 1.5261, 0.5930, 1.0185], [ 0.1240, -0.5568, 0.0075, ..., -2.0863, 1.1360, -0.5638]], ..., [[ 0.1240, -0.5568, 0.0075, ..., 1.5261, 0.5930, 1.0185], [ 0.4570, -2.0863, 1.1360, ..., -1.1691, 1.2557, -0.1304], [-0.9929, 1.5261, 0.5930, ..., 1.5261, 0.5930, 1.0185], [ 0.4570, -2.0863, 1.1360, ..., -2.0863, 1.1360, -0.5638]], [[ 0.4570, -2.0863, 1.1360, ..., -0.5568, 0.0075, -0.0272], [ 0.1240, -0.5568, 0.0075, ..., 1.5261, 0.5930, 1.0185], [ 0.1240, -0.5568, 0.0075, ..., -1.1691, 1.2557, -0.1304], [-0.9929, 1.5261, 0.5930, ..., 1.5261, 0.5930, 1.0185]], [[ 0.1240, -0.5568, 0.0075, ..., 1.5261, 0.5930, 1.0185], [ 0.4570, -2.0863, 1.1360, ..., -2.0863, 1.1360, -0.5638], [ 0.0159, -1.1691, 1.2557, ..., -0.5568, 0.0075, -0.0272], [ 0.1240, -0.5568, 0.0075, ..., -2.0863, 1.1360, -0.5638]]], grad_fn=&lt;ViewBackward&gt;) . Note that the tensor is initially randomized float values. But, unlike the one-hot case, these values (this lookup table) will be trained/adjusted during backpropagation! . class DNACNNEmbedding(nn.Module): def __init__(self): super(DNACNNEmbedding, self).__init__() self.embedding = nn.Embedding(num_embeddings=64, embedding_dim=4) self.net=nn.Sequential( nn.Conv1d(in_channels=4, out_channels=16, kernel_size=16), # -&gt; [64, 16, 1485] nn.ReLU(), nn.AdaptiveMaxPool1d(16), #-&gt; [64, 16, 16] nn.Flatten(), #-&gt; [64,256] nn.Linear(256,1), # -&gt; [64,1] , a single logit output per instance. # nn.Sigmoid() -&gt; no need for final activation since we will use nn.BCEWithLogitsLoss() ) def forward(self, x): y = self.net(self.embedding(x).view(64,4,1500)) return y . The DNA Transformer! . Let&#39;s create a class using HuggingFace within an nn.module. . Note that for the Transformer, I will be pulling an untrained model from Huggingface. That involves invoking the config of the desired model, but instantiating the model itself from the config only (not from_pretrained). I think this is actually quite annoying and could warrant a pull request at a future date (there ought to be a flag like .from_untrained). . As for the tokenizer, we don&#39;t really need the actual model&#39;s tokenizer with associated vocabulary, etc., since we can just map each nucleotide to an integer. This should be equivalent to character-level tokenization of each nucleotide with an untrained vocabulary, but also allowed for the use of nn.Embedding() for the second CNN (detailed above). . For illustrative purposes: . model = AutoModelForSequenceClassification.from_config( AutoConfig.from_pretrained(&#39;xlnet-base-cased&#39;) ) model . XLNetForSequenceClassification( (transformer): XLNetModel( (word_embedding): Embedding(32000, 768) (layer): ModuleList( (0): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (1): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (2): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (3): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (4): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (5): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (6): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (7): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (8): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (9): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (10): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) (11): XLNetLayer( (rel_attn): XLNetRelativeAttention( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (ff): XLNetFeedForward( (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (layer_1): Linear(in_features=768, out_features=3072, bias=True) (layer_2): Linear(in_features=3072, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (dropout): Dropout(p=0.1, inplace=False) ) ) (dropout): Dropout(p=0.1, inplace=False) ) (sequence_summary): SequenceSummary( (summary): Linear(in_features=768, out_features=768, bias=True) (first_dropout): Identity() (last_dropout): Dropout(p=0.1, inplace=False) ) (logits_proj): Linear(in_features=768, out_features=2, bias=True) ) . The transformer model is currently instantiated for classification (model.logits_proj has out_features=2). Note that we will need to take some form of activation across these logits (sigmoid, or even plain argmax). We can use BCEWithLogits (binary cross entropy with logits) to accomplish this. . Note due to the nature of binary (versus multlabel) classification, that we could instead have a single neuron output and use sigmoidal activation (analagous to logistic regression). . The key difference here is that a single neuronal output will evaluate the probability of a single output together, rather than the probability of each class independently. By keeping the output dimensions as $2$, we may gain a small amount out capacity of the network (each output gets its own weight) that would be implicit otherwise. More formally, the VC dimension of a parameter-rich network is greater, however small. . Still, the dimensions of the label space need to pairwise match. Since labels are currently set up in the dataset with cardinality of 1 (scalar, versus a one-hot or other label encoding of its own), I will instead just refactor the model slightly to have a single output dimension. It&#39;s not clear what the advantage in this toy example would be to changing or keeping this tiny aspect of the architecture would be, at any rate. . model.logits_proj = nn.Linear(in_features=768,out_features=1,bias=True) model.logits_proj . Linear(in_features=768, out_features=1, bias=True) . Perfect! This will be incorporated below. . In short, this step allows us to create a black-box network while keeping all of the business of training each of these three models as similar as possible (same loss, same LightningModule, etc., and I&#39;m sure there are even more concise methods). . class DNATransformer(nn.Module): def __init__(self,transformer_checkpoint=&#39;xlnet-base-cased&#39;): super(DNATransformer, self).__init__() #Load untrained model using checkpointed config. self.config = AutoConfig.from_pretrained(transformer_checkpoint) self.model = AutoModelForSequenceClassification.from_config(self.config) self.model.logits_proj = nn.Linear(in_features=768,out_features=1,bias=True) def forward(self,x): return self.model(x) . Integration: The Training Module . Now, the three models defined above are ready to be invoked into during train, test, and inference. We can organize this under a LightningModule: . class DNALitModule(pl.LightningModule): def __init__(self,net:nn.Module): super().__init__() self.net = net self.criterion = nn.BCEWithLogitsLoss() #Sigmoid--&gt;BCE Loss included. def training_step(self, batch, batch_idx): # -- x, y = batch[&#39;X&#39;],batch[&#39;y&#39;] y_hat = self.net(x) #take the most probable index. loss = self.criterion(y_hat, y) self.log(&#39;train_loss&#39;, loss) return loss # -- def validation_step(self, batch, batch_idx): # -- x, y = batch[&#39;X&#39;],batch[&#39;y&#39;] y_hat = self.net(x) loss = self.criterion(y_hat, y) self.log(&#39;val_loss&#39;, loss) # -- def test_step(self, batch, batch_idx): # -- x, y = batch[&#39;X&#39;],batch[&#39;y&#39;] y_hat = self.net(x) loss = self.criterion(y_hat, y) self.log(&#39;test_loss&#39;, loss) # -- def configure_optimizers(self): optimizer = torch.optim.Adam(self.parameters(), lr=1e-3) return optimizer . Training . We are now ready to train the selection of models. . Datamodules ‚úîÔ∏è . | CNN ‚úîÔ∏è . | CNN with Embeddings ‚úîÔ∏è . | Transformer ‚úîÔ∏è . | . Time to train! . cnn_dm = dm_onehot cnn_emb_dm = transformer_dm = dm_int dna_cnn = DNALitModule(DNACNN()) dna_cnn_emb = DNALitModule(DNACNNEmbedding()) dna_transformer = DNALitModule(DNATransformer()) trainer_cnn = pl.Trainer(fast_dev_run=False,gpus=0,max_epochs=5) trainer_cnn_emb = pl.Trainer(fast_dev_run=True,gpus=0,max_epochs=5) trainer_transformer = pl.Trainer(fast_dev_run=True,gpus=0,max_epochs=5) . GPU available: False, used: False TPU available: None, using: 0 TPU cores GPU available: False, used: False TPU available: None, using: 0 TPU cores Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es). GPU available: False, used: False TPU available: None, using: 0 TPU cores Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es). . trainer_cnn.fit(model = dna_cnn, datamodule=cnn_dm) . | Name | Type | Params 0 | net | DNACNN | 1.3 K 1 | criterion | BCEWithLogitsLoss | 0 1.3 K Trainable params 0 Non-trainable params 1.3 K Total params 0.005 Total estimated model params size (MB) Epoch 0: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà | 63/126 [00:01&lt;00:01, 42.99it/s, loss=0.693, v_num=2] Validating: 0it [00:00, ?it/s] Epoch 0: 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 66/126 [00:01&lt;00:01, 44.31it/s, loss=0.693, v_num=2] Epoch 0: 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 79/126 [00:01&lt;00:00, 49.66it/s, loss=0.693, v_num=2] Epoch 0: 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 93/126 [00:01&lt;00:00, 54.90it/s, loss=0.693, v_num=2] Epoch 0: 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 107/126 [00:01&lt;00:00, 59.49it/s, loss=0.693, v_num=2] Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:01&lt;00:00, 64.68it/s, loss=0.693, v_num=2] Epoch 1: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà | 63/126 [00:01&lt;00:01, 46.28it/s, loss=0.686, v_num=2] Validating: 0it [00:00, ?it/s] Epoch 1: 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 70/126 [00:01&lt;00:01, 49.50it/s, loss=0.686, v_num=2] Epoch 1: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 84/126 [00:01&lt;00:00, 55.32it/s, loss=0.686, v_num=2] Epoch 1: 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 98/126 [00:01&lt;00:00, 60.48it/s, loss=0.686, v_num=2] Epoch 1: 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 112/126 [00:01&lt;00:00, 65.05it/s, loss=0.686, v_num=2] Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:01&lt;00:00, 68.75it/s, loss=0.686, v_num=2] Epoch 2: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà | 63/126 [00:01&lt;00:01, 46.30it/s, loss=0.638, v_num=2] Validating: 0it [00:00, ?it/s] Epoch 2: 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 70/126 [00:01&lt;00:01, 49.45it/s, loss=0.638, v_num=2] Epoch 2: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 84/126 [00:01&lt;00:00, 55.38it/s, loss=0.638, v_num=2] Epoch 2: 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 99/126 [00:01&lt;00:00, 61.02it/s, loss=0.638, v_num=2] Epoch 2: 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 114/126 [00:01&lt;00:00, 65.97it/s, loss=0.638, v_num=2] Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:01&lt;00:00, 69.39it/s, loss=0.638, v_num=2] Epoch 3: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà | 63/126 [00:01&lt;00:01, 46.66it/s, loss=0.509, v_num=2] Validating: 0it [00:00, ?it/s] Epoch 3: 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 75/126 [00:01&lt;00:00, 52.35it/s, loss=0.509, v_num=2] Epoch 3: 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 90/126 [00:01&lt;00:00, 58.41it/s, loss=0.509, v_num=2] Epoch 3: 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 105/126 [00:01&lt;00:00, 63.64it/s, loss=0.509, v_num=2] Epoch 3: 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 120/126 [00:01&lt;00:00, 68.29it/s, loss=0.509, v_num=2] Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:01&lt;00:00, 69.46it/s, loss=0.509, v_num=2] Epoch 4: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà | 63/126 [00:01&lt;00:01, 47.62it/s, loss=0.37, v_num=2] Validating: 0it [00:00, ?it/s] Epoch 4: 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 75/126 [00:01&lt;00:00, 53.08it/s, loss=0.37, v_num=2] Epoch 4: 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 90/126 [00:01&lt;00:00, 58.86it/s, loss=0.37, v_num=2] Epoch 4: 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 105/126 [00:01&lt;00:00, 63.93it/s, loss=0.37, v_num=2] Epoch 4: 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 120/126 [00:01&lt;00:00, 68.72it/s, loss=0.37, v_num=2] Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:01&lt;00:00, 70.22it/s, loss=0.37, v_num=2] Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:01&lt;00:00, 69.97it/s, loss=0.37, v_num=2] . 1 . trainer_cnn_emb.fit(model = dna_cnn_emb, datamodule=cnn_emb_dm) . | Name | Type | Params 0 | net | DNACNNEmbedding | 1.6 K 1 | criterion | BCEWithLogitsLoss | 0 1.6 K Trainable params 0 Non-trainable params 1.6 K Total params 0.006 Total estimated model params size (MB) Epoch 0: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà | 1/2 [00:00&lt;00:00, 4.81it/s, loss=0.697] Validating: 0it [00:00, ?it/s] Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00, 9.00it/s, loss=0.697] Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00, 8.90it/s, loss=0.697] . 1 . . These models range in complexity greatly, and my hope is this will serve as a future-proof skeleton for a number of implementations. Other variants of CNNs, RNNs, and other Pytorch objects should be facile to swap and implement, and HuggingFace will likely serve as a choice deep NLP API for some time. . # model = AutoModelForSequenceClassification.from_config(config) # model.logits_proj = nn.Linear(in_features=768,out_features=1,bias=True) .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/02/17/DNA_CNN.html",
            "relUrl": "/jupyter/2021/02/17/DNA_CNN.html",
            "date": " ‚Ä¢ Feb 17, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Graph Neural Nets for Molecular Property Prediction",
            "content": "Background and Intuition . VantAI Prediction Challenge, 2021 . &quot;The goal of this Challenge is to predict lipophilicity, which is the ability of a chemical compound to dissolve in fats. Given that lipophilicity plays a key role in drug uptake and metabolism, it is an important physiochemical parameter for which prediction based on chemical structure could have a significant impact on drug discovery programs. Lipophilicity prediction is also a common task in machine learning prediction challenges.&quot; . In other words, we are to learn a function $f(x) = y$ where $x$ is the SMILES representation of a given chemical structure, and $y$ is the $ log d$ value of lipophilicity of the molecule. . Unless otherwise noted, such as in the case of open-source packages, I certify that all work is entirely my own. . Simon Levine-Gottreich . M.S. Computational Biology ,&#39;21 . Carnegie Mellon University . Graph Neural Nets . Per literature, GNNs seem to outperform classical statistical learning methods on benchmark molecular structure. Though some controversy exists about generalizability, I will proceed with A GNN. First, I will built a dataloader using pytorch-geometric and train and evaluate a simple GCN per the package tutorial. This should give a good baseline. . I will proceed to then use a more complex graph neural net framework via Principal Neighbourhood Aggregation (arxiv.org/abs/2004.05718). . Finally, in the interest of the competition (this is sometimes a no-no when data is limited and specific), I will augment the existing dataset with a second $SMILES to log d_{ text{lipophilicity}}$ Stanford&#39;s MoleculeNet Lipophilicity regression (http://moleculenet.ai/datasets-1) dataset. . I will ensure that any common SMILES structures are eliminated from this augmented dataframe. Ideally, this will allow for the model to better generalize to the test set, as it was described by the challenge authors as . EDA, Preprocessing, and Augmentation . I will begin with some exploratory analysis of the dataset as well as show how conversion from SMILES to a graph object will occur under the hood. . import pandas as pd train_df = pd.read_csv(&#39;train/raw/60170fff5b630f3433c202be_train.csv&#39;) test_df = pd.read_csv(&#39;test/raw/60170fffa2720fa4d0b9067a_holdout_set.csv&#39;) eval_df = pd.read_csv(&#39;alt/raw/Lipophilicity.csv&#39;, usecols=[&#39;exp&#39;,&#39;smiles&#39;]).rename(columns={&#39;smiles&#39;:&#39;Smiles&#39;,&#39;exp&#39;: &#39;label&#39;}) train_df . Smiles label . 0 CC(C)Oc1cc(Oc2ccc(S(C)(=O)=O)cc2)cc(-c2ncc(Cl)... | 2.00 | . 1 CNS(=O)(=O)c1ccc(Oc2cc(OC(C)C)cc(-c3nccc(=O)[n... | 2.60 | . 2 CC(C)Oc1cc(Oc2cnc(C(=O)N(C)C)c(F)c2)cc(-c2nccc... | 1.50 | . 3 CC(C)Oc1cc(Oc2cnc(C(=O)N(C)C)cn2)cc(-c2nccc(=O... | 1.30 | . 4 CCC(CC)Oc1cc(Oc2cnc(C(=O)N(C)C)nc2)cc(-c2nccc(... | 1.90 | . ... ... | ... | . 3824 CCC(C)C(N)C(=O)NC(C(=O)NCC(=O)NC(Cc1ccccc1)C(=... | -0.99 | . 3825 CCC(C)C(N)C(=O)NC(C)C(=O)NC(C)C(=O)NC(C(=O)O)C... | -2.82 | . 3826 NC(Cc1ccccc1)C(=O)NC(Cc1ccccc1)C(=O)NCC(=O)NC(... | 0.17 | . 3827 CC(C)CC(NC(=O)C(NC(=O)C(CC(C)C)NC(=O)C(N)C(C)C... | -1.23 | . 3828 CC(C)CC(NC(=O)C(N)Cc1c[nH]c2ccccc12)C(=O)NC(CC... | 0.23 | . 3829 rows √ó 2 columns . train_df[&#39;label&#39;].plot.hist(title=&#39;Training Data - Lipophilicity Frequency&#39;,alpha=0.50) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fefe2942210&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; eval_df[&#39;label&#39;].plot.hist(title=&#39;Extra Data - Lipophilicity Frequency&#39;,alpha=0.50) #note column names differ... . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fefe4f5fe10&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Interesting! We see a quasi-Guassian pattern in these continuous labels. I&#39;ll now convert the representations from SMILES to the graphical scruture needed (note the dataloader to come will do this automatically.) . But first, I&#39;d like to check on the data quality. Are there any identical SMILES? . sum(train_df[&#39;Smiles&#39;].value_counts()!=1) . 26 . Yes! 26. I won&#39;t filter this yet but I&#39;ll keep it in mind. . Anyway, onto converting the SMILES to graphs! . import rdkit from tqdm.notebook import trange, tqdm from ogb.utils import smiles2graph tqdm.pandas(&#39;Converting SMILES to molecular graph...&#39;) train_df[&#39;graph&#39;] = train_df.progress_apply(lambda row: smiles2graph(row[&#39;Smiles&#39;]),axis=1) . . test_df[&#39;graph&#39;]=test_df.progress_apply(lambda row: smiles2graph(row[&#39;Smiles&#39;]),axis=1) eval_df[&#39;graph&#39;]=eval_df.progress_apply(lambda row: smiles2graph(row[&#39;Smiles&#39;]),axis=1) . . row = train_df.sample(1) row . Smiles label graph . 1027 COc1ccc(NC(=O)c2ccc(-c3ccncc3)c(C)c2)cc1N1CCN(... | 2.56 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . print(row[&#39;graph&#39;].values[0]) . {&#39;edge_index&#39;: array([[ 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 7, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 12, 19, 19, 20, 19, 21, 5, 22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 27, 28, 27, 29, 29, 30, 23, 2, 30, 24, 21, 9, 18, 13], [ 1, 0, 2, 1, 3, 2, 4, 3, 5, 4, 6, 5, 7, 6, 8, 7, 9, 7, 10, 9, 11, 10, 12, 11, 13, 12, 14, 13, 15, 14, 16, 15, 17, 16, 18, 17, 19, 12, 20, 19, 21, 19, 22, 5, 23, 22, 24, 23, 25, 24, 26, 25, 27, 26, 28, 27, 29, 27, 30, 29, 2, 23, 24, 30, 9, 21, 13, 18]]), &#39;edge_feat&#39;: array([[0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 1], [1, 0, 1], [0, 0, 1], [0, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [0, 0, 1], [0, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [0, 0, 0], [0, 0, 0], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [3, 0, 1], [3, 0, 1], [0, 0, 0], [0, 0, 0], [3, 0, 1], [3, 0, 1], [3, 0, 1], [3, 0, 1]]), &#39;node_feat&#39;: array([[5, 0, 4, 5, 3, 0, 2, 0, 0], [7, 0, 2, 5, 0, 0, 1, 0, 0], [5, 0, 3, 5, 0, 0, 1, 1, 1], [5, 0, 3, 5, 1, 0, 1, 1, 1], [5, 0, 3, 5, 1, 0, 1, 1, 1], [5, 0, 3, 5, 0, 0, 1, 1, 1], [6, 0, 3, 5, 1, 0, 1, 0, 0], [5, 0, 3, 5, 0, 0, 1, 0, 0], [7, 0, 1, 5, 0, 0, 1, 0, 0], [5, 0, 3, 5, 0, 0, 1, 1, 1], [5, 0, 3, 5, 1, 0, 1, 1, 1], [5, 0, 3, 5, 1, 0, 1, 1, 1], [5, 0, 3, 5, 0, 0, 1, 1, 1], [5, 0, 3, 5, 0, 0, 1, 1, 1], [5, 0, 3, 5, 1, 0, 1, 1, 1], [5, 0, 3, 5, 1, 0, 1, 1, 1], [6, 0, 2, 5, 0, 0, 1, 1, 1], [5, 0, 3, 5, 1, 0, 1, 1, 1], [5, 0, 3, 5, 1, 0, 1, 1, 1], [5, 0, 3, 5, 0, 0, 1, 1, 1], [5, 0, 4, 5, 3, 0, 2, 0, 0], [5, 0, 3, 5, 1, 0, 1, 1, 1], [5, 0, 3, 5, 1, 0, 1, 1, 1], [5, 0, 3, 5, 0, 0, 1, 1, 1], [6, 0, 3, 5, 0, 0, 1, 0, 1], [5, 0, 4, 5, 2, 0, 2, 0, 1], [5, 0, 4, 5, 2, 0, 2, 0, 1], [6, 0, 3, 5, 0, 0, 2, 0, 1], [5, 0, 4, 5, 3, 0, 2, 0, 0], [5, 0, 4, 5, 2, 0, 2, 0, 1], [5, 0, 4, 5, 2, 0, 2, 0, 1]]), &#39;num_nodes&#39;: 31} . I&#39;ll actually begin by processing and saving the combined train and extra valid dataframes for possible use later as a &quot;new&quot; csv... . eval_df=eval_df[train_df.columns] #reordering columns to match training... eval_df . Smiles label graph . 0 Cn1c(CN2CCN(CC2)c3ccc(Cl)cc3)nc4ccccc14 | 3.54 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 1 COc1cc(OC)c(cc1NC(=O)CSCC(=O)O)S(=O)(=O)N2C(C)... | -1.18 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 2 COC(=O)[C@@H](N1CCc2sccc2C1)c3ccccc3Cl | 3.69 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 4, 5,... | . 3 OC[C@H](O)CN1C(=O)C(Cc2ccccc12)NC(=O)c3cc4cc(C... | 3.37 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 4, 5,... | . 4 Cc1cccc(C[C@H](NC(=O)c2cc(nn2C)C(C)(C)C)C(=O)N... | 3.10 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . ... ... | ... | ... | . 4195 OCCc1ccc(NC(=O)c2cc3cc(Cl)ccc3[nH]2)cc1 | 3.85 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4196 CCN(C1CCN(CCC(c2ccc(F)cc2)c3ccc(F)cc3)CC1)C(=O... | 3.21 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4197 COc1cccc2[nH]ncc12 | 2.10 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4198 Clc1ccc2ncccc2c1C(=O)NCC3CCCCC3 | 2.65 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4199 CN1C(=O)C=C(CCc2ccc3ccccc3c2)N=C1N | 2.70 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 4, 5,... | . 4200 rows √ó 3 columns . sum(train_df[&#39;Smiles&#39;].value_counts()!=1) . 26 . Remember this? Well, now I should make sure the final augmented training dataset has no duplicates and absolutely NO overlap with the train set (and the test set for that matter...) . augmented_train_df = pd.concat([train_df,eval_df]) augmented_train_df . Smiles label graph . 0 CC(C)Oc1cc(Oc2ccc(S(C)(=O)=O)cc2)cc(-c2ncc(Cl)... | 2.00 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 1, 3, 3, 4, 4, 5,... | . 1 CNS(=O)(=O)c1ccc(Oc2cc(OC(C)C)cc(-c3nccc(=O)[n... | 2.60 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 2, 5,... | . 2 CC(C)Oc1cc(Oc2cnc(C(=O)N(C)C)c(F)c2)cc(-c2nccc... | 1.50 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 1, 3, 3, 4, 4, 5,... | . 3 CC(C)Oc1cc(Oc2cnc(C(=O)N(C)C)cn2)cc(-c2nccc(=O... | 1.30 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 1, 3, 3, 4, 4, 5,... | . 4 CCC(CC)Oc1cc(Oc2cnc(C(=O)N(C)C)nc2)cc(-c2nccc(... | 1.90 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 2, 5,... | . ... ... | ... | ... | . 4195 OCCc1ccc(NC(=O)c2cc3cc(Cl)ccc3[nH]2)cc1 | 3.85 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4196 CCN(C1CCN(CCC(c2ccc(F)cc2)c3ccc(F)cc3)CC1)C(=O... | 3.21 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4197 COc1cccc2[nH]ncc12 | 2.10 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4198 Clc1ccc2ncccc2c1C(=O)NCC3CCCCC3 | 2.65 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4199 CN1C(=O)C=C(CCc2ccc3ccccc3c2)N=C1N | 2.70 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 4, 5,... | . 8029 rows √ó 3 columns . len(augmented_train_df[augmented_train_df.duplicated([&#39;Smiles&#39;],keep=False)]) . 618 . Apparently many of the VantAI SMILES fingerprints match. I&#39;ll drop these duplicate since we don&#39;t want to overfit. I&#39;ll quickly check after for any overlap with the test set. . augmented_train_df = augmented_train_df.drop_duplicates([&#39;Smiles&#39;]) augmented_train_df . Smiles label graph . 0 CC(C)Oc1cc(Oc2ccc(S(C)(=O)=O)cc2)cc(-c2ncc(Cl)... | 2.00 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 1, 3, 3, 4, 4, 5,... | . 1 CNS(=O)(=O)c1ccc(Oc2cc(OC(C)C)cc(-c3nccc(=O)[n... | 2.60 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 2, 5,... | . 2 CC(C)Oc1cc(Oc2cnc(C(=O)N(C)C)c(F)c2)cc(-c2nccc... | 1.50 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 1, 3, 3, 4, 4, 5,... | . 3 CC(C)Oc1cc(Oc2cnc(C(=O)N(C)C)cn2)cc(-c2nccc(=O... | 1.30 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 1, 3, 3, 4, 4, 5,... | . 4 CCC(CC)Oc1cc(Oc2cnc(C(=O)N(C)C)nc2)cc(-c2nccc(... | 1.90 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 2, 5,... | . ... ... | ... | ... | . 4194 CC1CC(N(C(=O)C)c2ccccc2)c3ccccc3N1S(=O)(=O)c4c... | 3.68 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4195 OCCc1ccc(NC(=O)c2cc3cc(Cl)ccc3[nH]2)cc1 | 3.85 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4196 CCN(C1CCN(CCC(c2ccc(F)cc2)c3ccc(F)cc3)CC1)C(=O... | 3.21 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4198 Clc1ccc2ncccc2c1C(=O)NCC3CCCCC3 | 2.65 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4199 CN1C(=O)C=C(CCc2ccc3ccccc3c2)N=C1N | 2.70 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 4, 5,... | . 7718 rows √ó 3 columns . sum(test_df[&#39;Smiles&#39;].isin(eval_df[&#39;Smiles&#39;]).astype(int)) . 0 . Phew! No overlap. The test set is good to go! . Now, saving the big augmented dataframe as a CSV (minus the graph column; my Geometric dataloader will do this for free) as this will be used as a final &quot;training&quot; dataset (augmented for maximum performance) on the final model. I will also quickly save the Stanford CSV again but without any SMILES common to the VantAI training set. This will provide an evaluation dataset for model iteration without compromising the (somewhat limited) training set. . augmented_train_df.drop(columns=[&#39;graph&#39;]).to_csv(&#39;./train_augmented/raw/train_augmented.csv&#39;,index=False) . eval_df[&#39;Smiles&#39;].isin(train_df[&#39;Smiles&#39;]).astype(int) . 0 0 1 0 2 0 3 0 4 0 .. 4195 0 4196 0 4197 1 4198 0 4199 0 Name: Smiles, Length: 4200, dtype: int64 . eval_df_filtered = eval_df[~eval_df[&#39;Smiles&#39;].isin(train_df[&#39;Smiles&#39;])] eval_df_filtered . Smiles label graph . 0 Cn1c(CN2CCN(CC2)c3ccc(Cl)cc3)nc4ccccc14 | 3.54 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 1 COc1cc(OC)c(cc1NC(=O)CSCC(=O)O)S(=O)(=O)N2C(C)... | -1.18 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 2 COC(=O)[C@@H](N1CCc2sccc2C1)c3ccccc3Cl | 3.69 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 4, 5,... | . 3 OC[C@H](O)CN1C(=O)C(Cc2ccccc12)NC(=O)c3cc4cc(C... | 3.37 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 4, 5,... | . 4 Cc1cccc(C[C@H](NC(=O)c2cc(nn2C)C(C)(C)C)C(=O)N... | 3.10 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . ... ... | ... | ... | . 4194 CC1CC(N(C(=O)C)c2ccccc2)c3ccccc3N1S(=O)(=O)c4c... | 3.68 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4195 OCCc1ccc(NC(=O)c2cc3cc(Cl)ccc3[nH]2)cc1 | 3.85 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4196 CCN(C1CCN(CCC(c2ccc(F)cc2)c3ccc(F)cc3)CC1)C(=O... | 3.21 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4198 Clc1ccc2ncccc2c1C(=O)NCC3CCCCC3 | 2.65 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 3, 4, 4, 5,... | . 4199 CN1C(=O)C=C(CCc2ccc3ccccc3c2)N=C1N | 2.70 | {&#39;edge_index&#39;: [[0, 1, 1, 2, 2, 3, 2, 4, 4, 5,... | . 3915 rows √ó 3 columns . eval_df_filtered.drop(columns=[&#39;graph&#39;]).to_csv(&#39;./eval/raw/Lipophilicity_filtered.csv&#39;,index=False) . Machine Learning . Data . I&#39;ll begin by using Pytorch-Geometric&#39;s MoleculeNet inherited dataset class. Credit for this object to Pytorch Geometric! . !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html !pip install -q torch-geometric . import os import os.path as osp import re import torch from torch_geometric.data import (InMemoryDataset, Data, download_url, extract_gz) try: from rdkit import Chem except ImportError: Chem = None x_map = { &#39;atomic_num&#39;: list(range(0, 119)), &#39;chirality&#39;: [ &#39;CHI_UNSPECIFIED&#39;, &#39;CHI_TETRAHEDRAL_CW&#39;, &#39;CHI_TETRAHEDRAL_CCW&#39;, &#39;CHI_OTHER&#39;, ], &#39;degree&#39;: list(range(0, 11)), &#39;formal_charge&#39;: list(range(-5, 7)), &#39;num_hs&#39;: list(range(0, 9)), &#39;num_radical_electrons&#39;: list(range(0, 5)), &#39;hybridization&#39;: [ &#39;UNSPECIFIED&#39;, &#39;S&#39;, &#39;SP&#39;, &#39;SP2&#39;, &#39;SP3&#39;, &#39;SP3D&#39;, &#39;SP3D2&#39;, &#39;OTHER&#39;, ], &#39;is_aromatic&#39;: [False, True], &#39;is_in_ring&#39;: [False, True], } e_map = { &#39;bond_type&#39;: [ &#39;misc&#39;, &#39;SINGLE&#39;, &#39;DOUBLE&#39;, &#39;TRIPLE&#39;, &#39;AROMATIC&#39;, ], &#39;stereo&#39;: [ &#39;STEREONONE&#39;, &#39;STEREOZ&#39;, &#39;STEREOE&#39;, &#39;STEREOCIS&#39;, &#39;STEREOTRANS&#39;, &#39;STEREOANY&#39;, ], &#39;is_conjugated&#39;: [False, True], } class MoleculeNet(InMemoryDataset): r&quot;&quot;&quot;The `MoleculeNet &lt;http://moleculenet.ai/datasets-1&gt;`_ benchmark collection from the `&quot;MoleculeNet: A Benchmark for Molecular Machine Learning&quot; &lt;https://arxiv.org/abs/1703.00564&gt;`_ paper, containing datasets from physical chemistry, biophysics and physiology. All datasets come with the additional node and edge features introduced by the `Open Graph Benchmark &lt;https://ogb.stanford.edu/docs/graphprop/&gt;`_. Args: root (string): Root directory where the dataset should be saved. name (string): The name of the dataset (:obj:`&quot;ESOL&quot;`, :obj:`&quot;FreeSolv&quot;`, :obj:`&quot;Lipo&quot;`, :obj:`&quot;PCBA&quot;`, :obj:`&quot;MUV&quot;`, :obj:`&quot;HIV&quot;`, :obj:`&quot;BACE&quot;`, :obj:`&quot;BBPB&quot;`, :obj:`&quot;Tox21&quot;`, :obj:`&quot;ToxCast&quot;`, :obj:`&quot;SIDER&quot;`, :obj:`&quot;ClinTox&quot;`). transform (callable, optional): A function/transform that takes in an :obj:`torch_geometric.data.Data` object and returns a transformed version. The data object will be transformed before every access. (default: :obj:`None`) pre_transform (callable, optional): A function/transform that takes in an :obj:`torch_geometric.data.Data` object and returns a transformed version. The data object will be transformed before being saved to disk. (default: :obj:`None`) pre_filter (callable, optional): A function that takes in an :obj:`torch_geometric.data.Data` object and returns a boolean value, indicating whether the data object should be included in the final dataset. (default: :obj:`None`) &quot;&quot;&quot; # Format: name: [display_name, url_name, csv_name, smiles_idx, y_idx] names = { &#39;train&#39;: [&#39;Train&#39;, &#39;60170fff5b630f3433c202be_train.csv&#39;, &#39;60170fff5b630f3433c202be_train&#39;, 0, 1], &#39;test&#39;:[&#39;Test&#39;,&#39;60170fffa2720fa4d0b9067a_holdout_set.csv&#39;,&#39;60170fffa2720fa4d0b9067a_holdout_set&#39;,0,1], &#39;eval&#39;: [&#39;Lipophilicity Filtered&#39;, &#39;Lipophilicity_filtered.csv&#39;, &#39;Lipophilicity_filtered&#39;, 0, 1], #stanford dataset &#39;train_augmented&#39;:[&#39;Train Augmented&#39;,&#39;train_augmented.csv&#39;,&#39;train_augmented&#39;,0,1], #vantai train + stanford } def __init__(self, root, name, transform=None, pre_transform=None, pre_filter=None): if Chem is None: raise ImportError(&#39;`MoleculeNet` requires `rdkit`.&#39;) self.name = name.lower() assert self.name in self.names.keys() super(MoleculeNet, self).__init__(root, transform, pre_transform, pre_filter) self.data, self.slices = torch.load(self.processed_paths[0]) @property def raw_dir(self): return osp.join(self.root, self.name, &#39;raw&#39;) @property def processed_dir(self): return osp.join(self.root, self.name, &#39;processed&#39;) @property def raw_file_names(self): return f&#39;{self.names[self.name][2]}.csv&#39; @property def processed_file_names(self): return &#39;data.pt&#39; def process(self): with open(self.raw_paths[0], &#39;r&#39;) as f: # dataset = f.read().split(&#39; n&#39;)[1:-1] dataset = f.read().split(&#39; n&#39;)[1:] #issue w/ test loader dataset = [x for x in dataset if len(x) &gt; 0] # Filter empty lines. data_list = [] for line in dataset: line = re.sub(r&#39; &quot;.* &quot;&#39;, &#39;&#39;, line) # Replace &quot;.*&quot; strings. line = line.split(&#39;,&#39;) smiles = line[self.names[self.name][3]] ys = line[self.names[self.name][4]] ys = ys if isinstance(ys, list) else [ys] ys = [float(y) if len(y) &gt; 0 else float(&#39;NaN&#39;) for y in ys] y = torch.tensor(ys, dtype=torch.float).view(1, -1) mol = Chem.MolFromSmiles(smiles) if mol is None: continue xs = [] for atom in mol.GetAtoms(): x = [] x.append(x_map[&#39;atomic_num&#39;].index(atom.GetAtomicNum())) x.append(x_map[&#39;chirality&#39;].index(str(atom.GetChiralTag()))) x.append(x_map[&#39;degree&#39;].index(atom.GetTotalDegree())) x.append(x_map[&#39;formal_charge&#39;].index(atom.GetFormalCharge())) x.append(x_map[&#39;num_hs&#39;].index(atom.GetTotalNumHs())) x.append(x_map[&#39;num_radical_electrons&#39;].index( atom.GetNumRadicalElectrons())) x.append(x_map[&#39;hybridization&#39;].index( str(atom.GetHybridization()))) x.append(x_map[&#39;is_aromatic&#39;].index(atom.GetIsAromatic())) x.append(x_map[&#39;is_in_ring&#39;].index(atom.IsInRing())) xs.append(x) x = torch.tensor(xs, dtype=torch.long).view(-1, 9) edge_indices, edge_attrs = [], [] for bond in mol.GetBonds(): i = bond.GetBeginAtomIdx() j = bond.GetEndAtomIdx() e = [] e.append(e_map[&#39;bond_type&#39;].index(str(bond.GetBondType()))) e.append(e_map[&#39;stereo&#39;].index(str(bond.GetStereo()))) e.append(e_map[&#39;is_conjugated&#39;].index(bond.GetIsConjugated())) edge_indices += [[i, j], [j, i]] edge_attrs += [e, e] edge_index = torch.tensor(edge_indices) edge_index = edge_index.t().to(torch.long).view(2, -1) edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3) # Sort indices. if edge_index.numel() &gt; 0: perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort() edge_index, edge_attr = edge_index[:, perm], edge_attr[perm] data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, smiles=smiles) if self.pre_filter is not None and not self.pre_filter(data): continue if self.pre_transform is not None: data = self.pre_transform(data) data_list.append(data) torch.save(self.collate(data_list), self.processed_paths[0]) def __repr__(self): return &#39;{}({})&#39;.format(self.names[self.name][0], len(self)) . Datasets . Now we have a dataset class that handles the CSVs and converts to a graphical object, as desired. The next step is to pull the CSVs. Note I will use the extra Stanford MoleculeNet mol-lipo dataset (filtered for VantAI overlap...) as an evaluation set. . train_dataset=MoleculeNet(root=&#39;./&#39;,name=&#39;train&#39;) train_dataset.process() test_dataset=MoleculeNet(root=&#39;./&#39;,name=&#39;test&#39;) test_dataset.process() eval_dataset=MoleculeNet(root=&#39;./&#39;,name=&#39;eval&#39;) eval_dataset.process() . Processing... Done! . test_dataset . Test(141) . print() print(f&#39;Dataset: {train_dataset}:&#39;) print(&#39;====================&#39;) print(f&#39;Number of graphs: {len(train_dataset)}&#39;) print(f&#39;Number of features: {train_dataset.num_features}&#39;) print(f&#39;Number of classes: {train_dataset.num_classes}&#39;) data = train_dataset[420] # Get the a sample graph object. print() print(data) print(&#39;=============================================================&#39;) # Gather some statistics about the first graph. print(f&#39;Number of nodes: {data.num_nodes}&#39;) print(f&#39;Number of edges: {data.num_edges}&#39;) print(f&#39;Average node degree: {data.num_edges / data.num_nodes:.2f}&#39;) print(f&#39;Contains isolated nodes: {data.contains_isolated_nodes()}&#39;) print(f&#39;Contains self-loops: {data.contains_self_loops()}&#39;) print(f&#39;Is undirected: {data.is_undirected()}&#39;) . Dataset: Train(3828): ==================== Number of graphs: 3828 Number of features: 9 Number of classes: 1 Data(edge_attr=[80, 3], edge_index=[2, 80], smiles=&#34;Cc1c(Cl)ccc(OC2CCN(C3CCN(C(=O)NS(=O)(=O)c4ccc(N(C)C)cc4)CC3)CC2)c1Cl&#34;, x=[37, 9], y=[1, 1]) ============================================================= Number of nodes: 37 Number of edges: 80 Average node degree: 2.16 Contains isolated nodes: False Contains self-loops: False Is undirected: True . torch.manual_seed(12345) print(f&#39;Number of training graphs: {len(train_dataset)}&#39;) print(f&#39;Number of test graphs: {len(test_dataset)}&#39;) print(f&#39;Number of eval graphs: {len(eval_dataset)}&#39;) . Number of training graphs: 3828 Number of test graphs: 140 Number of eval graphs: 3915 . That looks good! . Basic GCN: . from torch_geometric.data import DataLoader train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) eval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False) for step, data in enumerate(train_loader): print(f&#39;Step {step + 1}:&#39;) print(&#39;=======&#39;) print(f&#39;Number of graphs in the current batch: {data.num_graphs}&#39;) print(data) print() . Step 1: ======= Number of graphs in the current batch: 64 Batch(batch=[1683], edge_attr=[3618, 3], edge_index=[2, 3618], smiles=[64], x=[1683, 9], y=[64, 1]) Step 2: ======= Number of graphs in the current batch: 64 Batch(batch=[1812], edge_attr=[3930, 3], edge_index=[2, 3930], smiles=[64], x=[1812, 9], y=[64, 1]) Step 3: ======= Number of graphs in the current batch: 64 Batch(batch=[1665], edge_attr=[3620, 3], edge_index=[2, 3620], smiles=[64], x=[1665, 9], y=[64, 1]) Step 4: ======= Number of graphs in the current batch: 64 Batch(batch=[1759], edge_attr=[3802, 3], edge_index=[2, 3802], smiles=[64], x=[1759, 9], y=[64, 1]) Step 5: ======= Number of graphs in the current batch: 64 Batch(batch=[1711], edge_attr=[3702, 3], edge_index=[2, 3702], smiles=[64], x=[1711, 9], y=[64, 1]) Step 6: ======= Number of graphs in the current batch: 64 Batch(batch=[1708], edge_attr=[3704, 3], edge_index=[2, 3704], smiles=[64], x=[1708, 9], y=[64, 1]) Step 7: ======= Number of graphs in the current batch: 64 Batch(batch=[1676], edge_attr=[3632, 3], edge_index=[2, 3632], smiles=[64], x=[1676, 9], y=[64, 1]) Step 8: ======= Number of graphs in the current batch: 64 Batch(batch=[1720], edge_attr=[3730, 3], edge_index=[2, 3730], smiles=[64], x=[1720, 9], y=[64, 1]) Step 9: ======= Number of graphs in the current batch: 64 Batch(batch=[1631], edge_attr=[3526, 3], edge_index=[2, 3526], smiles=[64], x=[1631, 9], y=[64, 1]) Step 10: ======= Number of graphs in the current batch: 64 Batch(batch=[1747], edge_attr=[3806, 3], edge_index=[2, 3806], smiles=[64], x=[1747, 9], y=[64, 1]) Step 11: ======= Number of graphs in the current batch: 64 Batch(batch=[1654], edge_attr=[3576, 3], edge_index=[2, 3576], smiles=[64], x=[1654, 9], y=[64, 1]) Step 12: ======= Number of graphs in the current batch: 64 Batch(batch=[1771], edge_attr=[3830, 3], edge_index=[2, 3830], smiles=[64], x=[1771, 9], y=[64, 1]) Step 13: ======= Number of graphs in the current batch: 64 Batch(batch=[1790], edge_attr=[3898, 3], edge_index=[2, 3898], smiles=[64], x=[1790, 9], y=[64, 1]) Step 14: ======= Number of graphs in the current batch: 64 Batch(batch=[1725], edge_attr=[3722, 3], edge_index=[2, 3722], smiles=[64], x=[1725, 9], y=[64, 1]) Step 15: ======= Number of graphs in the current batch: 64 Batch(batch=[1739], edge_attr=[3770, 3], edge_index=[2, 3770], smiles=[64], x=[1739, 9], y=[64, 1]) Step 16: ======= Number of graphs in the current batch: 64 Batch(batch=[1761], edge_attr=[3820, 3], edge_index=[2, 3820], smiles=[64], x=[1761, 9], y=[64, 1]) Step 17: ======= Number of graphs in the current batch: 64 Batch(batch=[1807], edge_attr=[3948, 3], edge_index=[2, 3948], smiles=[64], x=[1807, 9], y=[64, 1]) Step 18: ======= Number of graphs in the current batch: 64 Batch(batch=[1696], edge_attr=[3682, 3], edge_index=[2, 3682], smiles=[64], x=[1696, 9], y=[64, 1]) Step 19: ======= Number of graphs in the current batch: 64 Batch(batch=[1838], edge_attr=[4000, 3], edge_index=[2, 4000], smiles=[64], x=[1838, 9], y=[64, 1]) Step 20: ======= Number of graphs in the current batch: 64 Batch(batch=[1723], edge_attr=[3748, 3], edge_index=[2, 3748], smiles=[64], x=[1723, 9], y=[64, 1]) Step 21: ======= Number of graphs in the current batch: 64 Batch(batch=[1688], edge_attr=[3664, 3], edge_index=[2, 3664], smiles=[64], x=[1688, 9], y=[64, 1]) Step 22: ======= Number of graphs in the current batch: 64 Batch(batch=[1580], edge_attr=[3428, 3], edge_index=[2, 3428], smiles=[64], x=[1580, 9], y=[64, 1]) Step 23: ======= Number of graphs in the current batch: 64 Batch(batch=[1712], edge_attr=[3728, 3], edge_index=[2, 3728], smiles=[64], x=[1712, 9], y=[64, 1]) Step 24: ======= Number of graphs in the current batch: 64 Batch(batch=[1767], edge_attr=[3824, 3], edge_index=[2, 3824], smiles=[64], x=[1767, 9], y=[64, 1]) Step 25: ======= Number of graphs in the current batch: 64 Batch(batch=[1930], edge_attr=[4192, 3], edge_index=[2, 4192], smiles=[64], x=[1930, 9], y=[64, 1]) Step 26: ======= Number of graphs in the current batch: 64 Batch(batch=[1672], edge_attr=[3606, 3], edge_index=[2, 3606], smiles=[64], x=[1672, 9], y=[64, 1]) Step 27: ======= Number of graphs in the current batch: 64 Batch(batch=[1725], edge_attr=[3722, 3], edge_index=[2, 3722], smiles=[64], x=[1725, 9], y=[64, 1]) Step 28: ======= Number of graphs in the current batch: 64 Batch(batch=[1760], edge_attr=[3850, 3], edge_index=[2, 3850], smiles=[64], x=[1760, 9], y=[64, 1]) Step 29: ======= Number of graphs in the current batch: 64 Batch(batch=[1782], edge_attr=[3888, 3], edge_index=[2, 3888], smiles=[64], x=[1782, 9], y=[64, 1]) Step 30: ======= Number of graphs in the current batch: 64 Batch(batch=[1714], edge_attr=[3738, 3], edge_index=[2, 3738], smiles=[64], x=[1714, 9], y=[64, 1]) Step 31: ======= Number of graphs in the current batch: 64 Batch(batch=[1755], edge_attr=[3778, 3], edge_index=[2, 3778], smiles=[64], x=[1755, 9], y=[64, 1]) Step 32: ======= Number of graphs in the current batch: 64 Batch(batch=[1722], edge_attr=[3716, 3], edge_index=[2, 3716], smiles=[64], x=[1722, 9], y=[64, 1]) Step 33: ======= Number of graphs in the current batch: 64 Batch(batch=[1696], edge_attr=[3654, 3], edge_index=[2, 3654], smiles=[64], x=[1696, 9], y=[64, 1]) Step 34: ======= Number of graphs in the current batch: 64 Batch(batch=[1694], edge_attr=[3674, 3], edge_index=[2, 3674], smiles=[64], x=[1694, 9], y=[64, 1]) Step 35: ======= Number of graphs in the current batch: 64 Batch(batch=[1768], edge_attr=[3840, 3], edge_index=[2, 3840], smiles=[64], x=[1768, 9], y=[64, 1]) Step 36: ======= Number of graphs in the current batch: 64 Batch(batch=[1788], edge_attr=[3874, 3], edge_index=[2, 3874], smiles=[64], x=[1788, 9], y=[64, 1]) Step 37: ======= Number of graphs in the current batch: 64 Batch(batch=[1823], edge_attr=[3954, 3], edge_index=[2, 3954], smiles=[64], x=[1823, 9], y=[64, 1]) Step 38: ======= Number of graphs in the current batch: 64 Batch(batch=[1764], edge_attr=[3858, 3], edge_index=[2, 3858], smiles=[64], x=[1764, 9], y=[64, 1]) Step 39: ======= Number of graphs in the current batch: 64 Batch(batch=[1687], edge_attr=[3658, 3], edge_index=[2, 3658], smiles=[64], x=[1687, 9], y=[64, 1]) Step 40: ======= Number of graphs in the current batch: 64 Batch(batch=[1772], edge_attr=[3810, 3], edge_index=[2, 3810], smiles=[64], x=[1772, 9], y=[64, 1]) Step 41: ======= Number of graphs in the current batch: 64 Batch(batch=[1704], edge_attr=[3702, 3], edge_index=[2, 3702], smiles=[64], x=[1704, 9], y=[64, 1]) Step 42: ======= Number of graphs in the current batch: 64 Batch(batch=[1752], edge_attr=[3798, 3], edge_index=[2, 3798], smiles=[64], x=[1752, 9], y=[64, 1]) Step 43: ======= Number of graphs in the current batch: 64 Batch(batch=[1772], edge_attr=[3856, 3], edge_index=[2, 3856], smiles=[64], x=[1772, 9], y=[64, 1]) Step 44: ======= Number of graphs in the current batch: 64 Batch(batch=[1554], edge_attr=[3352, 3], edge_index=[2, 3352], smiles=[64], x=[1554, 9], y=[64, 1]) Step 45: ======= Number of graphs in the current batch: 64 Batch(batch=[1772], edge_attr=[3854, 3], edge_index=[2, 3854], smiles=[64], x=[1772, 9], y=[64, 1]) Step 46: ======= Number of graphs in the current batch: 64 Batch(batch=[1789], edge_attr=[3868, 3], edge_index=[2, 3868], smiles=[64], x=[1789, 9], y=[64, 1]) Step 47: ======= Number of graphs in the current batch: 64 Batch(batch=[1657], edge_attr=[3612, 3], edge_index=[2, 3612], smiles=[64], x=[1657, 9], y=[64, 1]) Step 48: ======= Number of graphs in the current batch: 64 Batch(batch=[1738], edge_attr=[3778, 3], edge_index=[2, 3778], smiles=[64], x=[1738, 9], y=[64, 1]) Step 49: ======= Number of graphs in the current batch: 64 Batch(batch=[1709], edge_attr=[3714, 3], edge_index=[2, 3714], smiles=[64], x=[1709, 9], y=[64, 1]) Step 50: ======= Number of graphs in the current batch: 64 Batch(batch=[1716], edge_attr=[3696, 3], edge_index=[2, 3696], smiles=[64], x=[1716, 9], y=[64, 1]) Step 51: ======= Number of graphs in the current batch: 64 Batch(batch=[1678], edge_attr=[3650, 3], edge_index=[2, 3650], smiles=[64], x=[1678, 9], y=[64, 1]) Step 52: ======= Number of graphs in the current batch: 64 Batch(batch=[1655], edge_attr=[3572, 3], edge_index=[2, 3572], smiles=[64], x=[1655, 9], y=[64, 1]) Step 53: ======= Number of graphs in the current batch: 64 Batch(batch=[1698], edge_attr=[3688, 3], edge_index=[2, 3688], smiles=[64], x=[1698, 9], y=[64, 1]) Step 54: ======= Number of graphs in the current batch: 64 Batch(batch=[1690], edge_attr=[3676, 3], edge_index=[2, 3676], smiles=[64], x=[1690, 9], y=[64, 1]) Step 55: ======= Number of graphs in the current batch: 64 Batch(batch=[1795], edge_attr=[3878, 3], edge_index=[2, 3878], smiles=[64], x=[1795, 9], y=[64, 1]) Step 56: ======= Number of graphs in the current batch: 64 Batch(batch=[1752], edge_attr=[3812, 3], edge_index=[2, 3812], smiles=[64], x=[1752, 9], y=[64, 1]) Step 57: ======= Number of graphs in the current batch: 64 Batch(batch=[1801], edge_attr=[3916, 3], edge_index=[2, 3916], smiles=[64], x=[1801, 9], y=[64, 1]) Step 58: ======= Number of graphs in the current batch: 64 Batch(batch=[1776], edge_attr=[3838, 3], edge_index=[2, 3838], smiles=[64], x=[1776, 9], y=[64, 1]) Step 59: ======= Number of graphs in the current batch: 64 Batch(batch=[1609], edge_attr=[3476, 3], edge_index=[2, 3476], smiles=[64], x=[1609, 9], y=[64, 1]) Step 60: ======= Number of graphs in the current batch: 52 Batch(batch=[1435], edge_attr=[3130, 3], edge_index=[2, 3130], smiles=[52], x=[1435, 9], y=[52, 1]) . from torch.nn import Linear import torch.nn.functional as F from torch_geometric.nn import GCNConv from torch_geometric.nn import global_mean_pool class AtomEncoder(torch.nn.Module): def __init__(self, hidden_channels): super(AtomEncoder, self).__init__() self.embeddings = torch.nn.ModuleList() for i in range(9): self.embeddings.append(torch.nn.Embedding(100, hidden_channels)) def reset_parameters(self): for embedding in self.embeddings: embedding.reset_parameters() def forward(self, x): if x.dim() == 1: x = x.unsqueeze(1) out = 0 for i in range(x.size(1)): out += self.embeddings[i](x[:, i]) return out class GCN(torch.nn.Module): def __init__(self, hidden_channels): super(GCN, self).__init__() torch.manual_seed(12345) self.emb = AtomEncoder(train_dataset.num_node_features) self.conv1 = GCNConv(train_dataset.num_node_features, hidden_channels) self.conv2 = GCNConv(hidden_channels, hidden_channels) self.conv3 = GCNConv(hidden_channels, hidden_channels) self.lin = Linear(hidden_channels, train_dataset.num_classes) def forward(self, x, edge_index, batch): # 1. Obtain node embeddings x = self.emb(x) x = self.conv1(x, edge_index) x = x.relu() x = self.conv2(x, edge_index) x = x.relu() x = self.conv3(x, edge_index) # 2. Readout layer x = global_mean_pool(x, batch) # [batch_size, hidden_channels] # 3. Apply a final classifier x = F.dropout(x, p=0.5, training=self.training) x = self.lin(x) return x model = GCN(hidden_channels=64) print(model) model = GCN(hidden_channels=64) optimizer = torch.optim.Adam(model.parameters(), lr=0.01) criterion = torch.nn.L1Loss() def train(): model.train() loss_sum = 0 for data in train_loader: # Iterate in batches over the training dataset. out = model(data.x, data.edge_index, data.batch) # Perform a single forward pass. loss = criterion(out, data.y) # Compute the loss (Mean Absolute Error) loss.backward() # Derive gradients. optimizer.step() # Update parameters based on gradients. optimizer.zero_grad() # Clear gradients. loss_sum+=loss return loss #loss_sum / len(train_loader.dataset) # Derive average loss over minibatch. def test(loader): model.eval() loss_sum = 0 for data in loader: # Iterate in batches over the training/test dataset. out = model(data.x, data.edge_index, data.batch) loss = criterion(out,data.y) return loss #final loss, not averaged . GCN( (emb): AtomEncoder( (embeddings): ModuleList( (0): Embedding(100, 9) (1): Embedding(100, 9) (2): Embedding(100, 9) (3): Embedding(100, 9) (4): Embedding(100, 9) (5): Embedding(100, 9) (6): Embedding(100, 9) (7): Embedding(100, 9) (8): Embedding(100, 9) ) ) (conv1): GCNConv(9, 64) (conv2): GCNConv(64, 64) (conv3): GCNConv(64, 64) (lin): Linear(in_features=64, out_features=1, bias=True) ) . for epoch in range(1, 21): train() train_loss = test(train_loader) test_loss = test(eval_loader) print(f&#39;Epoch: {epoch:03d}, Train MAE: {train_loss:.4f}, Eval MAE: {test_loss:.4f}&#39;) . Epoch: 001, Train MAE: 0.8141, Eval MAE: 0.8941 Epoch: 002, Train MAE: 0.7051, Eval MAE: 0.9485 Epoch: 003, Train MAE: 0.9569, Eval MAE: 0.9822 Epoch: 004, Train MAE: 1.0505, Eval MAE: 0.9541 Epoch: 005, Train MAE: 0.8054, Eval MAE: 0.7166 Epoch: 006, Train MAE: 0.9615, Eval MAE: 0.8790 Epoch: 007, Train MAE: 0.7860, Eval MAE: 0.8597 Epoch: 008, Train MAE: 0.8864, Eval MAE: 0.6617 Epoch: 009, Train MAE: 0.7309, Eval MAE: 0.7956 Epoch: 010, Train MAE: 0.7744, Eval MAE: 0.7239 Epoch: 011, Train MAE: 0.5969, Eval MAE: 0.7038 Epoch: 012, Train MAE: 0.8718, Eval MAE: 0.7157 Epoch: 013, Train MAE: 0.5916, Eval MAE: 0.7652 Epoch: 014, Train MAE: 0.6704, Eval MAE: 0.7045 Epoch: 015, Train MAE: 0.7025, Eval MAE: 0.6639 Epoch: 016, Train MAE: 0.7971, Eval MAE: 0.8985 Epoch: 017, Train MAE: 0.5322, Eval MAE: 0.6610 Epoch: 018, Train MAE: 0.8521, Eval MAE: 0.6547 Epoch: 019, Train MAE: 0.6657, Eval MAE: 0.7317 Epoch: 020, Train MAE: 0.7162, Eval MAE: 0.7053 . Now, we have a baseline model training (and learning...something). I won&#39;t waste time on this model because I know other architectures will outperform this basic one per literature. I will also use the augmented dataset for this training/testing round, and then predict on the test CSV for submission. . PNA . As I mentioned in the intro, Principal Neighborhood Aggregation was shown on the seperate ZINC chemical dataset to be far better performing than other graph-regressors. I will use the implementation from https://github.com/lukecavabarrett/pna going forward. . augmented_dataset = MoleculeNet(root=&#39;./&#39;,name=&#39;train_augmented&#39;) augmented_dataset.process() test_dataset = MoleculeNet(root=&#39;./&#39;,name=&#39;test&#39;) test_dataset.process() . test_dataset . Test(141) . len(test_dataset) == len(test_df) . True . from torch_geometric.data import DataLoader train_loader = DataLoader(augmented_dataset, batch_size=32, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False) # eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False) for step, data in enumerate(train_loader): print(f&#39;Step {step + 1}:&#39;) print(&#39;=======&#39;) print(f&#39;Number of graphs in the current batch: {data.num_graphs}&#39;) print(data) print() . in the current batch: 32 Batch(batch=[862], edge_attr=[1868, 3], edge_index=[2, 1868], smiles=[32], x=[862, 9], y=[32, 1]) Step 127: ======= Number of graphs in the current batch: 32 Batch(batch=[911], edge_attr=[1980, 3], edge_index=[2, 1980], smiles=[32], x=[911, 9], y=[32, 1]) Step 128: ======= Number of graphs in the current batch: 32 Batch(batch=[866], edge_attr=[1878, 3], edge_index=[2, 1878], smiles=[32], x=[866, 9], y=[32, 1]) Step 129: ======= Number of graphs in the current batch: 32 Batch(batch=[912], edge_attr=[1978, 3], edge_index=[2, 1978], smiles=[32], x=[912, 9], y=[32, 1]) Step 130: ======= Number of graphs in the current batch: 32 Batch(batch=[802], edge_attr=[1746, 3], edge_index=[2, 1746], smiles=[32], x=[802, 9], y=[32, 1]) Step 131: ======= Number of graphs in the current batch: 32 Batch(batch=[841], edge_attr=[1816, 3], edge_index=[2, 1816], smiles=[32], x=[841, 9], y=[32, 1]) Step 132: ======= Number of graphs in the current batch: 32 Batch(batch=[863], edge_attr=[1876, 3], edge_index=[2, 1876], smiles=[32], x=[863, 9], y=[32, 1]) Step 133: ======= Number of graphs in the current batch: 32 Batch(batch=[844], edge_attr=[1842, 3], edge_index=[2, 1842], smiles=[32], x=[844, 9], y=[32, 1]) Step 134: ======= Number of graphs in the current batch: 32 Batch(batch=[886], edge_attr=[1932, 3], edge_index=[2, 1932], smiles=[32], x=[886, 9], y=[32, 1]) Step 135: ======= Number of graphs in the current batch: 32 Batch(batch=[918], edge_attr=[1998, 3], edge_index=[2, 1998], smiles=[32], x=[918, 9], y=[32, 1]) Step 136: ======= Number of graphs in the current batch: 32 Batch(batch=[848], edge_attr=[1832, 3], edge_index=[2, 1832], smiles=[32], x=[848, 9], y=[32, 1]) Step 137: ======= Number of graphs in the current batch: 32 Batch(batch=[928], edge_attr=[2028, 3], edge_index=[2, 2028], smiles=[32], x=[928, 9], y=[32, 1]) Step 138: ======= Number of graphs in the current batch: 32 Batch(batch=[883], edge_attr=[1932, 3], edge_index=[2, 1932], smiles=[32], x=[883, 9], y=[32, 1]) Step 139: ======= Number of graphs in the current batch: 32 Batch(batch=[893], edge_attr=[1958, 3], edge_index=[2, 1958], smiles=[32], x=[893, 9], y=[32, 1]) Step 140: ======= Number of graphs in the current batch: 32 Batch(batch=[855], edge_attr=[1868, 3], edge_index=[2, 1868], smiles=[32], x=[855, 9], y=[32, 1]) Step 141: ======= Number of graphs in the current batch: 32 Batch(batch=[870], edge_attr=[1878, 3], edge_index=[2, 1878], smiles=[32], x=[870, 9], y=[32, 1]) Step 142: ======= Number of graphs in the current batch: 32 Batch(batch=[894], edge_attr=[1942, 3], edge_index=[2, 1942], smiles=[32], x=[894, 9], y=[32, 1]) Step 143: ======= Number of graphs in the current batch: 32 Batch(batch=[938], edge_attr=[2046, 3], edge_index=[2, 2046], smiles=[32], x=[938, 9], y=[32, 1]) Step 144: ======= Number of graphs in the current batch: 32 Batch(batch=[896], edge_attr=[1956, 3], edge_index=[2, 1956], smiles=[32], x=[896, 9], y=[32, 1]) Step 145: ======= Number of graphs in the current batch: 32 Batch(batch=[829], edge_attr=[1794, 3], edge_index=[2, 1794], smiles=[32], x=[829, 9], y=[32, 1]) Step 146: ======= Number of graphs in the current batch: 32 Batch(batch=[880], edge_attr=[1936, 3], edge_index=[2, 1936], smiles=[32], x=[880, 9], y=[32, 1]) Step 147: ======= Number of graphs in the current batch: 32 Batch(batch=[885], edge_attr=[1932, 3], edge_index=[2, 1932], smiles=[32], x=[885, 9], y=[32, 1]) Step 148: ======= Number of graphs in the current batch: 32 Batch(batch=[901], edge_attr=[1966, 3], edge_index=[2, 1966], smiles=[32], x=[901, 9], y=[32, 1]) Step 149: ======= Number of graphs in the current batch: 32 Batch(batch=[818], edge_attr=[1768, 3], edge_index=[2, 1768], smiles=[32], x=[818, 9], y=[32, 1]) Step 150: ======= Number of graphs in the current batch: 32 Batch(batch=[862], edge_attr=[1876, 3], edge_index=[2, 1876], smiles=[32], x=[862, 9], y=[32, 1]) Step 151: ======= Number of graphs in the current batch: 32 Batch(batch=[897], edge_attr=[1948, 3], edge_index=[2, 1948], smiles=[32], x=[897, 9], y=[32, 1]) Step 152: ======= Number of graphs in the current batch: 32 Batch(batch=[830], edge_attr=[1820, 3], edge_index=[2, 1820], smiles=[32], x=[830, 9], y=[32, 1]) Step 153: ======= Number of graphs in the current batch: 32 Batch(batch=[849], edge_attr=[1852, 3], edge_index=[2, 1852], smiles=[32], x=[849, 9], y=[32, 1]) Step 154: ======= Number of graphs in the current batch: 32 Batch(batch=[880], edge_attr=[1920, 3], edge_index=[2, 1920], smiles=[32], x=[880, 9], y=[32, 1]) Step 155: ======= Number of graphs in the current batch: 32 Batch(batch=[860], edge_attr=[1876, 3], edge_index=[2, 1876], smiles=[32], x=[860, 9], y=[32, 1]) Step 156: ======= Number of graphs in the current batch: 32 Batch(batch=[865], edge_attr=[1902, 3], edge_index=[2, 1902], smiles=[32], x=[865, 9], y=[32, 1]) Step 157: ======= Number of graphs in the current batch: 32 Batch(batch=[879], edge_attr=[1914, 3], edge_index=[2, 1914], smiles=[32], x=[879, 9], y=[32, 1]) Step 158: ======= Number of graphs in the current batch: 32 Batch(batch=[813], edge_attr=[1782, 3], edge_index=[2, 1782], smiles=[32], x=[813, 9], y=[32, 1]) Step 159: ======= Number of graphs in the current batch: 32 Batch(batch=[876], edge_attr=[1900, 3], edge_index=[2, 1900], smiles=[32], x=[876, 9], y=[32, 1]) Step 160: ======= Number of graphs in the current batch: 32 Batch(batch=[849], edge_attr=[1856, 3], edge_index=[2, 1856], smiles=[32], x=[849, 9], y=[32, 1]) Step 161: ======= Number of graphs in the current batch: 32 Batch(batch=[860], edge_attr=[1878, 3], edge_index=[2, 1878], smiles=[32], x=[860, 9], y=[32, 1]) Step 162: ======= Number of graphs in the current batch: 32 Batch(batch=[852], edge_attr=[1842, 3], edge_index=[2, 1842], smiles=[32], x=[852, 9], y=[32, 1]) Step 163: ======= Number of graphs in the current batch: 32 Batch(batch=[805], edge_attr=[1740, 3], edge_index=[2, 1740], smiles=[32], x=[805, 9], y=[32, 1]) Step 164: ======= Number of graphs in the current batch: 32 Batch(batch=[953], edge_attr=[2070, 3], edge_index=[2, 2070], smiles=[32], x=[953, 9], y=[32, 1]) Step 165: ======= Number of graphs in the current batch: 32 Batch(batch=[875], edge_attr=[1914, 3], edge_index=[2, 1914], smiles=[32], x=[875, 9], y=[32, 1]) Step 166: ======= Number of graphs in the current batch: 32 Batch(batch=[877], edge_attr=[1900, 3], edge_index=[2, 1900], smiles=[32], x=[877, 9], y=[32, 1]) Step 167: ======= Number of graphs in the current batch: 32 Batch(batch=[864], edge_attr=[1902, 3], edge_index=[2, 1902], smiles=[32], x=[864, 9], y=[32, 1]) Step 168: ======= Number of graphs in the current batch: 32 Batch(batch=[922], edge_attr=[2012, 3], edge_index=[2, 2012], smiles=[32], x=[922, 9], y=[32, 1]) Step 169: ======= Number of graphs in the current batch: 32 Batch(batch=[907], edge_attr=[1962, 3], edge_index=[2, 1962], smiles=[32], x=[907, 9], y=[32, 1]) Step 170: ======= Number of graphs in the current batch: 32 Batch(batch=[823], edge_attr=[1806, 3], edge_index=[2, 1806], smiles=[32], x=[823, 9], y=[32, 1]) Step 171: ======= Number of graphs in the current batch: 32 Batch(batch=[896], edge_attr=[1944, 3], edge_index=[2, 1944], smiles=[32], x=[896, 9], y=[32, 1]) Step 172: ======= Number of graphs in the current batch: 32 Batch(batch=[904], edge_attr=[1968, 3], edge_index=[2, 1968], smiles=[32], x=[904, 9], y=[32, 1]) Step 173: ======= Number of graphs in the current batch: 32 Batch(batch=[891], edge_attr=[1936, 3], edge_index=[2, 1936], smiles=[32], x=[891, 9], y=[32, 1]) Step 174: ======= Number of graphs in the current batch: 32 Batch(batch=[844], edge_attr=[1828, 3], edge_index=[2, 1828], smiles=[32], x=[844, 9], y=[32, 1]) Step 175: ======= Number of graphs in the current batch: 32 Batch(batch=[927], edge_attr=[2028, 3], edge_index=[2, 2028], smiles=[32], x=[927, 9], y=[32, 1]) Step 176: ======= Number of graphs in the current batch: 32 Batch(batch=[840], edge_attr=[1822, 3], edge_index=[2, 1822], smiles=[32], x=[840, 9], y=[32, 1]) Step 177: ======= Number of graphs in the current batch: 32 Batch(batch=[854], edge_attr=[1864, 3], edge_index=[2, 1864], smiles=[32], x=[854, 9], y=[32, 1]) Step 178: ======= Number of graphs in the current batch: 32 Batch(batch=[848], edge_attr=[1840, 3], edge_index=[2, 1840], smiles=[32], x=[848, 9], y=[32, 1]) Step 179: ======= Number of graphs in the current batch: 32 Batch(batch=[903], edge_attr=[1966, 3], edge_index=[2, 1966], smiles=[32], x=[903, 9], y=[32, 1]) Step 180: ======= Number of graphs in the current batch: 32 Batch(batch=[846], edge_attr=[1822, 3], edge_index=[2, 1822], smiles=[32], x=[846, 9], y=[32, 1]) Step 181: ======= Number of graphs in the current batch: 32 Batch(batch=[877], edge_attr=[1892, 3], edge_index=[2, 1892], smiles=[32], x=[877, 9], y=[32, 1]) Step 182: ======= Number of graphs in the current batch: 32 Batch(batch=[886], edge_attr=[1940, 3], edge_index=[2, 1940], smiles=[32], x=[886, 9], y=[32, 1]) Step 183: ======= Number of graphs in the current batch: 32 Batch(batch=[823], edge_attr=[1782, 3], edge_index=[2, 1782], smiles=[32], x=[823, 9], y=[32, 1]) Step 184: ======= Number of graphs in the current batch: 32 Batch(batch=[858], edge_attr=[1864, 3], edge_index=[2, 1864], smiles=[32], x=[858, 9], y=[32, 1]) Step 185: ======= Number of graphs in the current batch: 32 Batch(batch=[819], edge_attr=[1782, 3], edge_index=[2, 1782], smiles=[32], x=[819, 9], y=[32, 1]) Step 186: ======= Number of graphs in the current batch: 32 Batch(batch=[875], edge_attr=[1908, 3], edge_index=[2, 1908], smiles=[32], x=[875, 9], y=[32, 1]) Step 187: ======= Number of graphs in the current batch: 32 Batch(batch=[913], edge_attr=[1990, 3], edge_index=[2, 1990], smiles=[32], x=[913, 9], y=[32, 1]) Step 188: ======= Number of graphs in the current batch: 32 Batch(batch=[903], edge_attr=[1950, 3], edge_index=[2, 1950], smiles=[32], x=[903, 9], y=[32, 1]) Step 189: ======= Number of graphs in the current batch: 32 Batch(batch=[878], edge_attr=[1902, 3], edge_index=[2, 1902], smiles=[32], x=[878, 9], y=[32, 1]) Step 190: ======= Number of graphs in the current batch: 32 Batch(batch=[823], edge_attr=[1814, 3], edge_index=[2, 1814], smiles=[32], x=[823, 9], y=[32, 1]) Step 191: ======= Number of graphs in the current batch: 32 Batch(batch=[896], edge_attr=[1932, 3], edge_index=[2, 1932], smiles=[32], x=[896, 9], y=[32, 1]) Step 192: ======= Number of graphs in the current batch: 32 Batch(batch=[867], edge_attr=[1890, 3], edge_index=[2, 1890], smiles=[32], x=[867, 9], y=[32, 1]) Step 193: ======= Number of graphs in the current batch: 32 Batch(batch=[805], edge_attr=[1744, 3], edge_index=[2, 1744], smiles=[32], x=[805, 9], y=[32, 1]) Step 194: ======= Number of graphs in the current batch: 32 Batch(batch=[833], edge_attr=[1806, 3], edge_index=[2, 1806], smiles=[32], x=[833, 9], y=[32, 1]) Step 195: ======= Number of graphs in the current batch: 32 Batch(batch=[817], edge_attr=[1780, 3], edge_index=[2, 1780], smiles=[32], x=[817, 9], y=[32, 1]) Step 196: ======= Number of graphs in the current batch: 32 Batch(batch=[883], edge_attr=[1926, 3], edge_index=[2, 1926], smiles=[32], x=[883, 9], y=[32, 1]) Step 197: ======= Number of graphs in the current batch: 32 Batch(batch=[939], edge_attr=[2042, 3], edge_index=[2, 2042], smiles=[32], x=[939, 9], y=[32, 1]) Step 198: ======= Number of graphs in the current batch: 32 Batch(batch=[845], edge_attr=[1842, 3], edge_index=[2, 1842], smiles=[32], x=[845, 9], y=[32, 1]) Step 199: ======= Number of graphs in the current batch: 32 Batch(batch=[922], edge_attr=[2004, 3], edge_index=[2, 2004], smiles=[32], x=[922, 9], y=[32, 1]) Step 200: ======= Number of graphs in the current batch: 32 Batch(batch=[893], edge_attr=[1938, 3], edge_index=[2, 1938], smiles=[32], x=[893, 9], y=[32, 1]) Step 201: ======= Number of graphs in the current batch: 32 Batch(batch=[847], edge_attr=[1846, 3], edge_index=[2, 1846], smiles=[32], x=[847, 9], y=[32, 1]) Step 202: ======= Number of graphs in the current batch: 32 Batch(batch=[909], edge_attr=[1976, 3], edge_index=[2, 1976], smiles=[32], x=[909, 9], y=[32, 1]) Step 203: ======= Number of graphs in the current batch: 32 Batch(batch=[868], edge_attr=[1884, 3], edge_index=[2, 1884], smiles=[32], x=[868, 9], y=[32, 1]) Step 204: ======= Number of graphs in the current batch: 32 Batch(batch=[928], edge_attr=[2032, 3], edge_index=[2, 2032], smiles=[32], x=[928, 9], y=[32, 1]) Step 205: ======= Number of graphs in the current batch: 32 Batch(batch=[796], edge_attr=[1724, 3], edge_index=[2, 1724], smiles=[32], x=[796, 9], y=[32, 1]) Step 206: ======= Number of graphs in the current batch: 32 Batch(batch=[844], edge_attr=[1838, 3], edge_index=[2, 1838], smiles=[32], x=[844, 9], y=[32, 1]) Step 207: ======= Number of graphs in the current batch: 32 Batch(batch=[900], edge_attr=[1966, 3], edge_index=[2, 1966], smiles=[32], x=[900, 9], y=[32, 1]) Step 208: ======= Number of graphs in the current batch: 32 Batch(batch=[873], edge_attr=[1908, 3], edge_index=[2, 1908], smiles=[32], x=[873, 9], y=[32, 1]) Step 209: ======= Number of graphs in the current batch: 32 Batch(batch=[903], edge_attr=[1966, 3], edge_index=[2, 1966], smiles=[32], x=[903, 9], y=[32, 1]) Step 210: ======= Number of graphs in the current batch: 32 Batch(batch=[908], edge_attr=[1970, 3], edge_index=[2, 1970], smiles=[32], x=[908, 9], y=[32, 1]) Step 211: ======= Number of graphs in the current batch: 32 Batch(batch=[918], edge_attr=[1992, 3], edge_index=[2, 1992], smiles=[32], x=[918, 9], y=[32, 1]) Step 212: ======= Number of graphs in the current batch: 32 Batch(batch=[814], edge_attr=[1756, 3], edge_index=[2, 1756], smiles=[32], x=[814, 9], y=[32, 1]) Step 213: ======= Number of graphs in the current batch: 32 Batch(batch=[825], edge_attr=[1784, 3], edge_index=[2, 1784], smiles=[32], x=[825, 9], y=[32, 1]) Step 214: ======= Number of graphs in the current batch: 32 Batch(batch=[900], edge_attr=[1970, 3], edge_index=[2, 1970], smiles=[32], x=[900, 9], y=[32, 1]) Step 215: ======= Number of graphs in the current batch: 32 Batch(batch=[852], edge_attr=[1844, 3], edge_index=[2, 1844], smiles=[32], x=[852, 9], y=[32, 1]) Step 216: ======= Number of graphs in the current batch: 32 Batch(batch=[884], edge_attr=[1922, 3], edge_index=[2, 1922], smiles=[32], x=[884, 9], y=[32, 1]) Step 217: ======= Number of graphs in the current batch: 32 Batch(batch=[835], edge_attr=[1808, 3], edge_index=[2, 1808], smiles=[32], x=[835, 9], y=[32, 1]) Step 218: ======= Number of graphs in the current batch: 32 Batch(batch=[792], edge_attr=[1726, 3], edge_index=[2, 1726], smiles=[32], x=[792, 9], y=[32, 1]) Step 219: ======= Number of graphs in the current batch: 32 Batch(batch=[875], edge_attr=[1910, 3], edge_index=[2, 1910], smiles=[32], x=[875, 9], y=[32, 1]) Step 220: ======= Number of graphs in the current batch: 32 Batch(batch=[807], edge_attr=[1750, 3], edge_index=[2, 1750], smiles=[32], x=[807, 9], y=[32, 1]) Step 221: ======= Number of graphs in the current batch: 32 Batch(batch=[816], edge_attr=[1778, 3], edge_index=[2, 1778], smiles=[32], x=[816, 9], y=[32, 1]) Step 222: ======= Number of graphs in the current batch: 32 Batch(batch=[830], edge_attr=[1796, 3], edge_index=[2, 1796], smiles=[32], x=[830, 9], y=[32, 1]) Step 223: ======= Number of graphs in the current batch: 32 Batch(batch=[904], edge_attr=[1964, 3], edge_index=[2, 1964], smiles=[32], x=[904, 9], y=[32, 1]) Step 224: ======= Number of graphs in the current batch: 32 Batch(batch=[804], edge_attr=[1760, 3], edge_index=[2, 1760], smiles=[32], x=[804, 9], y=[32, 1]) Step 225: ======= Number of graphs in the current batch: 32 Batch(batch=[831], edge_attr=[1810, 3], edge_index=[2, 1810], smiles=[32], x=[831, 9], y=[32, 1]) Step 226: ======= Number of graphs in the current batch: 32 Batch(batch=[868], edge_attr=[1892, 3], edge_index=[2, 1892], smiles=[32], x=[868, 9], y=[32, 1]) Step 227: ======= Number of graphs in the current batch: 32 Batch(batch=[882], edge_attr=[1912, 3], edge_index=[2, 1912], smiles=[32], x=[882, 9], y=[32, 1]) Step 228: ======= Number of graphs in the current batch: 32 Batch(batch=[842], edge_attr=[1828, 3], edge_index=[2, 1828], smiles=[32], x=[842, 9], y=[32, 1]) Step 229: ======= Number of graphs in the current batch: 32 Batch(batch=[842], edge_attr=[1836, 3], edge_index=[2, 1836], smiles=[32], x=[842, 9], y=[32, 1]) Step 230: ======= Number of graphs in the current batch: 32 Batch(batch=[897], edge_attr=[1960, 3], edge_index=[2, 1960], smiles=[32], x=[897, 9], y=[32, 1]) Step 231: ======= Number of graphs in the current batch: 32 Batch(batch=[857], edge_attr=[1866, 3], edge_index=[2, 1866], smiles=[32], x=[857, 9], y=[32, 1]) Step 232: ======= Number of graphs in the current batch: 32 Batch(batch=[816], edge_attr=[1766, 3], edge_index=[2, 1766], smiles=[32], x=[816, 9], y=[32, 1]) Step 233: ======= Number of graphs in the current batch: 32 Batch(batch=[835], edge_attr=[1822, 3], edge_index=[2, 1822], smiles=[32], x=[835, 9], y=[32, 1]) Step 234: ======= Number of graphs in the current batch: 32 Batch(batch=[847], edge_attr=[1842, 3], edge_index=[2, 1842], smiles=[32], x=[847, 9], y=[32, 1]) Step 235: ======= Number of graphs in the current batch: 32 Batch(batch=[836], edge_attr=[1810, 3], edge_index=[2, 1810], smiles=[32], x=[836, 9], y=[32, 1]) Step 236: ======= Number of graphs in the current batch: 32 Batch(batch=[898], edge_attr=[1954, 3], edge_index=[2, 1954], smiles=[32], x=[898, 9], y=[32, 1]) Step 237: ======= Number of graphs in the current batch: 32 Batch(batch=[988], edge_attr=[2124, 3], edge_index=[2, 2124], smiles=[32], x=[988, 9], y=[32, 1]) Step 238: ======= Number of graphs in the current batch: 32 Batch(batch=[826], edge_attr=[1788, 3], edge_index=[2, 1788], smiles=[32], x=[826, 9], y=[32, 1]) Step 239: ======= Number of graphs in the current batch: 32 Batch(batch=[792], edge_attr=[1728, 3], edge_index=[2, 1728], smiles=[32], x=[792, 9], y=[32, 1]) Step 240: ======= Number of graphs in the current batch: 32 Batch(batch=[819], edge_attr=[1760, 3], edge_index=[2, 1760], smiles=[32], x=[819, 9], y=[32, 1]) Step 241: ======= Number of graphs in the current batch: 32 Batch(batch=[827], edge_attr=[1796, 3], edge_index=[2, 1796], smiles=[32], x=[827, 9], y=[32, 1]) Step 242: ======= Number of graphs in the current batch: 32 Batch(batch=[785], edge_attr=[1716, 3], edge_index=[2, 1716], smiles=[32], x=[785, 9], y=[32, 1]) Step 243: ======= Number of graphs in the current batch: 32 Batch(batch=[873], edge_attr=[1902, 3], edge_index=[2, 1902], smiles=[32], x=[873, 9], y=[32, 1]) Step 244: ======= Number of graphs in the current batch: 32 Batch(batch=[826], edge_attr=[1794, 3], edge_index=[2, 1794], smiles=[32], x=[826, 9], y=[32, 1]) Step 245: ======= Number of graphs in the current batch: 32 Batch(batch=[823], edge_attr=[1774, 3], edge_index=[2, 1774], smiles=[32], x=[823, 9], y=[32, 1]) Step 246: ======= Number of graphs in the current batch: 32 Batch(batch=[860], edge_attr=[1878, 3], edge_index=[2, 1878], smiles=[32], x=[860, 9], y=[32, 1]) Step 247: ======= Number of graphs in the current batch: 32 Batch(batch=[851], edge_attr=[1848, 3], edge_index=[2, 1848], smiles=[32], x=[851, 9], y=[32, 1]) Step 248: ======= Number of graphs in the current batch: 32 Batch(batch=[797], edge_attr=[1714, 3], edge_index=[2, 1714], smiles=[32], x=[797, 9], y=[32, 1]) Step 249: ======= Number of graphs in the current batch: 32 Batch(batch=[866], edge_attr=[1876, 3], edge_index=[2, 1876], smiles=[32], x=[866, 9], y=[32, 1]) Step 250: ======= Number of graphs in the current batch: 32 Batch(batch=[850], edge_attr=[1852, 3], edge_index=[2, 1852], smiles=[32], x=[850, 9], y=[32, 1]) Step 251: ======= Number of graphs in the current batch: 29 Batch(batch=[768], edge_attr=[1662, 3], edge_index=[2, 1662], smiles=[29], x=[768, 9], y=[29, 1]) . Final Training . Augmented dataset. Best model. Here goes nothing! . test_dataset #same as before! . Test(141) . test_dataset[0].y #note: No y! . tensor([[nan]]) . import torch import torch.nn.functional as F from torch.nn import ModuleList from torch.nn import Sequential, ReLU, Linear from torch.optim.lr_scheduler import ReduceLROnPlateau from torch_geometric.utils import degree from ogb.graphproppred import PygGraphPropPredDataset, Evaluator from ogb.graphproppred.mol_encoder import AtomEncoder from torch_geometric.data import DataLoader from torch_geometric.nn import BatchNorm, global_mean_pool from models.pytorch_geometric.pna import PNAConvSimple # Compute in-degree histogram over training data. deg = torch.zeros(10, dtype=torch.long) for data in train_dataset: d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long) deg += torch.bincount(d, minlength=deg.numel()) class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.node_emb = AtomEncoder(emb_dim=70) aggregators = [&#39;mean&#39;, &#39;min&#39;, &#39;max&#39;, &#39;std&#39;] scalers = [&#39;identity&#39;, &#39;amplification&#39;, &#39;attenuation&#39;] self.convs = ModuleList() self.batch_norms = ModuleList() for _ in range(4): conv = PNAConvSimple(in_channels=70, out_channels=70, aggregators=aggregators, scalers=scalers, deg=deg, post_layers=1) self.convs.append(conv) self.batch_norms.append(BatchNorm(70)) self.mlp = Sequential(Linear(70, 35), ReLU(), Linear(35, 17), ReLU(), Linear(17, 1)) def forward(self, x, edge_index, edge_attr, batch): x = self.node_emb(x) for conv, batch_norm in zip(self.convs, self.batch_norms): h = F.relu(batch_norm(conv(x, edge_index, edge_attr))) x = h + x # residual# x = F.dropout(x, 0.3, training=self.training) x = global_mean_pool(x, batch) return self.mlp(x) device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) model = Net().to(device) optimizer = torch.optim.Adam(model.parameters(), lr=0.01) scheduler = ReduceLROnPlateau(optimizer, mode=&#39;max&#39;, factor=0.5, patience=20, min_lr=0.0001) def train(epoch): model.train() total_loss = 0 for data in train_loader: data = data.to(device) optimizer.zero_grad() out = model(data.x, data.edge_index, None, data.batch) loss = torch.nn.L1Loss()(out.to(torch.float32), data.y.to(torch.float32)) loss.backward() total_loss += loss.item() * data.num_graphs optimizer.step() return total_loss / len(train_loader.dataset) . for epoch in range(1, 201): loss = train(epoch) #MAE print(f&#39;Epoch: {epoch:02d}, MAE Loss: {loss:.4f}&#39;) . Epoch: 01, MAE Loss: 0.9188 Epoch: 02, MAE Loss: 0.7788 Epoch: 03, MAE Loss: 0.7311 Epoch: 04, MAE Loss: 0.7065 Epoch: 05, MAE Loss: 0.6833 Epoch: 06, MAE Loss: 0.6653 Epoch: 07, MAE Loss: 0.6468 Epoch: 08, MAE Loss: 0.6503 Epoch: 09, MAE Loss: 0.6535 Epoch: 10, MAE Loss: 0.6401 Epoch: 11, MAE Loss: 0.6174 Epoch: 12, MAE Loss: 0.6173 Epoch: 13, MAE Loss: 0.6049 Epoch: 14, MAE Loss: 0.6101 Epoch: 15, MAE Loss: 0.5900 Epoch: 16, MAE Loss: 0.6102 Epoch: 17, MAE Loss: 0.5972 Epoch: 18, MAE Loss: 0.5914 Epoch: 19, MAE Loss: 0.5915 Epoch: 20, MAE Loss: 0.5790 Epoch: 21, MAE Loss: 0.5827 Epoch: 22, MAE Loss: 0.5725 Epoch: 23, MAE Loss: 0.5564 Epoch: 24, MAE Loss: 0.5542 Epoch: 25, MAE Loss: 0.5508 Epoch: 26, MAE Loss: 0.5416 Epoch: 27, MAE Loss: 0.5421 Epoch: 28, MAE Loss: 0.5392 Epoch: 29, MAE Loss: 0.5386 Epoch: 30, MAE Loss: 0.5344 Epoch: 31, MAE Loss: 0.5385 Epoch: 32, MAE Loss: 0.5304 Epoch: 33, MAE Loss: 0.5305 Epoch: 34, MAE Loss: 0.5361 Epoch: 35, MAE Loss: 0.5313 Epoch: 36, MAE Loss: 0.5287 Epoch: 37, MAE Loss: 0.5280 Epoch: 38, MAE Loss: 0.5306 Epoch: 39, MAE Loss: 0.5270 Epoch: 40, MAE Loss: 0.5246 Epoch: 41, MAE Loss: 0.5244 Epoch: 42, MAE Loss: 0.5227 Epoch: 43, MAE Loss: 0.5228 Epoch: 44, MAE Loss: 0.5072 Epoch: 45, MAE Loss: 0.5067 Epoch: 46, MAE Loss: 0.4983 Epoch: 47, MAE Loss: 0.4950 Epoch: 48, MAE Loss: 0.4992 Epoch: 49, MAE Loss: 0.4992 Epoch: 50, MAE Loss: 0.5015 Epoch: 51, MAE Loss: 0.4978 Epoch: 52, MAE Loss: 0.4873 Epoch: 53, MAE Loss: 0.4996 Epoch: 54, MAE Loss: 0.4977 Epoch: 55, MAE Loss: 0.4889 Epoch: 56, MAE Loss: 0.4972 Epoch: 57, MAE Loss: 0.4945 Epoch: 58, MAE Loss: 0.4889 Epoch: 59, MAE Loss: 0.4958 Epoch: 60, MAE Loss: 0.4822 Epoch: 61, MAE Loss: 0.4939 Epoch: 62, MAE Loss: 0.4841 Epoch: 63, MAE Loss: 0.4857 Epoch: 64, MAE Loss: 0.4939 Epoch: 65, MAE Loss: 0.4802 Epoch: 66, MAE Loss: 0.4740 Epoch: 67, MAE Loss: 0.4744 Epoch: 68, MAE Loss: 0.4758 Epoch: 69, MAE Loss: 0.4715 Epoch: 70, MAE Loss: 0.4730 Epoch: 71, MAE Loss: 0.4775 Epoch: 72, MAE Loss: 0.4741 Epoch: 73, MAE Loss: 0.4687 Epoch: 74, MAE Loss: 0.4732 Epoch: 75, MAE Loss: 0.4773 Epoch: 76, MAE Loss: 0.4698 Epoch: 77, MAE Loss: 0.4718 Epoch: 78, MAE Loss: 0.4768 Epoch: 79, MAE Loss: 0.4745 Epoch: 80, MAE Loss: 0.4700 Epoch: 81, MAE Loss: 0.4722 Epoch: 82, MAE Loss: 0.4713 Epoch: 83, MAE Loss: 0.4694 Epoch: 84, MAE Loss: 0.4766 Epoch: 85, MAE Loss: 0.4681 Epoch: 86, MAE Loss: 0.4614 Epoch: 87, MAE Loss: 0.4696 Epoch: 88, MAE Loss: 0.4621 Epoch: 89, MAE Loss: 0.4642 Epoch: 90, MAE Loss: 0.4590 Epoch: 91, MAE Loss: 0.4636 Epoch: 92, MAE Loss: 0.4602 Epoch: 93, MAE Loss: 0.4627 Epoch: 94, MAE Loss: 0.4689 Epoch: 95, MAE Loss: 0.4598 Epoch: 96, MAE Loss: 0.4568 Epoch: 97, MAE Loss: 0.4640 Epoch: 98, MAE Loss: 0.4668 Epoch: 99, MAE Loss: 0.4582 Epoch: 100, MAE Loss: 0.4520 Epoch: 101, MAE Loss: 0.4540 Epoch: 102, MAE Loss: 0.4644 Epoch: 103, MAE Loss: 0.4673 Epoch: 104, MAE Loss: 0.4647 Epoch: 105, MAE Loss: 0.4640 Epoch: 106, MAE Loss: 0.4604 Epoch: 107, MAE Loss: 0.4582 Epoch: 108, MAE Loss: 0.4589 Epoch: 109, MAE Loss: 0.4566 Epoch: 110, MAE Loss: 0.4519 Epoch: 111, MAE Loss: 0.4595 Epoch: 112, MAE Loss: 0.4534 Epoch: 113, MAE Loss: 0.4622 Epoch: 114, MAE Loss: 0.4583 Epoch: 115, MAE Loss: 0.4596 Epoch: 116, MAE Loss: 0.4599 Epoch: 117, MAE Loss: 0.4536 Epoch: 118, MAE Loss: 0.4560 Epoch: 119, MAE Loss: 0.4552 Epoch: 120, MAE Loss: 0.4524 Epoch: 121, MAE Loss: 0.4574 Epoch: 122, MAE Loss: 0.4578 Epoch: 123, MAE Loss: 0.4590 Epoch: 124, MAE Loss: 0.4595 Epoch: 125, MAE Loss: 0.4568 Epoch: 126, MAE Loss: 0.4576 Epoch: 127, MAE Loss: 0.4580 Epoch: 128, MAE Loss: 0.4575 Epoch: 129, MAE Loss: 0.4530 Epoch: 130, MAE Loss: 0.4535 Epoch: 131, MAE Loss: 0.4559 Epoch: 132, MAE Loss: 0.4592 Epoch: 133, MAE Loss: 0.4559 Epoch: 134, MAE Loss: 0.4560 Epoch: 135, MAE Loss: 0.4534 Epoch: 136, MAE Loss: 0.4581 Epoch: 137, MAE Loss: 0.4530 Epoch: 138, MAE Loss: 0.4533 Epoch: 139, MAE Loss: 0.4550 Epoch: 140, MAE Loss: 0.4506 Epoch: 141, MAE Loss: 0.4594 Epoch: 142, MAE Loss: 0.4485 Epoch: 143, MAE Loss: 0.4543 Epoch: 144, MAE Loss: 0.4495 Epoch: 145, MAE Loss: 0.4556 Epoch: 146, MAE Loss: 0.4506 Epoch: 147, MAE Loss: 0.4577 Epoch: 148, MAE Loss: 0.4525 Epoch: 149, MAE Loss: 0.4537 Epoch: 150, MAE Loss: 0.4564 Epoch: 151, MAE Loss: 0.4518 Epoch: 152, MAE Loss: 0.4504 Epoch: 153, MAE Loss: 0.4555 Epoch: 154, MAE Loss: 0.4523 Epoch: 155, MAE Loss: 0.4461 Epoch: 156, MAE Loss: 0.4549 Epoch: 157, MAE Loss: 0.4481 Epoch: 158, MAE Loss: 0.4527 Epoch: 159, MAE Loss: 0.4550 Epoch: 160, MAE Loss: 0.4495 Epoch: 161, MAE Loss: 0.4575 Epoch: 162, MAE Loss: 0.4541 Epoch: 163, MAE Loss: 0.4530 Epoch: 164, MAE Loss: 0.4471 Epoch: 165, MAE Loss: 0.4451 Epoch: 166, MAE Loss: 0.4555 Epoch: 167, MAE Loss: 0.4523 Epoch: 168, MAE Loss: 0.4499 Epoch: 169, MAE Loss: 0.4509 Epoch: 170, MAE Loss: 0.4487 Epoch: 171, MAE Loss: 0.4537 Epoch: 172, MAE Loss: 0.4545 Epoch: 173, MAE Loss: 0.4555 Epoch: 174, MAE Loss: 0.4563 Epoch: 175, MAE Loss: 0.4506 Epoch: 176, MAE Loss: 0.4492 Epoch: 177, MAE Loss: 0.4544 Epoch: 178, MAE Loss: 0.4462 Epoch: 179, MAE Loss: 0.4536 Epoch: 180, MAE Loss: 0.4562 Epoch: 181, MAE Loss: 0.4490 Epoch: 182, MAE Loss: 0.4580 Epoch: 183, MAE Loss: 0.4564 Epoch: 184, MAE Loss: 0.4504 Epoch: 185, MAE Loss: 0.4514 Epoch: 186, MAE Loss: 0.4497 Epoch: 187, MAE Loss: 0.4552 Epoch: 188, MAE Loss: 0.4580 Epoch: 189, MAE Loss: 0.4542 Epoch: 190, MAE Loss: 0.4518 Epoch: 191, MAE Loss: 0.4478 Epoch: 192, MAE Loss: 0.4499 Epoch: 193, MAE Loss: 0.4537 Epoch: 194, MAE Loss: 0.4475 Epoch: 195, MAE Loss: 0.4513 Epoch: 196, MAE Loss: 0.4507 Epoch: 197, MAE Loss: 0.4439 Epoch: 198, MAE Loss: 0.4528 Epoch: 199, MAE Loss: 0.4530 Epoch: 200, MAE Loss: 0.4502 . out = [float(model(data.x, data.edge_index, None, data.batch).cpu().detach().numpy().squeeze()) for data in test_loader] . len(out) . 141 . pd.DataFrame(out).plot(title=&#39;MAE Loss&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7feffde0a750&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; submission_df = pd.read_csv(&#39;test/raw/60170fffa2720fa4d0b9067a_holdout_set.csv&#39;) submission_df[&#39;predicted&#39;]=out submission_df.to_csv(&#39;submissions/60170fffa2720fa4d0b9067a_holdout_set.csv&#39;,index=False,) . This is by no means a finished product or production-ready pipeline, and remains a work in progress. Training must be optimized and the final model trained will likely need to be regularized for any real production purpose. However, I hope this shows both how simple graphical nets can be and how natural a domain chemical structures are for this framework. . In the future, I may update this post to include Transformer-based model in addition to the points mentioned above. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/02/01/GNN-mol.html",
            "relUrl": "/jupyter/2021/02/01/GNN-mol.html",
            "date": " ‚Ä¢ Feb 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "One GiGANtic Leap...",
            "content": "Introduction . Generative Adversarial Networks are more than a novel deep learning architecture. They are powerful game-theoretic constructs with potential to change much of our understanding of what is &quot;real&quot;. If what is real is useful insofar as what we perceive is useful, then we may be in touble. The advent of GANs in the early part of this century has distrupted the way reality is made. Deep networks are becoming immensely powerful at learning class-posterior distributions $P(Y|X=x)$. However, generative models are doing something much trickier: learning $P(X,Y)$. GANs use a discriminator of $P(Y|X)$ to accomplish this. Thus, adversarial nets are a balancing act to implement and train, but provide unparalleled results. On the one hand, this could seriousy destroy media (Microsoft&#39;s MSN recently replaced human authors with generative machines), social stability, and our politics (think DeepFakes). However, there is reason to be hopeful. . One realm I&#39;m particularly excited to see GANs making an entrance is in the biosciences. GANs (and really any high-fidelity generative model) may hold the promise to generate sythetic datasets to build better supervised learners on features such as genomes, proteomes, and bioimages. This could help bring machine learning closer to solving few-shot and zero-shot problems in medicine. . Another example is for the purposes of simulation and modeling. Tons of assumptions must be made during modeling (for, say, a pandemic) that may not prove realistic. On the other hand, epidemiological datasets may be limited in scope and depth, such that sparse or missing features from a data collection stage could prove vital if not for their low frequencies. Simulating patient risk pools for disease-fighting or drug design via a GAN is one potential method to overcome this, as we can sample from $P(Y,X=x_{minority})$ via a model trained to discriminate perfectly on $P(Y|X)$. . Bioengineering is another intriguing space, whereby in-silico experimentation meets the lab bench through simulating data like yeast genomes. There&#39;s even the potential to build a life factory through generating realistic but novel oligomers and augmenting existing genomes (possible given how splicing is currently solved). . In any event, we should know just how to 1) Train a GAN, and 2) Use it to augment an existing workflow. In this post, I will begin by using the COVID-19 cell atlas as a source of features, and $X$ will be the single-cell read counts. That is, single-cell profiling of COVID-19 patients will provide the raw data to build a generative adversarial network. We will begin by training a discriminator $f$ to learn a classification $f(X) = y$, or $P(Y|X)$. Then, the generative model will begin to simulate $X,y$ pairs, and be trained to accurately generate $P(X,Y)$. . This can be and is used to generate realistic data, or samples on $P(X|Y=y)$ (such as realistic read-counts of single-cell data given patient phenotypes). We can use this trained generative model to shore up minority classes to improve classifier performance (another $f(X) = y$, though on held-out test data), or simply to provide more data distributed in the same fashion as $X,y$. I will simply generate realistic fake data for now. . Data . I will use scanpy to load in a toy PBMC (peripheral blood mononuclear cell) dataset into a PyTorch Dataset object. Features will be normalized single-cell read counts, a correlate of gene expression. Labels will be cell types identified from Louvain clusters. . import sklearn.model_selection from sklearn.preprocessing import LabelEncoder import scanpy as sc data = sc.datasets.pbmc3k_processed() train, val = sklearn.model_selection.train_test_split(data,test_size=.25) le = LabelEncoder().fit(train.obs.louvain) train.y=le.transform(train.obs.louvain) val.y=le.transform(val.obs.louvain) . from torch.utils.data import Dataset,DataLoader import torch from matplotlib import pyplot as plt import matplotlib as mpl class ToySingleCellDataset(Dataset): def __init__(self,anndata): self.anndata = anndata self.X = self.anndata.X self.y = self.anndata.y self.setup_run = False def __len__(self): return len(self.anndata) def __getitem__(self,idx): assert self.setup_run==True, &#39;Run .setup()&#39; return self.X[idx], self.y[idx] def setup(self): self.X = torch.FloatTensor(self.X) self.y = torch.LongTensor(self.y) print(f&#39;Data loaded. X is of shape {self.X.shape}, with y of shape {self.y.shape}&#39;) self.setup_run=True def plot_umap(self): mpl.rcParams[&#39;figure.dpi&#39;]= 150 plt.style.use(&#39;dark_background&#39;) plt.scatter(self.anndata.obsm[&#39;X_umap&#39;][:,0],self.anndata.obsm[&#39;X_umap&#39;][:,1], c=self.y, alpha=0.20, cmap = &#39;Pastel1&#39;,s=20) plt.xlabel(&#39;Component 1&#39;) plt.ylabel(&#39;Component 2&#39;) plt.title(&#39;UMAP, Colored by Cell Type&#39;) . train_dataset, val_dataset = ToySingleCellDataset(train), ToySingleCellDataset(val) train_dataset.plot_umap() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-18T12:58:05.592480 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Model Architecture . Probabilistic Intuition . A symmetric view: . i) A generator (in the data-faker view) samples from $P(X|Y=y)$, the distribution of the feature over each class or label. . Note that $P(X,Y) = P(X|Y)P(Y) implies P(X|Y) = P(X,Y)/P(Y)$ | . ii) A discriminator samples on $P(Y|X=x)$, the distribution of the labels over each feature. . Note that $P(X,Y) = P(Y|X)P(X) implies P(Y|X) = P(X,Y)/P(X)$ | . This is because we are provided many instances of $x,y$, where the label space is finite. . Assuming the labels are discrete (i.e., in the case of a classifier and not a regressor), we can always sum to marginalize out the labels from the joint distribution to produce the feature prior $P(x) = sum_y P(X,Y=y)$. . Likewise, for a continuous feature $X$, we can always integrate to marginalize out the features and produce the class prior $P(y) = int_x P(Y,X=x)$. . Then, either the &quot;generator&quot; $P(X|Y)$ or the &quot;discriminator&quot; $P(Y|X)$ can be derived by the definition of conditional probability and Bayes&#39; rule (see the above bullets). . Implementation . We integrate this construction by using a GAN. Classically, this implies that we train a discriminator $f(X) = y$ on $ vec{x}, vec{y}$ and subsequently use this . The Generative Adversarial Network in PyTorch . import os from argparse import ArgumentParser from collections import OrderedDict import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms from torch.utils.data import DataLoader, random_split from torchvision.datasets import MNIST import pytorch_lightning as pl . import sklearn.model_selection from sklearn.preprocessing import LabelEncoder import scanpy as sc class SingleCellDataModule(pl.LightningDataModule): def __init__(self, data_dir: str = &#39;./data&#39;, batch_size: int = 64, num_workers: int = 8): super().__init__() self.data_dir = data_dir self.batch_size = batch_size self.num_workers = num_workers # self.dims is returned when you call dm.size() # Setting default dims here because we know them. # Could optionally be assigned dynamically in dm.setup() # self.dims = (1, 28, 28) # self.num_classes = 10 def setup(self, stage=None): # Assign train/val datasets for use in dataloaders. No test stage in our case. data = sc.datasets.pbmc3k_processed() train, val = sklearn.model_selection.train_test_split(data,test_size=.25) le = LabelEncoder().fit(train.obs.louvain) train.y=le.transform(train.obs.louvain) val.y=le.transform(val.obs.louvain) self.train_dataset, self.val_dataset = ToySingleCellDataset(train), ToySingleCellDataset(val) self.train_dataset.setup() self.val_dataset.setup() self.num_classes = len(self.train_dataset.y.unique()) def train_dataloader(self): return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers) def val_dataloader(self): return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers) . class Generator(nn.Module): def __init__(self, latent_dim, sc_instance_shape): super().__init__() self.sc_instance_shape = sc_instance_shape def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(latent_dim, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(sc_instance_shape))), nn.Tanh() ) def forward(self, z): sc_instance = self.model(z) sc_instance = sc_instance.view(sc_instance.size(0), *self.sc_instance_shape) return sc_instance . class Discriminator(nn.Module): def __init__(self, sc_instance_shape): super().__init__() self.model = nn.Sequential( nn.Linear(int(np.prod(sc_instance_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, 1), nn.Sigmoid(), ) def forward(self, sc_instance): sc_instance_flat = sc_instance.view(sc_instance.size(0), -1) validity = self.model(sc_instance_flat) return validity . class SingleCellGAN(pl.LightningModule): def __init__( self, data_shape, latent_dim: int = 100, lr: float = 0.0002, b1: float = 0.5, b2: float = 0.999, batch_size: int = 64, **kwargs ): super().__init__() self.save_hyperparameters() # networks # data_shape = (1838,) self.generator = Generator(latent_dim=self.hparams.latent_dim, sc_instance_shape=data_shape) self.discriminator = Discriminator(sc_instance_shape=data_shape) self.validation_z = torch.randn(8, self.hparams.latent_dim) self.example_input_array = torch.zeros(2, self.hparams.latent_dim) def forward(self, z): return self.generator(z) def adversarial_loss(self, y_hat, y): return F.binary_cross_entropy(y_hat, y) def training_step(self, batch, batch_idx, optimizer_idx): sc_instances, _ = batch # sample noise z = torch.randn(sc_instances.shape[0], self.hparams.latent_dim) z = z.type_as(sc_instances) # train generator if optimizer_idx == 0: # # generate images # self.generated_sc_instances = self(z) # # log sampled images # sample_instances = self.generated_sc_instances[:6] # grid = torchvision.utils.make_grid(sample_sc_instances) # self.logger.experiment.add_image(&#39;generated_images&#39;, grid, 0) # ground truth result (ie: all fake) # put on GPU because we created this tensor inside training_loop valid = torch.ones(sc_instances.size(0), 1) valid = valid.type_as(sc_instances) # adversarial loss is binary cross-entropy g_loss = self.adversarial_loss(self.discriminator(self(z)), valid) tqdm_dict = {&#39;g_loss&#39;: g_loss} output = OrderedDict({ &#39;loss&#39;: g_loss, &#39;progress_bar&#39;: tqdm_dict, &#39;log&#39;: tqdm_dict }) return output # train discriminator if optimizer_idx == 1: # Measure discriminator&#39;s ability to classify real from generated samples # how well can it label as real? valid = torch.ones(sc_instances.size(0), 1) valid = valid.type_as(sc_instances) real_loss = self.adversarial_loss(self.discriminator(sc_instances), valid) # how well can it label as fake? fake = torch.zeros(sc_instances.size(0), 1) fake = fake.type_as(sc_instances) fake_loss = self.adversarial_loss( self.discriminator(self(z).detach()), fake) # discriminator loss is the average of these d_loss = (real_loss + fake_loss) / 2 tqdm_dict = {&#39;d_loss&#39;: d_loss} output = OrderedDict({ &#39;loss&#39;: d_loss, &#39;progress_bar&#39;: tqdm_dict, &#39;log&#39;: tqdm_dict }) return output def configure_optimizers(self): lr = self.hparams.lr b1 = self.hparams.b1 b2 = self.hparams.b2 opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2)) opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2)) return [opt_g, opt_d], [] def on_epoch_end(self): z = self.validation_z.type_as(self.generator.model[0].weight) # log sampled images sample_sc_instances = self(z) grid = torchvision.utils.make_grid(sample_sc_instances) self.logger.experiment.add_image(&#39;generated_sc_instances&#39;, grid, self.current_epoch) . dm = SingleCellDataModule() dm.setup() model = SingleCellGAN(tuple(next(iter(dm.train_dataset))[0].shape)) trainer = pl.Trainer(gpus=1, max_epochs=25, fast_dev_run=False) trainer.fit(model, dm) . /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You have set progress_bar_refresh_rate &lt; 20 on Google Colab. This may crash. Consider using progress_bar_refresh_rate &gt;= 20 in Trainer. warnings.warn(*args, **kwargs) GPU available: True, used: True TPU available: None, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . Data loaded. X is of shape torch.Size([1978, 1838]), with y of shape torch.Size([1978]) Data loaded. X is of shape torch.Size([660, 1838]), with y of shape torch.Size([660]) . /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop warnings.warn(*args, **kwargs) | Name | Type | Params | In sizes | Out sizes -- 0 | generator | Generator | 2.6 M | [2, 100] | [2, 1838] 1 | discriminator | Discriminator | 1.1 M | ? | ? -- 3.7 M Trainable params 0 Non-trainable params 3.7 M Total params /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The {progress_bar:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0 Please use self.log(...) inside the lightningModule instead. # log on a step or aggregate epoch metric to the logger and/or progress bar # (inside LightningModule) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) warnings.warn(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0 Please use self.log(...) inside the lightningModule instead. # log on a step or aggregate epoch metric to the logger and/or progress bar # (inside LightningModule) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) warnings.warn(*args, **kwargs) . . 1 . Conclusion . We&#39;ve trained a GAN! Now, the model can be saved and the Generator object can be put into production as a single-cell data synthesizer. I hope you&#39;ve seen how powerful this framework can be and I look forward to seeing what generative models can do for the biosciences. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/16/generate_me.html",
            "relUrl": "/jupyter/2021/01/16/generate_me.html",
            "date": " ‚Ä¢ Jan 16, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Kaokore Facial Verification",
            "content": ". Overview . The recent Kaokore (https://arxiv.org/abs/2002.08595) facial dataset presents some exciting creative opportunities for machine learning on these images. We have nearly 9,000 faces, each associated with a male or female gender label as well as a social status label (one of Noble, Warrior, Incarnation, or Commoner). . Approach . I&#39;ve built a quick ResNet18 pipeline. No training here and this is totally unsupervised (for now!). Rather, I simply embed each painting into a vector $ vec{v}$ of cardinality 1,000. Then, these vectors are fitted to a k-nearest-neighbors ($k=5$ for now) estimator from scikit-learn. Finally, I embed my some human faces (including my own) as queries to this estimator, then retrieve the indices of the k-most-similar vectors. The results are pretty rough, and my plan is to fine-tune FaceNet or another more niche network on these data. Another possibility is a more recent self-supervised model trained with Contrastive Predictive Coding (CPC). . . %matplotlib inline from IPython.display import Image from IPython.display import display import joblib from torchvision.datasets import ImageFolder from torch.utils.data import DataLoader import torchvision.transforms as transforms import numpy as np import torchvision.models as models from tqdm.notebook import tqdm from sklearn.neighbors import NearestNeighbors from sklearn.externals import joblib import torch from torch.autograd import Variable from PIL import Image from torchvision.utils import make_grid from IPython.display import Image from IPython.display import display import torchvision DEVICE = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) . /usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+. warnings.warn(msg, category=FutureWarning) . !git clone https://github.com/rois-codh/kaokore . Cloning into &#39;kaokore&#39;... remote: Enumerating objects: 85, done. remote: Counting objects: 100% (85/85), done. remote: Compressing objects: 100% (75/75), done. remote: Total 85 (delta 33), reused 39 (delta 7), pack-reused 0 Unpacking objects: 100% (85/85), done. . !cd kaokore &amp;&amp; python3 download.py --dataset_version 1.2 . Downloading Kaokore version 1.2, saving to kaokore Downloading 8848 images using 16 threads 36% 3179/8848 [02:14&lt;01:51, 50.83it/s]Grayscale -&gt; RGB : kaokore/images_256/00003184.jpg 36% 3185/8848 [02:14&lt;01:54, 49.55it/s]Grayscale -&gt; RGB : kaokore/images_256/00003185.jpg Grayscale -&gt; RGB : kaokore/images_256/00003186.jpg Grayscale -&gt; RGB : kaokore/images_256/00003188.jpg Grayscale -&gt; RGB : kaokore/images_256/00003190.jpg Grayscale -&gt; RGB : kaokore/images_256/00003191.jpg Grayscale -&gt; RGB : kaokore/images_256/00003192.jpg 36% 3191/8848 [02:14&lt;02:04, 45.41it/s]Grayscale -&gt; RGB : kaokore/images_256/00003193.jpg Grayscale -&gt; RGB : kaokore/images_256/00003194.jpg Grayscale -&gt; RGB : kaokore/images_256/00003187.jpg Grayscale -&gt; RGB : kaokore/images_256/00003195.jpg Grayscale -&gt; RGB : kaokore/images_256/00003189.jpg 36% 3196/8848 [02:15&lt;02:05, 44.94it/s]Grayscale -&gt; RGB : kaokore/images_256/00003196.jpg Grayscale -&gt; RGB : kaokore/images_256/00003197.jpg Grayscale -&gt; RGB : kaokore/images_256/00003198.jpg Grayscale -&gt; RGB : kaokore/images_256/00003200.jpg 36% 3202/8848 [02:15&lt;02:07, 44.27it/s]Grayscale -&gt; RGB : kaokore/images_256/00003199.jpg 100% 8846/8848 [06:25&lt;00:00, 15.86it/s] 100% 8848/8848 [06:25&lt;00:00, 22.94it/s] . class Embedder: def __init__(self): self.image_embeddings=[] self.model = models.resnet18(pretrained=True).to(DEVICE) self.imsize=256 def load_images(self): self.dataset = ImageFolder(root=&#39;./kaokore&#39;,transform=torchvision.transforms.ToTensor()) self.loader = DataLoader(self.dataset) def embed_images(self): for x,_ in tqdm(self.loader): x = x.to(DEVICE) fx = self.model(x) self.image_embeddings.append(fx.detach().cpu().numpy().flatten()) def generate_estimator(self):#,model_fname): knn_estimator = NearestNeighbors(n_neighbors=5, metric=&quot;cosine&quot;) knn_estimator.fit(np.array(self.image_embeddings)) # save the model to disk ? # joblib.dump(knn_estimator, model_fname) return knn_estimator def embed_single_image(self,image_fpath): &quot;&quot;&quot;load image, returns cuda tensor&quot;&quot;&quot; loader = transforms.Compose([transforms.Scale(self.imsize), transforms.ToTensor()]) image = Image.open(image_fpath) image = loader(image).float() image = Variable(image, requires_grad=True) image = image.unsqueeze(0) #this is for VGG, may not be needed for ResNet image = image.to(DEVICE) embedding = self.model(image).detach().cpu().numpy().flatten() return embedding def query_knn(self, query_embedding): _, top_indices = self.estimator.kneighbors([query_embedding]) # find k nearest train neighbours top_indices=top_indices.flatten() return top_indices . # Fit kNN model on embedded images k = 5 image_embedder = Embedder() print(&#39;Loading images...&#39;) image_embedder.load_images() print(&#39;Embedding Kaokore paintings with a neural network...&#39;) image_embedder.embed_images() . Loading images... Embedding Kaokore paintings with a neural network... . print(f&#39;Fitting and saving {k}-nearest-neighbour model on image embeddings...&#39;) knn=image_embedder.generate_estimator() . Fitting and saving 5-nearest-neighbour model on image embeddings... . knn . NearestNeighbors(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;cosine&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0) . test1=&#39;/content/63212182948__89FADCD6-CDD0-4821-A046-9C3237E9732B.jpeg&#39; test2=&#39;/content/63212325784__F25E76D5-9E72-408A-9256-2BFE5A7E4895.jpeg&#39; test3=&#39;/content/test.jpg&#39; images_dataset = ImageFolder(root=&#39;./kaokore&#39;) . from PIL import Image import matplotlib.pyplot as plt results=[] test_img_fpaths = [test1,test2,test3] # inferrer = Inference(image_embedder) for img in test_img_fpaths: print(img) test_embedding= image_embedder.embed_single_image(img) _,top_k_indices = knn.kneighbors([test_embedding]) top_k_indices=top_k_indices.flatten() # top_k_indices = image_embedder.query_knn(test_embedding) print(f&#39;top {k} indices found were n n{top_k_indices}&#39;) print(&#39;Saving a grid of these images...&#39;) results.append(top_k_indices) . /content/63212182948__89FADCD6-CDD0-4821-A046-9C3237E9732B.jpeg top 5 indices found were [5891 6531 3822 7168 2401] Saving a grid of these images... /content/63212325784__F25E76D5-9E72-408A-9256-2BFE5A7E4895.jpeg . /usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead. &#34;please use transforms.Resize instead.&#34;) /usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead. &#34;please use transforms.Resize instead.&#34;) . top 5 indices found were [7182 6006 139 5121 4420] Saving a grid of these images... /content/test.jpg top 5 indices found were [6648 2944 1457 2391 6892] Saving a grid of these images... . /usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead. &#34;please use transforms.Resize instead.&#34;) . from IPython.display import display, Image img_list1 = [images_dataset[i][0] for i in results[0]] img_list2 = [images_dataset[i][0] for i in results[1]] img_list3 = [images_dataset[i][0] for i in results[2]] . Now, let&#39;s see what paintings match a picture of me... . display(*img_list3) . Work in progress... . I hope to fine-tune a triplet-loss or self-supervised model on the paintings at some point. This should result in better matches. Stay tuned! .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/11/kaokore.html",
            "relUrl": "/jupyter/2021/01/11/kaokore.html",
            "date": " ‚Ä¢ Jan 11, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Everything but the Model",
            "content": "Background . Most of the attention with regard to machine learning goes to the models, with good reason. We are at the point where we can reasonably model any $X to y$ relationship we wish. Difficult classification and clustering problems are increasingly tractable with clever approaches in statistical and rules-based learning. Even if we have no clue how to begin to go about developing such an approach (or such an approach would be intractable / biased), we can train deep neural networks ‚Äì Universal Approximators ‚Äì to theoretically represent almost any $f(x)= y = Wx + b$ relationship given properly trained weights $W$. . Still, humans are likely to be of use for the foreseeable future, and one of the more useful domains is the preparation of dat for such modeling. This includes topics such as feature selection and feature extraction, and how we might select the best model based on a rigorous, generalizable evaluation metric. . In this post, I will outline some of these useful topics with the hope that you will be able to tackle most everything about machine learning but the machine itself. . Example Dataset . I will use the COVID-19 Cell Atlas&#39; nasal immunodefiency swab dataset (https://www.sanger.ac.uk/group/vento-tormo-group/) for examples to follow. This is a recent dataset of real patients. I will use the scanpy package to load in the data and take a pandas dataframe for examples for feed into scikit-learn. . For the response variable, I will use Vasoactive agents required during hospitalization, a proxy for severity of symptoms and infection. . CellType log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Lab number Donor Id Age Sex Race ... Pre-existing Hypertension Pre-existing immunocompromised condition Smoking SARS-CoV-2 PCR SARS-CoV-2 Ab Symptomatic Admitted to hospital Highest level of respiratory support Vasoactive agents required during hospitalization 28-day death . GW1_AAACGGGAGCTAGTCT-1 Secretary epithelium | 14.909096 | 5687 | 0.042059 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AAAGTAGTCCTAGGGC-1 Secretary epithelium KRT5 | 13.611947 | 3967 | 0.097771 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AACACGTCAGCGTCCA-1 Ciliated epithelium | 9.366322 | 513 | 0.036419 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AACCATGAGAATCTCC-1 Secretary epithelium | 15.217731 | 6260 | 0.053720 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AACCATGCATCCTTGC-1 Ciliated epithelium | 9.134426 | 439 | 0.016043 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . 5 rows √ó 26 columns . X.columns . Index([&#39;CellType&#39;, &#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;, &#39;MT_fraction&#39;, &#39;Viral Molecules&#39;, &#39;Lab number&#39;, &#39;Donor Id&#39;, &#39;Age&#39;, &#39;Sex&#39;, &#39;Race&#39;, &#39;Ethnicity&#39;, &#39;BMI&#39;, &#39;Pre-existing heart disease&#39;, &#39;Pre-existing lung disease&#39;, &#39;Pre-existing kidney disease&#39;, &#39;Pre-existing diabetes&#39;, &#39;Pre-existing Hypertension&#39;, &#39;Pre-existing immunocompromised condition&#39;, &#39;Smoking&#39;, &#39;SARS-CoV-2 PCR&#39;, &#39;SARS-CoV-2 Ab&#39;, &#39;Symptomatic&#39;, &#39;Admitted to hospital&#39;, &#39;Highest level of respiratory support&#39;, &#39;28-day death&#39;], dtype=&#39;object&#39;) . This is a good toy dataset. We have a mixture of categorical, continuous-valued, integer-valued, string-valued, and others, as well as a clean binary Yes or No response variable. . Wrangling . A few short operations will make life easier later on. . Dealing with NaN (Missing Values) . You can see in the SARS-CoV-2 Ab column that we have NaN values. Although classifier implementations may have built-in accommodations, it may best best to deal with these values in a way we can fully control. . print(X[&#39;SARS-CoV-2 Ab&#39;]) . GW1_AAACGGGAGCTAGTCT-1 NaN GW1_AAAGTAGTCCTAGGGC-1 NaN GW1_AACACGTCAGCGTCCA-1 NaN GW1_AACCATGAGAATCTCC-1 NaN GW1_AACCATGCATCCTTGC-1 NaN ... GW13_TTTCCTCCAAGCCTAT-1 NaN GW13_TTTCCTCCAAGTCTGT-1 NaN GW13_TTTGTCAAGCCCAATT-1 NaN GW13_TTTGTCAGTAGGACAC-1 NaN GW13_TTTGTCATCGTGTAGT-1 NaN Name: SARS-CoV-2 Ab, Length: 4936, dtype: category Categories (3, object): [&#39;Not done&#39; &lt; &#39;Negative&#39; &lt; &#39;Positive&#39;] . I suspect this is just an indicator variable to show whether or not the patient recieved an antibody test, we can look at unique values to be sure. . X[&#39;SARS-CoV-2 Ab&#39;].unique() . [NaN] Categories (0, object): [] . It is. In this case, let&#39;s just drop the column rather. It is likely uninformative with regard to the reponse variable y = Vasoactive agents required during hospitalization. . X = X.drop(columns=[&#39;SARS-CoV-2 Ab&#39;]) print(X.shape,dataset.obs.shape, sep=&#39; n&#39;) . (4936, 24) (4936, 26) . As expected. . Dealing with Categorial Features . We will need to encode nominal and/or ordinal features to a one-hot representation. We can easily exclude numerical-valued columns from this process. We should also binarize the $y$ labels. . numer_cols = list(X._get_numeric_data().columns) cat_cols = list(set(X.columns) - set(numer_cols)) print(f&#39;numerical columns: n{numer_cols} n ncategorical columns: n {cat_cols}&#39;) . numerical columns: [&#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;, &#39;MT_fraction&#39;, &#39;Viral Molecules&#39;] categorical columns: [&#39;Race&#39;, &#39;Lab number&#39;, &#39;Pre-existing immunocompromised condition&#39;, &#39;Ethnicity&#39;, &#39;28-day death&#39;, &#39;Admitted to hospital&#39;, &#39;Pre-existing diabetes&#39;, &#39;BMI&#39;, &#39;Pre-existing Hypertension&#39;, &#39;Donor Id&#39;, &#39;Pre-existing kidney disease&#39;, &#39;Age&#39;, &#39;Symptomatic&#39;, &#39;Pre-existing heart disease&#39;, &#39;Pre-existing lung disease&#39;, &#39;Smoking&#39;, &#39;Sex&#39;, &#39;Highest level of respiratory support&#39;, &#39;CellType&#39;, &#39;SARS-CoV-2 PCR&#39;] . With the exception of Age (should be numeric) and BMI (should be ordinal), this looks fine (since we have a BMI range, we should use an ordinal encoder in this case). The rest are simply categorical. . numer_cols.append(&#39;Age&#39;) cat_cols.remove(&#39;Age&#39;) print(f&#39;numerical columns: n{numer_cols} n ncategorical columns: n {cat_cols}&#39;) . numerical columns: [&#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;, &#39;MT_fraction&#39;, &#39;Viral Molecules&#39;, &#39;Age&#39;] categorical columns: [&#39;Race&#39;, &#39;Lab number&#39;, &#39;Pre-existing immunocompromised condition&#39;, &#39;Ethnicity&#39;, &#39;28-day death&#39;, &#39;Admitted to hospital&#39;, &#39;Pre-existing diabetes&#39;, &#39;BMI&#39;, &#39;Pre-existing Hypertension&#39;, &#39;Donor Id&#39;, &#39;Pre-existing kidney disease&#39;, &#39;Symptomatic&#39;, &#39;Pre-existing heart disease&#39;, &#39;Pre-existing lung disease&#39;, &#39;Smoking&#39;, &#39;Sex&#39;, &#39;Highest level of respiratory support&#39;, &#39;CellType&#39;, &#39;SARS-CoV-2 PCR&#39;] . ord_cols = [&#39;BMI&#39;] cat_cols = [i for i in cat_cols if i not in ord_cols] X[cat_cols].head(3) . Race Lab number Pre-existing immunocompromised condition Ethnicity 28-day death Admitted to hospital Pre-existing diabetes Pre-existing Hypertension Donor Id Pre-existing kidney disease Symptomatic Pre-existing heart disease Pre-existing lung disease Smoking Sex Highest level of respiratory support CellType SARS-CoV-2 PCR . GW1_AAACGGGAGCTAGTCT-1 White | CV19-1-S3.2A | Yes | Not Hispanic or Latino | No | Yes | No | No | GWAS_1 | No | Yes | No | No | Never or unknown | F | Mechanical ventilation with intubation | Secretary epithelium | Positive | . GW1_AAAGTAGTCCTAGGGC-1 White | CV19-1-S3.2A | Yes | Not Hispanic or Latino | No | Yes | No | No | GWAS_1 | No | Yes | No | No | Never or unknown | F | Mechanical ventilation with intubation | Secretary epithelium KRT5 | Positive | . GW1_AACACGTCAGCGTCCA-1 White | CV19-1-S3.2A | Yes | Not Hispanic or Latino | No | Yes | No | No | GWAS_1 | No | Yes | No | No | Never or unknown | F | Mechanical ventilation with intubation | Ciliated epithelium | Positive | . Now we can encode these properly using scikit-learn or a builtin pandas method (we will use the latter). While we could use an ordinal encoder, whereby one clas is mapped to an integer, we should actually use one-hot encoding as this is a continuous input, valid for scikit-learn estimators. Note that a NaN is treated as a distinct category, and note how Pandas will automatically ignore numeerical columns of X. We just need to watch out for the BMI variable since it should be ordinal. . We then can append the BMI column with ordinal information. . import pandas as pd X_cat = pd.get_dummies(X[cat_cols]) X_cat.head(2) . Race_White Race_Black Race_Asian Race_Other Lab number_CV19-1-S3.2A Lab number_CV19-11-S3.2A Lab number_CV19-12-S3.2A Lab number_CV19-13-S3.2A Pre-existing immunocompromised condition_No Pre-existing immunocompromised condition_Yes ... CellType_Secretary epithelium KRT5 CellType_Squamous epithelium 1 CellType_Squamous epithelium 2 CellType_Ciliated epithelium CellType_Neutrophil CellType_Erythrocytes CellType_Low quality CellType_filtered cells and doublets SARS-CoV-2 PCR_Positive SARS-CoV-2 PCR_Negative . GW1_AAACGGGAGCTAGTCT-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . GW1_AAAGTAGTCCTAGGGC-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 rows √ó 65 columns . X[&#39;BMI&#39;].unique() . [&#39;30.0-39.9 (obese)&#39;, &#39;25.0-29.9 (overweight)&#39;, &#39;Unknown&#39;] Categories (3, object): [&#39;25.0-29.9 (overweight)&#39; &lt; &#39;30.0-39.9 (obese)&#39; &lt; &#39;Unknown&#39;] . scale_mapper = {&#39;Unknown&#39;:1,&#39;25.0-29.9 (overweight)&#39;:2,&#39;30.0-39.9 (obese)&#39;:3} X_ord = X[&#39;BMI&#39;].replace(scale_mapper) X_ord . GW1_AAACGGGAGCTAGTCT-1 3 GW1_AAAGTAGTCCTAGGGC-1 3 GW1_AACACGTCAGCGTCCA-1 3 GW1_AACCATGAGAATCTCC-1 3 GW1_AACCATGCATCCTTGC-1 3 .. GW13_TTTCCTCCAAGCCTAT-1 1 GW13_TTTCCTCCAAGTCTGT-1 1 GW13_TTTGTCAAGCCCAATT-1 1 GW13_TTTGTCAGTAGGACAC-1 1 GW13_TTTGTCATCGTGTAGT-1 1 Name: BMI, Length: 4936, dtype: int64 . We now have X_cat and X_ord. All that&#39;s left is to binarize the response variable y. . from sklearn import preprocessing y = preprocessing.LabelEncoder().fit_transform(y) y . array([1, 1, 1, ..., 0, 0, 0]) . Feature Rescaling . Before we combine these encoded categorical columns, it may be prudent to rescale numerical features, especially if each feature is not on the same given scale. This may be done to remove the bias of certain features given downstream tasks. For instance, take the following features from our $X$: log2p1_RNAcount and nFeature_RNA. . X[[&#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;]].head() . log2p1_RNAcount nFeature_RNA . GW1_AAACGGGAGCTAGTCT-1 14.909096 | 5687 | . GW1_AAAGTAGTCCTAGGGC-1 13.611947 | 3967 | . GW1_AACACGTCAGCGTCCA-1 9.366322 | 513 | . GW1_AACCATGAGAATCTCC-1 15.217731 | 6260 | . GW1_AACCATGCATCCTTGC-1 9.134426 | 439 | . In an extraction process (such as PCA, making use of covariance), or when using a classifier making use of Euclidean distance, the feature with the largest numerical range will be naturally more weighted. . x_1 = X[&#39;log2p1_RNAcount&#39;] x_2 = X[&#39;nFeature_RNA&#39;] range_1 = x_1.max()-x_1.min() range_2 = x_2.max()-x_2.min() print(range_1, range_2, sep=&#39; n&#39;) . 12.476429788752128 9651 . nFeature_RNA therefore has a much more significant bearing on an outcome contingent upon this range. . We therefore rescale in a few ways: . 1) with min/max rescaling $x_i&#39; = frac{x_i - min(x)}{max(x) - min(x)}$ | simple, preserves mean of dataset. Useful for image pixel intensity, for instance. | . | 2) with $z$-score normalization . $x_i&#39; = frac{x_i - bar{x}}{ sigma}$ = $ text{number of standard deviations from the mean}$ = $z text{-score}$ | standardizes features, typically with $ mu = 0, sigma^2 = 1$. This is a better choice than min/max for things like PCA since in that case we want to select components maximizing variance of the feature matrix, without getting caught up by the scale of that variance. | . | 3) with median and interquartile range rescaling (robust rescaling). . Remove the median value and scale according to interquartile range. | better choice if there are significant outliers. | . | . Let&#39;s go with 3) somewhat arbitrarily but also in the case of outliers. . from sklearn import preprocessing robust_scaler = preprocessing.RobustScaler() X_numer = robust_scaler.fit_transform(X[numer_cols]) X_numer . array([[ 2.10213024, 5.15834811, 0.35863219, 0. , -0.76190476], [ 1.52049401, 3.41525209, 2.21766008, 0. , -0.76190476], [-0.38322616, -0.08512795, 0.17041353, 0. , -0.76190476], ..., [ 1.54686863, 2.91867241, 0.05551372, 0. , 0. ], [-0.48147742, -0.28781353, 0.42904635, 0. , 0. ], [-0.26773767, -0.06080568, -0.07087445, 0. , 0. ]]) . Finally, we can recombine these processed data. . X[numer_cols] = X_numer X.head(2) . CellType log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Lab number Donor Id Age Sex Race ... Pre-existing kidney disease Pre-existing diabetes Pre-existing Hypertension Pre-existing immunocompromised condition Smoking SARS-CoV-2 PCR Symptomatic Admitted to hospital Highest level of respiratory support 28-day death . GW1_AAACGGGAGCTAGTCT-1 Secretary epithelium | 2.102130 | 5.158348 | 0.358632 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AAAGTAGTCCTAGGGC-1 Secretary epithelium KRT5 | 1.520494 | 3.415252 | 2.217660 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . 2 rows √ó 24 columns . X[&#39;BMI&#39;] = X_ord X[&#39;BMI&#39;] . GW1_AAACGGGAGCTAGTCT-1 3 GW1_AAAGTAGTCCTAGGGC-1 3 GW1_AACACGTCAGCGTCCA-1 3 GW1_AACCATGAGAATCTCC-1 3 GW1_AACCATGCATCCTTGC-1 3 .. GW13_TTTCCTCCAAGCCTAT-1 1 GW13_TTTCCTCCAAGTCTGT-1 1 GW13_TTTGTCAAGCCCAATT-1 1 GW13_TTTGTCAGTAGGACAC-1 1 GW13_TTTGTCATCGTGTAGT-1 1 Name: BMI, Length: 4936, dtype: int64 . X_concat = pd.concat([X_cat,X.drop(columns=cat_cols)],axis=1) X_concat . Race_White Race_Black Race_Asian Race_Other Lab number_CV19-1-S3.2A Lab number_CV19-11-S3.2A Lab number_CV19-12-S3.2A Lab number_CV19-13-S3.2A Pre-existing immunocompromised condition_No Pre-existing immunocompromised condition_Yes ... CellType_Low quality CellType_filtered cells and doublets SARS-CoV-2 PCR_Positive SARS-CoV-2 PCR_Negative log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age BMI . GW1_AAACGGGAGCTAGTCT-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 2.102130 | 5.158348 | 0.358632 | 0.0 | -0.761905 | 3 | . GW1_AAAGTAGTCCTAGGGC-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 1.520494 | 3.415252 | 2.217660 | 0.0 | -0.761905 | 3 | . GW1_AACACGTCAGCGTCCA-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | -0.383226 | -0.085128 | 0.170414 | 0.0 | -0.761905 | 3 | . GW1_AACCATGAGAATCTCC-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 2.240521 | 5.739042 | 0.747728 | 0.0 | -0.761905 | 3 | . GW1_AACCATGCATCCTTGC-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | -0.487207 | -0.160122 | -0.509505 | 0.0 | -0.761905 | 3 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . GW13_TTTCCTCCAAGCCTAT-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0.274377 | 0.276666 | -0.477400 | 0.0 | 0.000000 | 1 | . GW13_TTTCCTCCAAGTCTGT-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0.378640 | 0.409425 | -0.530748 | 0.0 | 0.000000 | 1 | . GW13_TTTGTCAAGCCCAATT-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 1.546869 | 2.918672 | 0.055514 | 0.0 | 0.000000 | 1 | . GW13_TTTGTCAGTAGGACAC-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | -0.481477 | -0.287814 | 0.429046 | 0.0 | 0.000000 | 1 | . GW13_TTTGTCATCGTGTAGT-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | -0.267738 | -0.060806 | -0.070874 | 0.0 | 0.000000 | 1 | . 4936 rows √ó 71 columns . Feature Extraction . Feature extraction is the process of reducing dimensionality to find (important) latent features in a given feature set. This may be useful in the case of our dataset since our encoded feature size has swelled to 71! The motivation is also very philosophical. What is the self, if not the most important features that others learn to associate your identity with? Your identity in the world is one of useful features learned by others. We will see how we can extract and/or learn a representation of features such that a model operating on such a representation can generalize to as many of the original features as possible. . Extraction can be done in a variety of ways for a variety of use cases. As for why, let&#39;s say we have a dataset with a massive number of features, such that training a network to make use of all the features somewhat equally. We want to use as few (latent) features as possible to completely represent the original dataset. That is, we are not selecting features, but rather finding a new, smaller feature set derivative of the original. This may be a linear mapping, or a non-linear manifold-learning process. . Methods Outlined and a Simple Autoencoder . Feature learning can be accomplished via common dimensionality reduction methods such as PCA, UMAP (shown above on dataset), and tSNE (technically, a non-linear method of extracting features). Linear Discriminant Analysis (LDA) is a useful technique to maximize class separation when labels are known. I will mention that when in doubt, use UMAP. It&#39;s very effective, extensible, and can even be used as a quasi-clustering technique. . Another method of feature-representation is of course the autoencoder and variational autoencoder. The former works to reconstruct the feature set based upon an encoder-decoder neural network, trained with gradient descent to learn weights. We then discard the decoder layer and have are left with a function useful for identifying critical latent representations of the original features. . In the vanilla autoencoder&#39;s case, a single value for each encoding dimension is found (this is usually the final layer of a multilayer perceptron, or a linear head stacked upon a convolutional net or other flavor). In the variational case, the network&#39;s encoder learns a probability distribution for each latent attribute. The decoder then randomly samples from this space to generate an input vector for the decoder. This enforces a smooth representation of latent space, allowing for variational expectation maximization / inference. As far as feature learning goes, allows for latent features to be maximally disentangled, as an isotropic Gaussian (such that any single dimension / single Gaussian of this multivariate distribution over features is not covariant with any other). . I will show a simple autoencoder implementation below using the neat pytorch-lightning package. We will train and the reconstructed features will be compared to the original. . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) . import torch from torch import nn, optim from torch.utils.data import random_split, DataLoader,Dataset from torch.nn import functional as F from torchvision import transforms import pytorch_lightning as pl class CovidDataset(Dataset): def __init__(self,X,y): self.data = torch.FloatTensor(X.values.astype(&#39;float&#39;)) self.labels = torch.LongTensor(y) #not needed for autoencoder. def __len__(self): return len(self.data) def __getitem__(self,index): return self.data[index], self.labels[index] . CovidDataset(X_test,y_test)[1] . (tensor([ 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.4500, 0.7216, -0.4589, 0.0000, 0.0000, 1.0000]), tensor(0)) . class CovidDataModule(pl.LightningDataModule): def __init__(self,train_dataset,val_dataset): super().__init__() self.train_dataset = train_dataset self.val_dataset = val_dataset def train_dataloader(self): return DataLoader(self.train_dataset, batch_size=64) def val_dataloader(self): return DataLoader(self.val_dataset, batch_size=64) . covid_dm = CovidDataModule(CovidDataset(X_train,y_train), CovidDataset(X_test,y_test)) covid_dm . &lt;__main__.CovidDataModule at 0x7f9b1ee75ac0&gt; . import torch from torch import nn, optim from torch.utils.data import random_split, DataLoader from torch.nn import functional as F import torch.nn as nn from torchvision import transforms import pytorch_lightning as pl class CovidAutoencoder(pl.LightningModule): def __init__(self): super().__init__() self.encoder = nn.Sequential( nn.Linear(71,128), nn.ReLU(inplace=True), nn.Linear(128, 64), nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3) ) self.decoder = nn.Sequential( nn.Linear(3, 12), nn.ReLU(True), nn.Linear(12, 64), nn.ReLU(True), nn.Linear(64, 128), nn.ReLU(True), nn.Linear(128, 71, nn.Tanh()) ) def forward(self, x): x = self.encoder(x) x = self.decoder(x) return x def configure_optimizers(self): return torch.optim.Adam(self.parameters(), lr=1e-3) def training_step(self, batch, batch_idx): x, _ = batch #no need for labels fx = self(x) loss = F.mse_loss(x, fx) self.log(&#39;train_loss&#39;, loss) return loss def validation_step(self, batch, batch_idx): x, _ = batch #no need for labels fx = self(x) loss = F.mse_loss(x, fx) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) . model = CovidAutoencoder() trainer = pl.Trainer(fast_dev_run=False,max_epochs=25) trainer.fit(model=model, datamodule=covid_dm) . GPU available: False, used: False GPU available: False, used: False TPU available: None, using: 0 TPU cores TPU available: None, using: 0 TPU cores | Name | Type | Params 0 | encoder | Sequential | 18.3 K 1 | decoder | Sequential | 18.4 K 36.6 K Trainable params 0 Non-trainable params 36.6 K Total params | Name | Type | Params 0 | encoder | Sequential | 18.3 K 1 | decoder | Sequential | 18.4 K 36.6 K Trainable params 0 Non-trainable params 36.6 K Total params Epoch 0: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 202.21it/s, loss=0.205, v_num=11, val_loss_epoch=0.479] Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 253.67it/s, loss=0.205, v_num=11, val_loss_epoch=0.194, val_loss_step=0.133] Epoch 1: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 203.16it/s, loss=0.161, v_num=11, val_loss_epoch=0.194, val_loss_step=0.133] Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 256.48it/s, loss=0.161, v_num=11, val_loss_epoch=0.138, val_loss_step=0.126] Epoch 2: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 196.89it/s, loss=0.0841, v_num=11, val_loss_epoch=0.138, val_loss_step=0.126] Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 248.57it/s, loss=0.0841, v_num=11, val_loss_epoch=0.0676, val_loss_step=0.0759] Epoch 3: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 198.54it/s, loss=0.0484, v_num=11, val_loss_epoch=0.0676, val_loss_step=0.0759] Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 251.11it/s, loss=0.0484, v_num=11, val_loss_epoch=0.0427, val_loss_step=0.0468] Epoch 4: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 195.16it/s, loss=0.041, v_num=11, val_loss_epoch=0.0427, val_loss_step=0.0468] Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 245.66it/s, loss=0.041, v_num=11, val_loss_epoch=0.0385, val_loss_step=0.0436] Epoch 5: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 192.33it/s, loss=0.0365, v_num=11, val_loss_epoch=0.0385, val_loss_step=0.0436] Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 243.62it/s, loss=0.0365, v_num=11, val_loss_epoch=0.0336, val_loss_step=0.0375] Epoch 6: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 203.55it/s, loss=0.0271, v_num=11, val_loss_epoch=0.0336, val_loss_step=0.0375] Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 255.54it/s, loss=0.0271, v_num=11, val_loss_epoch=0.0266, val_loss_step=0.0288] Epoch 7: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 195.35it/s, loss=0.0223, v_num=11, val_loss_epoch=0.0266, val_loss_step=0.0288] Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 244.57it/s, loss=0.0223, v_num=11, val_loss_epoch=0.0223, val_loss_step=0.026] Epoch 8: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 201.95it/s, loss=0.018, v_num=11, val_loss_epoch=0.0223, val_loss_step=0.026] Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 252.41it/s, loss=0.018, v_num=11, val_loss_epoch=0.0182, val_loss_step=0.0233] Epoch 9: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 206.98it/s, loss=0.016, v_num=11, val_loss_epoch=0.0182, val_loss_step=0.0233] Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 259.34it/s, loss=0.016, v_num=11, val_loss_epoch=0.0163, val_loss_step=0.021] Epoch 10: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 197.63it/s, loss=0.0144, v_num=11, val_loss_epoch=0.0163, val_loss_step=0.021] Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 247.62it/s, loss=0.0144, v_num=11, val_loss_epoch=0.0148, val_loss_step=0.0187] Epoch 11: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 184.67it/s, loss=0.013, v_num=11, val_loss_epoch=0.0148, val_loss_step=0.0187] Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 231.88it/s, loss=0.013, v_num=11, val_loss_epoch=0.0135, val_loss_step=0.0167] Epoch 12: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 200.98it/s, loss=0.0119, v_num=11, val_loss_epoch=0.0135, val_loss_step=0.0167] Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 245.95it/s, loss=0.0119, v_num=11, val_loss_epoch=0.0125, val_loss_step=0.015] Epoch 13: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 194.80it/s, loss=0.0111, v_num=11, val_loss_epoch=0.0125, val_loss_step=0.015] Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 245.03it/s, loss=0.0111, v_num=11, val_loss_epoch=0.0118, val_loss_step=0.0139] Epoch 14: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 193.38it/s, loss=0.0104, v_num=11, val_loss_epoch=0.0118, val_loss_step=0.0139] Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 242.07it/s, loss=0.0104, v_num=11, val_loss_epoch=0.0113, val_loss_step=0.0131] Epoch 15: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 183.31it/s, loss=0.00997, v_num=11, val_loss_epoch=0.0113, val_loss_step=0.0131] Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 232.88it/s, loss=0.00997, v_num=11, val_loss_epoch=0.0108, val_loss_step=0.0122] Epoch 16: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 193.03it/s, loss=0.00961, v_num=11, val_loss_epoch=0.0108, val_loss_step=0.0122] Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 241.64it/s, loss=0.00961, v_num=11, val_loss_epoch=0.0105, val_loss_step=0.0116] Epoch 17: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 191.10it/s, loss=0.00933, v_num=11, val_loss_epoch=0.0105, val_loss_step=0.0116] Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 241.13it/s, loss=0.00933, v_num=11, val_loss_epoch=0.0102, val_loss_step=0.0111] Epoch 18: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 190.33it/s, loss=0.00905, v_num=11, val_loss_epoch=0.0102, val_loss_step=0.0111] Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 238.68it/s, loss=0.00905, v_num=11, val_loss_epoch=0.00986, val_loss_step=0.0105] Epoch 19: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 192.96it/s, loss=0.00879, v_num=11, val_loss_epoch=0.00986, val_loss_step=0.0105] Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 243.82it/s, loss=0.00879, v_num=11, val_loss_epoch=0.00951, val_loss_step=0.01] Epoch 20: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 193.37it/s, loss=0.00859, v_num=11, val_loss_epoch=0.00951, val_loss_step=0.01] Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 244.96it/s, loss=0.00859, v_num=11, val_loss_epoch=0.00927, val_loss_step=0.00952] Epoch 21: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 179.89it/s, loss=0.00841, v_num=11, val_loss_epoch=0.00927, val_loss_step=0.00952] Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 227.89it/s, loss=0.00841, v_num=11, val_loss_epoch=0.00903, val_loss_step=0.00911] Epoch 22: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 190.09it/s, loss=0.00819, v_num=11, val_loss_epoch=0.00903, val_loss_step=0.00911] Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 239.86it/s, loss=0.00819, v_num=11, val_loss_epoch=0.00873, val_loss_step=0.00874] Epoch 23: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 186.86it/s, loss=0.00799, v_num=11, val_loss_epoch=0.00873, val_loss_step=0.00874] Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 237.02it/s, loss=0.00799, v_num=11, val_loss_epoch=0.00845, val_loss_step=0.00826] Epoch 24: 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/78 [00:00&lt;00:00, 192.72it/s, loss=0.00781, v_num=11, val_loss_epoch=0.00845, val_loss_step=0.00826] Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 242.28it/s, loss=0.00781, v_num=11, val_loss_epoch=0.00818, val_loss_step=0.00798] Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00&lt;00:00, 236.67it/s, loss=0.00781, v_num=11, val_loss_epoch=0.00818, val_loss_step=0.00798] . 1 . In our case, mean square error is defined as $ frac{1}{n} sum_{i=1}^n( vec{x}-f( vec{x}))$. Thus, across the training dataset, we were able to train the network to reconstruct with a final loss (on a batch of 64 samples) of $ sim 0.008$, implying very low reconstructive loss despite only $3$ latent dimensions. . Feature Selection . Phew! Feature extraction, unsupervised learning, and data representation is its own discipline. We could spend days on that topic, and there are exciting developments along those lines ‚Äì think state of the art language models and graphical embeddings. A future project I have in mind is actually strictly feature extraction, whereby a deep health embedding across time-series health data could improve clinical decisionmaking. . One critical aspect of the extraction process is that you do lose interpretability. In addition, you may not only lose linearity between $X to X_{embdedded}$ but also determinism. Some of these methods are effective, but indeed stochastic. This is of course on top of the &quot;black box&quot; problem of learning systems in general (just try inspecting even the simple autoencoder to parse its choices in reconstruction). . This is all to say that selection of existing features is a valuable skill to have. We can choose features as humans have provided them, rather than extracting features, and have stronger explainability. We can also use feature extraction / dimensionality reduction after this step. . Note that the following $X$ and $y$ do not correspond to processed/extracted data. . Variance Thresholding . We can select those features that are highly variant. That is, we will omit features (the numerical ones) that are not highly variant across samples. | That is, our new feature set is such that $ hat{ vec{x}} = x_i&gt;Var(x_i)$ such that $Var(x) = left( frac{1}{n} sum_{i=1}^n(x_i- mu)^2 right) forall x_i in hat{ vec{x}}$, our original feature vector. | Note: this is not a good idea to do when features are not in identical units, as is the case here. | Also, if a feature has previously been standardized to zero-mean and unit-variance, this technique is useless since we&#39;ve already asserted the variance. | . As such, the following is purely for illustrative purposes. . X_numer = X[numer_cols] X_numer.head(3) . log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age . GW1_AAACGGGAGCTAGTCT-1 14.909096 | 5687 | 0.042059 | 0 | 18 | . GW1_AAAGTAGTCCTAGGGC-1 13.611947 | 3967 | 0.097771 | 0 | 18 | . GW1_AACACGTCAGCGTCCA-1 9.366322 | 513 | 0.036419 | 0 | 18 | . from sklearn.feature_selection import VarianceThreshold thresholder = VarianceThreshold(threshold=0.50) thresholder.fit_transform(X_numer).shape . (4936, 3) . X[numer_cols].shape . (4936, 5) . We&#39;ve eliminated two features from our dataset. . Handling Highly Correlated Features . First, we must check for correlated features. If we don&#39;t we could be including redundant information and overfit to our dataset unnecessarily. . X.head(4) . CellType log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Lab number Donor Id Age Sex Race ... Pre-existing kidney disease Pre-existing diabetes Pre-existing Hypertension Pre-existing immunocompromised condition Smoking SARS-CoV-2 PCR Symptomatic Admitted to hospital Highest level of respiratory support 28-day death . GW1_AAACGGGAGCTAGTCT-1 Secretary epithelium | 2.102130 | 5.158348 | 0.358632 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AAAGTAGTCCTAGGGC-1 Secretary epithelium KRT5 | 1.520494 | 3.415252 | 2.217660 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACACGTCAGCGTCCA-1 Ciliated epithelium | -0.383226 | -0.085128 | 0.170414 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACCATGAGAATCTCC-1 Secretary epithelium | 2.240521 | 5.739042 | 0.747728 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . 4 rows √ó 24 columns . X_corr = X.corr().abs() X_corr . log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age BMI . log2p1_RNAcount 1.000000 | 0.908318 | 0.001661 | 0.107501 | 0.519020 | 0.035823 | . nFeature_RNA 0.908318 | 1.000000 | 0.086117 | 0.025669 | 0.443298 | 0.144945 | . MT_fraction 0.001661 | 0.086117 | 1.000000 | 0.021125 | 0.314239 | 0.180895 | . Viral Molecules 0.107501 | 0.025669 | 0.021125 | 1.000000 | 0.041918 | 0.028833 | . Age 0.519020 | 0.443298 | 0.314239 | 0.041918 | 1.000000 | 0.081834 | . BMI 0.035823 | 0.144945 | 0.180895 | 0.028833 | 0.081834 | 1.000000 | . Note that pandas considers strictly continuous-valued, numerical features. We can see that the diagonal, by definition, is strictly unit-valued. However, more interesting observations are apparent. For instance, nFeature_RNA is highly correlated to log2p1_RNAcount. This makes sense, as we would expect the number of features to increase with the single-cell RNA counts (and probably logarithmically, as each feature corresponds to many such counts). . We could choose to drop any features with high correlation, or simply . import numpy as np def upper_triangle(df:pd.DataFrame): return df.where(np.triu(np.ones(df.shape),k=1).astype(bool)) X_corr_upper = upper_triangle(X_corr) X_corr_upper . log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age BMI . log2p1_RNAcount NaN | 0.908318 | 0.001661 | 0.107501 | 0.519020 | 0.035823 | . nFeature_RNA NaN | NaN | 0.086117 | 0.025669 | 0.443298 | 0.144945 | . MT_fraction NaN | NaN | NaN | 0.021125 | 0.314239 | 0.180895 | . Viral Molecules NaN | NaN | NaN | NaN | 0.041918 | 0.028833 | . Age NaN | NaN | NaN | NaN | NaN | 0.081834 | . BMI NaN | NaN | NaN | NaN | NaN | NaN | . to_drop = [c for c in X_corr_upper if any(X_corr_upper[c]&gt;0.90)] to_drop . [&#39;nFeature_RNA&#39;] . X_uncorrelated = X.drop(columns=to_drop) X_uncorrelated.head() . CellType log2p1_RNAcount MT_fraction Viral Molecules Lab number Donor Id Age Sex Race Ethnicity ... Pre-existing kidney disease Pre-existing diabetes Pre-existing Hypertension Pre-existing immunocompromised condition Smoking SARS-CoV-2 PCR Symptomatic Admitted to hospital Highest level of respiratory support 28-day death . GW1_AAACGGGAGCTAGTCT-1 Secretary epithelium | 2.102130 | 0.358632 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AAAGTAGTCCTAGGGC-1 Secretary epithelium KRT5 | 1.520494 | 2.217660 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACACGTCAGCGTCCA-1 Ciliated epithelium | -0.383226 | 0.170414 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACCATGAGAATCTCC-1 Secretary epithelium | 2.240521 | 0.747728 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACCATGCATCCTTGC-1 Ciliated epithelium | -0.487207 | -0.509505 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . 5 rows √ó 23 columns . As expected. . Removing Irrelevant Features for Supervised Learning . This is an analaous case to Linear Discriminant Analysis (LDA)1, in that we use the feature-target relationship to select only those features relevant to a target vector using a Chi-square ($ chi^2$) test for independence or an ANOVA (analysis of variance; for $ vec{x} in mathbb{R}$). In this fashion, we remove features that do not result in significant mean differences. This is not a classifier per se, but rather an intuitive method of eliminating features unlikely to be helpful in supervised settings. . . not to be confused with generative Latent Dirichlet Allocation, which allows sets of observations $ vec{y}$ to be explained by unobserved groups $ vec{x}_{unknown}$ that explain why some parts of the features of the data are similar).&#8617; . | Chi-Square Reduction of Categorical Features . We compute $ chi^2 = sum_i frac{(O_i-E_i)^2}{E^i}$. This is the sum of the number of class observations $O_i$ of a given categorical feature $i$ and the expected number $E_i$ if that feature was independent to the target vector. We then can select or omit feature $x_i$ based upon is $ chi^2$ value and the sklearn selector (a lower statistic is &quot;better&quot; in that the feature matches closely with what is expected, and as a result will be omitted). . from sklearn.feature_selection import chi2, SelectKBest X_cat.head(3) . 28-day death_No 28-day death_Yes Pre-existing diabetes_No Pre-existing diabetes_Yes Race_White Race_Black Race_Asian Race_Other Smoking_Never or unknown Smoking_Prior ... Lab number_CV19-1-S3.2A Lab number_CV19-11-S3.2A Lab number_CV19-12-S3.2A Lab number_CV19-13-S3.2A Sex_M Sex_F Pre-existing Hypertension_No Pre-existing Hypertension_Yes Pre-existing lung disease_No Pre-existing lung disease_Yes . GW1_AAACGGGAGCTAGTCT-1 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | . GW1_AAAGTAGTCCTAGGGC-1 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | . GW1_AACACGTCAGCGTCCA-1 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | . 3 rows √ó 65 columns . y . array([1, 1, 1, ..., 0, 0, 0]) . chi2_selector = SelectKBest(chi2,k=5) chi2_selector.fit_transform(X_cat,y) . array([[1, 0, 1, 0, 1], [1, 1, 1, 0, 1], [1, 0, 1, 0, 1], ..., [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0]], dtype=uint8) . ANOVA Reduction of Features . Now, for the continous-valued features, we can compute the ANOVA F-score between $X$ and $y$ and select the $k$-best similarly. Note that we could proceed with f_regression for continuous-valued labels. . In any case, we use ANOVA, a generalization of the Student&#39;s $t$-test from $2$ to $n$ means. We compute this by providing features as a set of regressors to be tested in sequence, such that we compute the $F$ statistic where $F= frac{ sigma^2_X}{ sigma^2_y}$, the ratio of the sample variances of the features and the target. This is the same as a ratio of $ frac{ text{explained variance}}{ text{unexplained variance}}$. Under the null hypothesis, we expect the value of $F$ to have an $F$-distribution. In other words, we can find dependence of $y$ on $X$, and as such use those features going forward. . X_numer.head() . log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age . GW1_AAACGGGAGCTAGTCT-1 14.909096 | 5687 | 0.042059 | 0 | 18 | . GW1_AAAGTAGTCCTAGGGC-1 13.611947 | 3967 | 0.097771 | 0 | 18 | . GW1_AACACGTCAGCGTCCA-1 9.366322 | 513 | 0.036419 | 0 | 18 | . GW1_AACCATGAGAATCTCC-1 15.217731 | 6260 | 0.053720 | 0 | 18 | . GW1_AACCATGCATCCTTGC-1 9.134426 | 439 | 0.016043 | 0 | 18 | . from sklearn.feature_selection import f_classif fvalue_selector = SelectKBest(f_classif, k=2) fvalue_selector.fit_transform(X_numer,y) . array([[5687., 18.], [3967., 18.], [ 513., 18.], ..., [3477., 50.], [ 313., 50.], [ 537., 50.]]) . We&#39;ve selected nFeature_RNA and Age. . cols = fvalue_selector.get_support(indices=True) X_numer_k_best = X_numer.iloc[:,cols] #note iloc for reproducibility in case of named columns. . Eliminating Features with Recursion . A final method of feature selection can be accomplished with Recursive Feature Elimination and Cross Validation. RFECV works by iterating on a supervised task, discarding those features sequentially until performance on a given metric (such as accuracy) diminishes. This is perhaps most useful when a given model is decided upon early, but the number of features is not practical. This is also an excellent retrospective step of development, as we can then explain in retrospect those features that tend to inform outcomes or not (a big plus for explainability). . I will demonstrate with a very simple model: logistic regression. We are therefore fitting a hyperplane to the data and taking the sigmoid function of the dependent response variable&#39;s occurrence via the log-odds function (&quot;logit&quot;; $ mathcal{l} = log_b frac{p}{1-p}$) . from sklearn.feature_selection import RFECV from sklearn import linear_model from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X_concat,y, test_size=0.33, random_state=100) logit_clf = linear_model.LogisticRegression() logit_clf.fit(X_train,y_train) logit_clf.score(X_test,y_test) . 1.0 . OK, so I didn&#39;t expect the baseline to be 100% accurate. Nevertheless, we can prune features! One at a time, we will initiate logistic regression, scoring by $-MSE(X,y)$. . logit_clf = linear_model.LogisticRegression() rfecv = RFECV(estimator = logit_clf, step=1, scoring=&#39;neg_mean_squared_error&#39;) rfecv.fit(X_train,y_train) rfecv.transform(X_train) . array([[2.], [2.], [2.], ..., [3.], [3.], [1.]]) . rfecv.n_features_ . 1 . (rfecv.support_) . array([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]) . np.where(rfecv.support_ == True) . (array([70]),) . X_train.columns[70] . &#39;BMI&#39; . Wow! This is truly surprising. We can predict COVID symptom severity (remember, y = dataset.obs[&#39;Vasoactive agents required during hospitalization&#39;]) with BMI only! Trying without BMI.... . X_train_alt = X_train.drop(columns=[&#39;BMI&#39;]) logit_clf = linear_model.LogisticRegression() rfecv_alt = RFECV(estimator = logit_clf, step=1, scoring=&#39;neg_mean_squared_error&#39;) rfecv_alt.fit(X_train_alt,y_train) rfecv_alt.transform(X_train_alt) . array([[0.], [0.], [0.], ..., [1.], [1.], [0.]]) . X_train_alt.columns[np.where(rfecv_alt.support_ == True)] . Index([&#39;Donor Id_GWAS_1&#39;], dtype=&#39;object&#39;) . Fascinating. We can rely on this Genome-Wide Association Study marker. The only issue is that Donor_id is likely specific to the clinical outcome, and is probably not a great physiological feature. However, the recursive, cross-validated feature elimination worked as intended. Let&#39;s reorder the columns per the RFECV ranking... . rfecv.ranking_ . array([22, 62, 21, 61, 3, 13, 19, 14, 63, 46, 16, 11, 51, 45, 56, 58, 44, 50, 69, 17, 10, 2, 66, 9, 18, 8, 71, 55, 64, 65, 68, 43, 53, 60, 42, 48, 67, 12, 7, 49, 59, 57, 15, 20, 6, 52, 5, 54, 38, 36, 25, 32, 31, 33, 40, 29, 23, 37, 30, 27, 39, 35, 34, 47, 70, 28, 24, 26, 41, 4, 1]) . sorted(zip(X_train.columns,rfecv.ranking_)) . [(&#39;28-day death_No&#39;, 45), (&#39;28-day death_Yes&#39;, 56), (&#39;Admitted to hospital_No&#39;, 58), (&#39;Admitted to hospital_Yes&#39;, 44), (&#39;Age&#39;, 4), (&#39;BMI&#39;, 1), (&#39;CellType_Bcells 1&#39;, 38), (&#39;CellType_Bcells 2&#39;, 36), (&#39;CellType_Bcells 3&#39;, 25), (&#39;CellType_Ciliated epithelium&#39;, 30), (&#39;CellType_Erythrocytes&#39;, 39), (&#39;CellType_Low quality&#39;, 35), (&#39;CellType_Mononuclear phagocytes&#39;, 33), (&#39;CellType_Neutrophil&#39;, 27), (&#39;CellType_Secretary epithelium&#39;, 40), (&#39;CellType_Secretary epithelium KRT5&#39;, 29), (&#39;CellType_Squamous epithelium 1&#39;, 23), (&#39;CellType_Squamous epithelium 2&#39;, 37), (&#39;CellType_Tcells CD4&#39;, 32), (&#39;CellType_Tcells CD8&#39;, 31), (&#39;CellType_filtered cells and doublets&#39;, 34), (&#39;Donor Id_GWAS_1&#39;, 2), (&#39;Donor Id_GWAS_10&#39;, 66), (&#39;Donor Id_GWAS_11&#39;, 9), (&#39;Donor Id_GWAS_12&#39;, 18), (&#39;Donor Id_GWAS_13&#39;, 8), (&#39;Donor Id_GWAS_2&#39;, 71), (&#39;Donor Id_GWAS_3&#39;, 55), (&#39;Donor Id_GWAS_4&#39;, 64), (&#39;Donor Id_GWAS_5&#39;, 65), (&#39;Donor Id_GWAS_8&#39;, 68), (&#39;Ethnicity_Hispanic or Latino&#39;, 11), (&#39;Ethnicity_Not Hispanic or Latino&#39;, 16), (&#39;Ethnicity_Unknown or not documented&#39;, 51), (&#39;Highest level of respiratory support_Mechanical ventilation with intubation&#39;, 6), (&#39;Highest level of respiratory support_Non-invasive ventilation (CPAP, BiPAP, HFNC)&#39;, 52), (&#39;Highest level of respiratory support_None&#39;, 54), (&#39;Highest level of respiratory support_Supplemental O2&#39;, 5), (&#39;Lab number_CV19-1-S3.2A&#39;, 3), (&#39;Lab number_CV19-11-S3.2A&#39;, 13), (&#39;Lab number_CV19-12-S3.2A&#39;, 19), (&#39;Lab number_CV19-13-S3.2A&#39;, 14), (&#39;MT_fraction&#39;, 26), (&#39;Pre-existing Hypertension_No&#39;, 17), (&#39;Pre-existing Hypertension_Yes&#39;, 10), (&#39;Pre-existing diabetes_No&#39;, 50), (&#39;Pre-existing diabetes_Yes&#39;, 69), (&#39;Pre-existing heart disease_No&#39;, 48), (&#39;Pre-existing heart disease_Yes&#39;, 67), (&#39;Pre-existing immunocompromised condition_No&#39;, 63), (&#39;Pre-existing immunocompromised condition_Yes&#39;, 46), (&#39;Pre-existing kidney disease_No&#39;, 43), (&#39;Pre-existing kidney disease_Yes&#39;, 53), (&#39;Pre-existing lung disease_No&#39;, 12), (&#39;Pre-existing lung disease_Yes&#39;, 7), (&#39;Race_Asian&#39;, 21), (&#39;Race_Black&#39;, 62), (&#39;Race_Other&#39;, 61), (&#39;Race_White&#39;, 22), (&#39;SARS-CoV-2 PCR_Negative&#39;, 70), (&#39;SARS-CoV-2 PCR_Positive&#39;, 47), (&#39;Sex_F&#39;, 20), (&#39;Sex_M&#39;, 15), (&#39;Smoking_Current&#39;, 57), (&#39;Smoking_Never or unknown&#39;, 49), (&#39;Smoking_Prior&#39;, 59), (&#39;Symptomatic_No&#39;, 60), (&#39;Symptomatic_Yes&#39;, 42), (&#39;Viral Molecules&#39;, 41), (&#39;log2p1_RNAcount&#39;, 28), (&#39;nFeature_RNA&#39;, 24)] . Conclusion . This concludes discussion of feature-based analysis of everything but the model. In future posts, I will outline how you may efficiently perform model selection and optimization. For now, I hope this has helped in gaining a feel for performing the processing beyond reading in data but before jumping into true machine learning. This ties in nicely with discussions of far more advanced topics, such as regularization and modeling using deep learning. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/09/all-but-the-model.html",
            "relUrl": "/jupyter/2021/01/09/all-but-the-model.html",
            "date": " ‚Ä¢ Jan 9, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Probability for Computational Biologists",
            "content": "Introduction . This will be a comprehensive outline of topics in probability, including random variables, distributions, and rules of probability. My hope is that this will serve as a picture of (almost) everything you need may need to know regarding probability. . Topics such as statistical and deep learning leverage these concepts as assumed knowledge. It is therefore vital to have at least working knowledge of what is to come. . Counting . Multiplication Rule . Given an experiment with many potential outcomes, we can simply multiply the number of outcomes at each step for the overall number of possible outcomes. . $n_{total} = prod_i n_i$ . Sampling . Sampling $k$ items from a population of size $n$ results in the number of possibilities as follows: . Replacement (place each of the $k$ samples back into the population after choosing!) . Order-Preserving (we count each unique order in which we select the $k$ samples) $n^k$ | . | Order Doesn&#39;t Matter (we only care about the class membership of the $k samples) $ {n+k-1} choose{k} $ | . | . | No Replacement . Order-Preserving $ frac{n!}{(n-k)!}$ | A permutations problem. | . | Order Doesn&#39;t Matter $n choose k$ = $ frac{n!}{k!(n-k)!}$ | A combinations problem; use the binomial coefficient | . | . | . Na&#239;ve Probability . If all outcomes in a given event space are equally likely, $P_{naive}(X=x_i) = frac{ text{outcomes favorable to } x_i}{ text{number of outcomes}}$ . | This is intuitive. The probability of heads given a strictly fair coin (i.e., $X sim Bernoulli(p=.50)$) over two trials is $ frac{1}{2}$. . | . Conditional Probability . Independence . Independent Events . Two events $A,B$ are independent if the outcome of one event has no bearing on the other. In other words, knowing the outcome of $B$ gives no information about the potential outcome of $A$: . For independent events $A,B$, $P(A,B) = P(A)P(B)$ | $P(A|B) = P(A)$ | $P(B|A) = P(B)$ | . | . Conditional Independence . Two events $A,B$ are conditionally independent given another event outcome $C$ if: $$P(A,B|C) = P(A|C)P(B|C)$$ That is, we can tease apart the probability of a given event if they share a relevant background variable. . As an example, take the problem of three genetic mutations, $i,j$ and $k$. Let $i$ and $j$ be tightly correlated in a dataset and assume the probability distributions of each event is known. At first glance, we might think we could never model the two mutations independently. We may even assume $i$ causes $j$ in some fashion, or vice versa. . However, if we discovered that $k$ was a definitely a mutation appearing in an upstream promoter region impacting both sites corresponding to $i$ and $j$, we could then show conditional independence between $i$ and $j$ given the mutation $k$. Suddenly, our assumptions change and we may be more inclined to target $k$ as an event outcome worthy of attention. . Unions, Intersects, Complements . Set theory can be useful in the realm of probabilty. At the end of the day, we use probabilistic models to try and understand real events. This is the case with both discrete and continuous outcomes. . De Morgan&#39;s Laws . De Morgan&#39;s Laws offer versatility in logical reasoning, proofs, set theory, and other areas of intrigue. One thing to note is that generally, we frequently use $AND$ and $OR$ operators in probability. Note the following: . $ lnot(A lor B)= lnot A land lnot B $ | $ lnot(A land B) = lnot A lor lnot B $ | . Joint, Marginal, and Conditional Probability . Joint Probability . $P(A,B)$ . Note, $P(A,B) = P(A)P(B|A)$ | We can tease apart the distributions by conditioning on the portion of the event space occupied by $B$ where $A$ also has a bearing. . | Note also how $P(A,B)=P(B,A) = P(B)P(A|B)$. While consistency is important, this is just a matter of our choice of labels on the events. . | . | . Marginal (Unconditional / Prior) Probability . $P(A)$ | . Conditional Probability . $P(A|B) = frac{P(A,B)}{P(B)}$ | The Probability of A given B is equal to the Probability of A and B over the (prior) Probability of B . | Note: We can easily see how Bayes&#39; Rule follows. Given that we can &quot;flip&quot; the order of the joint probability expression, what is the right side equivalent to? . $P(A|B) = frac{P(A,B)}{P(B)} to P(A|B)P(B) = P(A,B) = P(B|A)P(A)$ | $ implies P(A,B) = frac{P(B|A)P(A)}{P(B)}$, or Bayes&#39; Rule! | . | . Conditional Probability is Probability . $P(A|B)$ is a probability function like any other for a fixed $B$. Any theorem applicable to probability is relevant for conditional probability. | . Chain Rule for Probability . Note we can disentangle a joint probability by use of the &quot;chain&quot; rule, an extension of operations on two-event probabilities. | $P(A,B,C) = P(A|B,C)P(B,C) = P(A|B,C)P(B|C)P(C)$. | This is exaclty the same as calling the joint event space $B,C = D$ and writing $P(A,D) = P(A|D)P(D)$. | . Law of Total Probability . For an event $A$ and disjoint sample partitions $B_1,...,B_n$, we can always marginalize out irrelevant event spaces. . $P(A) = P(A|B_1)P(B_1) + ... + P(A|B_n)P(P_n)$ | . Bayes&#39; Rule . Combining the definitions of conditional probility $P(A|B) = frac{P(A,B)}{P(B)}$ and joint probability $P(A,B) = P(A|B)P(B) = P(B|A)P(A)$, we can describe Bayes&#39; Rule: . $$P(A|B) = frac{P(B|A)P(A)}{P(B)}$$ . For 3 events $A,B,C$, we can write $$P(A|B,C) = frac{P(A,B,C)}{P(B,C)} = frac{P(B,C|A)P(A)}{P(B,C)}$$ . We can also use the chain rule to our liking: . $$P(A|B,C) = frac{P(A,B,C)}{P(B,C)} = frac{P(A|B,C)P(B|C)P(C)}{P(B|C)P(C)} = P(A|B,C)$$ . Note that it may be useful to commute these terms depending on the circumstance. . Random Variables and their Distributions . A random variable can take on a number of values according to a mathematical function. This may be thought of as the probability of a given outcome of an experiment in a global sense. . Probability Mass Functions (PMF) and Probability Density Functions (PDF) . The Probability Mass Function is a function $f = P(X=x)$ that returns the probability associated with a discrete random variable $X$ taking on a particular value $x$. | The Probability Density Function is a function $f = P(X=x)$ returning the probability associated with a continuous random variable $X$ taking on some value $x$ | . Note that in either case, the probability returned is the probability of precisely that value. If the probability of any value up to and including $X=x$ is desired, the Cumulative Distribution Function is required, and if the probability of any value higher than $X=x$ is desired, then the survival function is the way to go. . Cumulative Distribution Functions (CDF) . A function yielding $P(X leq x)$. | . Survival Functions . A function yielding $P(X&gt;x)$. | . Independence of Random Variables . Two variables are considered independent if knowledge of one outcome does not give information about the other. That is, $P(X=x,Y=y) = P(X=x)P(Y=y)$, where we don&#39;t need conditional independence or marginalization. | . Expected Values and Indicators . Expected Values and Linearity . Expected Values . Otherwise known as mean, expectation, or first moment, the Expected Value is the weighted sum of all possible outcomes of a random variable. . $E(X) = sum_i x_i P(X=x_i) $ | . Linearity . Linearity implies that expected values of random variables are linear with respect to constants. . $E(aX + bY + c) = aE(X) + bE(Y) + c$ | . $ text{Distribution} implies text{Mean}$ . For a given set of random variables $X,Y$, if the variables have the same distribution, this implies they have the same mean. Note that the converse is not implied. . $E(g(X)) = E(g(Y))$, where $g$ is a distribution function. | . Conditional Expected Value . Expected values can be defined conditionally, as conditional distributions are probability functions. That is, $E(X|A) = sum_x x P(X=x|A)$ for some event A. . Indicator Random Variables . Indicator RVs . This random variable simply takes on a value of 1 if some event $A$ occurs, and 0 otherwise. . $I_A = 1$ if $A$, $0$ otherwise. | . Distribution of an Indicator RV . Note that $I_A sim Bernoulli(p=P(A)) $ . Fundamental Bridge of an Indicator RV . The expectation of the indicator for event $A$ is the probability of the event $A$ . $E(I_A) = P(A)$ | . Variance and Standard Deviation (w.r.t. Expectation) . With respect to Expectation, we can write Variance and Standard Deviation of some random variable $X$ similarly to the canonical forms. . $Var(X) = E(X-E(X))^2 = E(X^2) - (E(X))^2$ | $StdDev(X) = sqrt{(Var(X))}$ | . Properties of Continuous Random Variables, Law of the Unconscious Statistician (LOTUS), and the Universaility of Uniform (UoU) . Continuous Random Variables . Probability of a CRV in a Given Interval . The probability of a CRV taking on values in some interval can be found by taking the difference of cumulative distribution function at the bounds of the interval. . $P(a leq X leq b) = P(X leq b) - P(X leq a) = F_X(b) = F_X(a)$ | . For instance, in the case of a normally distributed random variable $X sim mathcal{N}( mu, sigma^2)$, $P(a leq X leq b) = phi( frac{b- mu}{ sigma}) - phi( frac{a- mu}{ sigma})$ . The Probability Density Function of a CRV . Note that we can take derivates or integrate between Probability Density Functions $f(x)$ and Cumulative Distribution Functions $F_X(x)$. . That is, $F&#39;(x)=f(x)$, and $F(x) = int_{- infty}^x f(t) dt$. Therefore, to get the probability of a CRV being on a certain interval, instead of integrating over the entire range of values and subtracting at bounds (as was done above), we can simply take the integral between the bounds: $F(b)-F(a) = int_a^bf(x)dx$ . Expected Values of Functions of CRVs versus DRVs . Expected values of a random variable $X$ follows intuitively. . Discrete: . For a discrete random variable $X$, we sum over the product of the probability mass function at a given value and the value the random variable may take on. | $E(X) = sum_x xP(X=x)$ | . Continuous: . For a continuous random variable, we integrate over the product of the probability density function and the continuous variable. | $E(X) = int_{- infty}^{ infty} x f(x) dx$ | . Law of the Unconscious Statistician (LOTUS) . So named for its purported tendency to be used as a definition rather than a rigorous mathematical statement, the LOTUS shows that the expected value of a function of a random variable $f(X)$ similarly to the expected value of the random variable $X$ itself. . That is, in the discrete case, $E(g(X)) = sum_x g(x) P(X=x)$, and for the continous case, $E(g(x)) = int_{- infty}^{ infty}g(x)f(x)dx$, where $P(X=x)$ is the probability mass function of $X$ at some discrete real value $x$ and $f(x)$ is the probability density function. . Note that the function of a random variable is itself a random variable, implying that it is sufficient to know the PMF or PDF of $X$ in order to find $E(g(X))$. . $g(RV_i) = RV_j$: The function of a random variable is itself a random variable. . I.e., one need only know the PMF/PDF of $X$ to find the PMF/PDF of $g(x)$. | . Universality of Uniform / Probability Integral Transform . Substitution of any $X_{cts}$ into its cumulative distribution function $F_X(x) = P(X leq x)$ yields $U(0,1)$. | Let $Y=F_X(X)$. Then, $F_Y(y) = P(Y leq y) = P(F_X(X) leq y) = P(X leq F^{-1}(y)) = F_X(F_X^{-1}(y)) = y$ for $Y sim U(0,1)$ and $X$ is some continous random variable with CDF $F_X$. . I.e. $F_X(X_{cts}) = int_{- infty}^{x}f(t)dt = int_{- infty}^{x}P(X leq t)dt = X sim U(0,1)$. | . | . Moments and Moment-Generating Functions . Moments . Moments describe the topology of a distribution. For instance, given RV $X$ with mean $ mu$ and standard deviation $ sigma$, for $ X_{ text{standardized}} = z = frac{(X- mu)}{ sigma}$. Then, the $k$th standard moment of $X$ is given by $$m_k=E(Z^k)$$ . Some important &quot;moments&quot; follow from this expression: . mean = $E(X) = m_1 = mu_1$ | variance = $Var(X) = mu_2 - mu_1^2$ | skewness = $Skew(X) = m_3$ = measure of (lack of) symmetry | kurtosis = $Kurt(X) = m_4 - 3$ = measure of tailedness | . Note other equivalent expressions also follow. . Moment-Generating Functions . The $k$th derivative of the moment generating function, when evaluated at $0$, is the $k$th moment of a random variable $X$. That is, $$ mu_k = E(X^k) = M_X^{(k)}(0)$$. Thus, the moment-generating function generates the moment. . For any random variable $X$, $M_X(t) = E(e^{tX})$ is the MGF for $X$ if it it exists for some open interval containing $0$. . We can show the first point by Taylor expansion. Note that the Taylor series for $f(x)$ about $x=t$ is $$f(x) = sum_{n=0}^{ infty} frac{f^{(n)}(t)}{n!}(x-a)^{n}$$ . With regard to the expansion of $e^{tX}$, we get that $M_X(t) = E(e^{tX}) = sum_{k=0}^ infty frac{E(X^k)t^k}{k!} = sum_{k=0}^ infty frac{ mu_k t^k}{k!} $ . There is more to say about moment-generating functions, but I will leave it for now. Like characteristic functions, they are an alternative specification for probability distribution to density functions or cumulative distribution functions. However, we almost always use PMFs/PDFs/CDFs in practice. . Joint Probability Density Functions (PDFs) and Cumulative Distribution Functions (CDFs) . Joint Cumulative Distributions . $F(x,y) = P(X leq x, Y leq y)$, as would be expected. For the discrete case, $p_{X,Y}(x,y) = P(X=x,Y=y)$. In the continuous case, $f_{X,Y}(x,y) = frac{ partial ^2}{ partial x partial y} F_{X,Y}(x,y)$ . Note that the joint PMF/PDF must be non-negative and sum/integrate to 1, respectively. . Marginal Distributions . Discrete Case: Marginal PMF from Joint PMF $P(X=x) = sum_y P(X=x,Y=y)$ | . | Continous Case: Marginal PDF from Joint PDF $f_X(x) = int_{- infty}^{ infty} f_{X,Y}(x,y)dy$ | . | . Independence of Random Variables . $X,Y$ are independent ($X perp ! ! ! perp Y$) iff . the joint CDF is the product of the marginal CDFs | the joint PMF/PDF is the product of the marginal PMFs/PDFs | the conditional distribution of $Y$ given $X$ is the marginal distribution of Y | . Further Topics . We will now take a look at topics that are more practical and/or obscure, such as relevant distributions and Markov Models. . Covariance and Transformations . Covariance and Correlation . Covariance is analagous to variance but with more than one random variable. That is, it explains the pairwise difference between the expectation of the product of the variables and the product of the expectations of each random variable: . $Cov(X,Y) = E((X-E(X))(Y-E(Y))) = E(XY) - E(X)E(Y)$ | Note that $Cov(X,X) = E(X^2) - E(X)^2 = Var(X)$ | . Correlation is simply the standardized covariance on $[-1,1]$: . $Corr(X,Y) = frac{Cov(X,Y)}{ sqrt{Var(X)Var(Y)}}$. Note that $Corr(X,X)=1$. | . Covariance and Independence . If two variables are independent, they are not correlated. However, uncorrelated variables could still be dependent. In other words, two correlated variables are dependent, but dependent variables need not be correlated. . To show this, consider $X sim mathcal{N}(0,1), Y = X^2). These two variables could be uncorrelated but still obviously are dependent: . import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt from matplotlib import colors plt.style.use(&#39;dark_background&#39;) mu, sigma = 0, 0.1 # mean and standard deviation s = np.random.normal(mu, sigma, 1000) count, bins, ignored = plt.hist(s, 50, density=True,alpha=.40) plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ), linewidth=2, color=&#39;r&#39;) plt.title(&#39;X ~ N(0,1)&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-22T15:11:53.278250 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ count, bins, ignored = plt.hist(s**2, 50, density=True,alpha=.40) plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ), linewidth=2, color=&#39;r&#39;) plt.title(&#39;Y=X^2&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-22T15:11:55.985804 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ plt.scatter(s,s**2) plt.title(&#39;X vs. Y=X^2&#39;) . Text(0.5, 1.0, &#39;X vs. Y=X^2&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-22T15:13:18.267450 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ print(f&#39;The correlation coefficient is {round(float(np.correlate(s,s**2)),4)} .&#39;) . The correlation coefficient is -0.1925 . . Covariance and Variance . Properties of Covariance . Correlation: Location and Space-Invariant . Transformations . Single Variable . | Two Variables . | . Convolutions . Convolution Integral . Relevance to &quot;Convolutional&quot; Neural Networks . Poisson Processes . Law of Large Numbers . Central Limit Theorem . Markov Chains . Markov Property . States . Transition Matrix . Chain Properties . Stationary DIstributions . Some Continuous Distribitions . Normal . Exponential . Gamma . Beta . Chi-Square . Some Discrete Distribitions . Sampling: Varying Number of Trials and Replacement . Bernoulli . Binomial . Geometric . First-Success . Negative Binomial . Hypergeometric . Poisson . Some Multivariate Distributions . Multinomial . Multivariate Uniform . Multivariate Normal . A Note on Mixture Models . EM and Mixture Models . In Case You Missed It: Special Cases of Distributions . - . Important Inequalities . Cauchy-Shwarz . Markov . Chebyshev . Jensen . Background: Formulas . Geometric Series . Exponential Function . Gamma and Beta Integrals . Euler&#39;s Approximation for a Harmonic Sum . Stirling&#39;s Approximation for Factorials .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/05/probability.html",
            "relUrl": "/jupyter/2021/01/05/probability.html",
            "date": " ‚Ä¢ Jan 5, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Magic in Numpy",
            "content": "Numpy Basics . We will now go over some basic approaches on a seemingly simple matrix for illustrative purposes. Hopefully some of the efficient and useful properties of Numpy become apparent. . A favorite work by a favorite artist, D√ºrer&#39;s Melencolia I (1514) includes sophisticated use of mathematical allegory, particularly in the top-right corner. It turns out D√ºrer actually makes many interesting points through his art, something you wouldn&#39;t expect from his messiah complex. . . . The matrix is thus: . import numpy as np X = np.array([[16, 3, 2, 13], [5, 10, 11, 8], [9, 6, 7, 12], [4, 15, 14, 1]]) print(X) . [[16 3 2 13] [ 5 10 11 8] [ 9 6 7 12] [ 4 15 14 1]] . type(X) . numpy.ndarray . Magic Squares . This matrix is purported to be a magic square. We must fit the following constraints: . 1) Magic 2) Square . Simple enough. Starting with the second condition, Numpy provides a number of methods. Though magic cubes and tesseracts are possible, we can begin with a square. Here&#39;s a simple function to detect if an array is square. . def is_square(M: np.ndarray) -&gt; bool: &#39;&#39;&#39; Arguments: M, a 2-d matrix Returns: a boolean, True if square &#39;&#39;&#39; assert M.ndim == 2 return True if M.shape[0] == M.shape[1] else False is_square(X) . True . Vectorized Summation: Magic . Now, the more involved condition. If a square is &quot;magic&quot;, the array exhibits the following properties1: . i) Each of the $n$ elements are of the set of distinct positive integers $[1,2,3,...,n^2]$, such that $n$ is the &quot;order&quot; of the square. D√ºrer thus presents a $4^{th}$ order magic square. . ii) The sum of the $n$ numbers about any horizontal, vertical or main diagnonal is the same number ‚Äì the &quot;magic&quot; constant. It is known that such magic constants can be given by $ mathcal{M}(X_n) = frac{1}{n} sum_{k=1}^{n^2}k = frac{1}{2}n(n^2+1)$ . . Aside: iii) The complement to a magic square is derived from subtracting every number in a given magic square by $n^2 + 1$. . Back to D√ºrer. We can check each condition as follows. . . there are others, see https://faculty.etsu.edu/stephen/matrix-magic-squares.pdf)&#8617; . | def is_magic(M, verbose = True)-&gt;bool: #By constraints i) &amp; ii) assert M.shape[0] == M.shape[1], &#39;Not a square.&#39; n = M.shape[0] assert np.array_equal(np.sort(M.flatten()), np.arange(n**2) + 1), &#39;Expected elements from [1,2,...,n^2]&#39; column_sums = np.sum(M,axis=0) #Note that summing across axis 0 actually returns column-wise sums, and vice-versa. row_sums = np.sum(M, axis=1) diagonal_sums = np.array([np.trace(M),np.trace(np.fliplr(M))]).astype(int) magic_num_sum = np.unique(np.concatenate( (column_sums,row_sums,diagonal_sums) )) if len(magic_num_sum) == 1: if verbose: print(f&#39;Magic number is {magic_num_sum} with order {n}.&#39;) return True . np.fliplr(X).diagonal().sum() == np.flipud(X).diagonal().sum() . True . np.trace(X) == np.diagonal(X).sum() . True . is_magic(X) . Magic number is [34] with order 4. . True . X . array([[16, 3, 2, 13], [ 5, 10, 11, 8], [ 9, 6, 7, 12], [ 4, 15, 14, 1]]) . Fast Indexing: Gnomon Magic . D√ºrer&#39;s square is actually a Gnomonic Magic Square ‚Äì that is, each non-overlapping root subsquare bordering the four sides of the square ($2 times 2$ subsquare), as well as the central subsquare, sums to the magic constant of the overarching square. . The Gnomon is the portion of the sundial casting a shadow. In a way we also cast a magic projection on subarrays of our main magic square. . We can verify this easily ‚Äì in Numpy, arrays can be efficiently split with minimal logic, rather than looping over each element and hard-indexing. . a,b,c,d = [quadrant for sub_x in np.split(X,2, axis = 0) for quadrant in np.split(sub_x,2, axis = 1)] n = X.shape[0] n_subsquare = np.sqrt(n).astype(int) start = n//2 - (n_subsquare // 2) end = n//2 + (n_subsquare // 2) e = X[start:end,start:end] sections = [a, b, c, d, e] sections . [array([[16, 3], [ 5, 10]]), array([[ 2, 13], [11, 8]]), array([[ 9, 6], [ 4, 15]]), array([[ 7, 12], [14, 1]]), array([[10, 11], [ 6, 7]])] . print(set([sum(s.flatten()) for s in sections])) . {34} . All quadrants sum to the magic number of 34. As such, we have verified the deliberate style of D√ºrer. . Linear Algebra . We will now move onto some essential linear algebra operations on the magic square. . X . array([[16, 3, 2, 13], [ 5, 10, 11, 8], [ 9, 6, 7, 12], [ 4, 15, 14, 1]]) . Rank . The rank of a matrix is the number of its linearly independent columns. That is, the dimensionality of the vector space spanned by a matrix&#39;s columns (or rows) is given by its rank, such that the span is the smallest linear subspace containing a set of vectors describing the matrix. . We can obtain the span of all linear combinations of some vectors $ vec{u}, vec{v}$ by computing $s vec{u} + t vec{v}$ for some scalar constants $s,t$. The dimensionality of the span of the row or column vectors of a matrix thus yields the row or column rank. . We will proceed using Numpy, which proceeds using singular value decompositon (SVD): . rank = np.linalg.matrix_rank(X) rank . 3 . Thus we have a rank-deficient matrix, since $3 &lt; 4$. 4 is the column-dimensionality of the Magic Square matrix but the columns only span 3 dimensions. Note that $rank(M) leq min (m,n)$ for an $m times n$ matrix $M$. . Note how Numpy uses the property that the rank is equal to the number of nonzero singular values as follows: . u,s,vh = np.linalg.svd(X) s . array([3.40000000e+01, 1.78885438e+01, 4.47213595e+00, 6.25921042e-16]) . We have 4 nonzero singular values, but the final value is extremely small. Numpy therefore considers this zero as the default tolerance is computed as M.max()*max(M.shape)*eps. . eps = np.finfo(float).eps tol = X.max()*max(X.shape)*eps tol . 1.4210854715202004e-14 . rank == len(s[s&gt;tol]) . True . rank . 3 . Determinant . The determinant is a useful encoding of the linear transformation described by a particular $n times n$ matrix. In geometric terms, it is analagous to the volume scaling factor of the linear transformation described by the matrix. . In other words, this is the volume of the parallelepiped (a rhomboid prism; a cube is to a square as a parallelepiped is to a parallelogram) spanned by the vectors (row or column) of a matrix. The sign of the determinant of a matrix denotes whether or not the orientation of a vector space is preserved by the transformation described by the matrix. . Two simple examples, then D√ºrer&#39;s: . A = np.array([[0,-1],[1,0]]) B = np.array([[-2,0],[0,2]]) . $A$ describes a 90-degree counterclockwise (‚Ü™Ô∏è) rotation. . $B$ describes a scaling by a factor of $2$ as well as a reflection about the $y$ axis. . print(np.linalg.det(A), np.linalg.det(B), sep=&#39; n&#39;) . 1.0 -4.0 . A simple rotation does not change &quot;volume&quot; nor orientation. A scaling on $x,y$ and a reflection about the $y$ axis is encoded in the determinant, however: the &quot;volume&quot; is changed in total by a factor of $4$ and the sign is negative, indicating a change in the orientation of previous vector space. . These are simple enough to compute by hand, but even a $4 times 4$ dimensional matrix as provided by D√ºrer is more onerous. Thankfully, Numpy works well: . X = np.array([[16, 3, 2, 13], [5, 10, 11, 8], [9, 6, 7, 12], [4, 15, 14, 1]]) np.linalg.det(X) . 1.449507180950607e-12 . An interesting observation: D√ºrer does not provide a pandiagonal magic square, as the determinant of this order-4 magic square is near to, but $not$, zero. In other words, if the broken diagonals (for instance, $16,11,12,15$, or $3,8,7,4$) summed to the magic number, the determinant would be zero [^1]. . Eigenvectors and Eigenvalues . We solve the characteristic equation of a matrix $M$ to find eigenvalues $ vec{ lambda}$. That is, we solve $|M - lambda I| = 0$ where $I$ is the identity matrix ($I_{ij} = 1 s.t i=j, 0 s.t. i not = j$) of identical dimensionality to $M$. . In the case of D√ºrer&#39;s magic square, we simply subtract a value $ lambda$ from each element on the main diagonal and set the resulting matrix&#39;s determinant equal to zero. . We can quickly avoid rote work using Numpy: . eigenvals, eigenvects = np.linalg.eig(X) eigenvects . array([[ 5.00000000e-01, 8.16496581e-01, 2.23606798e-01, -4.08248290e-01], [ 5.00000000e-01, -4.08248290e-01, -6.70820393e-01, 0.00000000e+00], [ 5.00000000e-01, -1.76752662e-16, 6.70820393e-01, -4.08248290e-01], [ 5.00000000e-01, -4.08248290e-01, -2.23606798e-01, 8.16496581e-01]]) . eigenvals . array([ 3.40000000e+01, 8.00000000e+00, 4.84818517e-17, -8.00000000e+00]) . Note the interesting property of magic squares: the principal (largest) eigenvalue of a magic square composed of positive elements is its magic constant! Of further note, but not applicable here, is the observation that if a magic square has some negative elements, then its magic constant is one of its eigenvalues.[^1] . To show the first point, consider that $[1,1,...,1]^T$ is an eigenvector of a matrix $M$ if every row sums to the same value $k$. This can be shown by computing $|M- lambda I| = 0$ for a matrix $M$ where all rows sum to the same constant $k$. Substituting values and simplifying, $k$ is an eigenvalue. The same holds for columns that sum to the same constant. . Now, we want to show that the entries in the vector $Mv$ are equal to $kv$, where $k$ is both the magic number and an eigenvalue, and $v$ is an eigenvector of $M$. Recall that the sum of all elements in an $n times n$ magic square $M^*$ is, by construction, equal to $ frac{n(n^2+1)}{2}$. Thus, since a magic square $M^*$ indeed does have each row sum to $k$, we have that $Av = kv$ for $[1,1,...,1]^T$. . This gives us another way to find the magic number of a magic square. . def get_magic_number(M): if is_magic(M, verbose= False) and is_square(M): return np.linalg.eig(X)[0].round(1).astype(int)[0] get_magic_number(X) . 34 . Conclusion . This concludes discussion (for now) of magic squares, essential Numpy, and some linear-algebraic approaches to simple matrices. From here, I hope to move to much more complex topics involving far more abstract data types and approaches to manipulation. Nonetheless, foundations will always be important and most likely present under the hood. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/04/linear-algebra.html",
            "relUrl": "/jupyter/2021/01/04/linear-algebra.html",
            "date": " ‚Ä¢ Jan 4, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "(simple) Autoencoders ‚â° PCA",
            "content": "Background . Shocker: The simplest autoencoder is actually the same as PCA. We will also see how VAEs perform expectation maximization. Autoencoders learn a meaningful representation of a signal. Using an encoder/decoder pair, autoencoders work by reconstructing a latent encoded representation of an original signal. By minimizing the loss between the original signal and the decoded latent representation, an encoder network can be trained to parse an instance of a dataset for its most meaningful features. . Sound familiar? That is precisely the goal of computing the principal components analysis (PCA) of a matrix. It turns out that autoencoders, by construction are exactly PCA. . The Simplest Autoencoder . Let a neural network be defined with a single hidden unit $W^T sigma(f(W vec{X}))$, with a linear activation function $ sigma$ (for this example). Let the weights on the encoder layer be denoted $W$. . We thus define a decoder to use weights $W^T$, and final outputs of the network should converge to a reconstruction of the original features $ vec{X}$ once properly trained. That is, our network should produce $ hat{ vec{X}}$ from $ vec{X} to vec{y} = WX to z = sigma(f(y))$ and finally $ hat{ vec{X}} = W^Tz$. . If we train by minimizing the $L_2$ divergence between $( vec{X}, hat{ vec{X}})$, we have an autoencoder, but we also learn the principle components of $ vec{X}$: . $$ hat{x} = w^Twx; div( hat{x},x) = |x- hat{x} |^2 = |x - w^TWx |^2 $$ $$ to via backprop to hat{w} = arg min_w E left[ | x-w^TWx |^2 right]$$ . This is equivalent to discovering a basis that spans the principle components of the input, as we discover the directions of maximum energy, equal to the variance of $X$ if $X$ is a zero-mean random variable. In other words, we find a linear mapping of the original features to the principle axis of the data. . Finally, if the learned decoder weight is not the same weight as the input weight (i.e., $U^T not = W^T$), we still learn a hidden representation $z$ that lies along the major axis of the input $X$. The minimum error direction here is by definition the pinciple eigen vector of $X$. . We could then find a useful component of $X$ (described perhaps by our training process or assumptions)by then projecting the eigen vector onto $X$. Again, if $U^T = W^T$, we arrive at the principle component(s) of $X$. . Of course, this is a roundabout method of obtaining principle components, but I hope it shows the rigorous grounding and versatility of perceptrons in learning representations of data. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/03/autoencoders.html",
            "relUrl": "/jupyter/2021/01/03/autoencoders.html",
            "date": " ‚Ä¢ Jan 3, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Tensors via Bioimaging",
            "content": "Background . 1.3 billion humans are able to use Microsoft Excel. Microsoft Excel is the modern clay tablet, an intuitive but ultimately limited instrument for the computational professional. After all, why should we be limited to two dimensions? Why should we pay for the privelege? Often, data is best represented as $n$-dimensional. . For instance, let&#39;s say you&#39;d like become a billionaire without too much effort. One way would be to totally automate clinical bioimage analysis at human-level fidelity using machine learning. . . A Diabetic retinopathy slide (https://www.kaggle.com/c/diabetic-retinopathy-detection). The condition is estimated to effect over 93 million people. . Whereas a highly skilled human could potentially spot abnormalities in the above retinopathy slide, a machine can do it better and much faster. Partially, this is because a machine views the below image somewhat na√Øvely as 150,528 dots, as we have a square RGB image with with 224 pixels per dimension. . Viewing the image not as an image but as a tensor (vector-valued matrices), or an $n$-dimensional generalization of a matrix, we can then move onto analyzing the image: segmentation, feature learning, classification (if labels are detected), and other interesting tasks that a human may or may not be able to do. . Numpy . We use Numpy for efficient vectorized tensor manipulation with the convenient abstraction of the Python programming language. Numpy fluency will carry a computational professional very far, and it will only begin to show limitations when deep learning and very large datasets are involved (though the syntax of major deep learning packages are very close to Numpy). . We can easily load the above png &quot;image&quot; into a numpy array using a number of packages. imageio is used below. . import imageio import numpy as np my_image = imageio.imread(&#39;10009_right.png&#39;) my_image . Array([[[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], ..., [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]]], dtype=uint8) . Above we see the image loaded into memory as an integer-valued 3-dimensional matrix. Basic Numpy / Python fluency is assumed (slicing, etc.). . my_image.shape #note memory persistence in Jupyter. . (224, 224, 3) . my_image.size . 150528 . my_image.ndim . 3 . The $224 times 224 times 3$, $150,528$-element, $3$-dimensional array can be now be subject to a multitude of useful manipulations. . For instance, we can sparsify the matrix using scipy.sparse for 2-d matrices and sparse (!pip install sparse) for $n$-dimensional arrays (tensors). This may be useful for quick compression or storage of many such images: note the large number of zero values corresponding to &quot;black&quot; portions of the image. . import sparse sparse.COO(my_image) . Formatcoo | . Data Typeuint8 | . Shape(224, 224, 3) | . nnz120885 | . Density0.8030731823979592 | . Read-onlyTrue | . Size2.9M | . Storage ratio20.1 | . The sparse matrix in general is less memory intensive. . Another example of manipulation: we can quickly invert and flip the image with numerical rigor. We will use matplotlib to display the numpy array. . np.invert is np.bitwise_not . True . import matplotlib.pyplot as plt plt.imshow(my_image) plt.axis(&#39;off&#39;) plt.title(&#39;All Axes&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; plt.imshow(np.invert(my_image)) plt.axis(&#39;off&#39;) plt.title(&#39;Inverted (Bitwise NOT; twos-complement)&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Said another way... . 255 - my_image[100,100,:] == np.invert(my_image)[100,100,:] #we are in 256-bit color. . Array([ True, True, True]) . plt.imshow(np.fliplr(my_image)) plt.axis(&#39;off&#39;) plt.title(&#39;Flipped (LR)&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; This leads to some intuitive operations. For instance, if we were looking to feature-engineer such image data for use in a statistical learning classifier, we may hypothesize the location of the fovea (bright central spot) as a useful aspect of the retinopathy image. . I&#39;m betting we can segment these features with a single line of numpy. For this and subsequent examples, let&#39;s take a sample of the first sheet (zeroth index) of the image for simplicity such that we have a 2-d array (pretend we read-in a greyscale image). . x = my_image[:,:,0] #take all elements of the first sheet/leaf of the array. plt.imshow(x) plt.axis(&#39;off&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Now for the single line. We will find the bright portion by the assumption that we can find it by taking $region = frac{ sum_i x_i}{N} * 2 * sigma( vec{x})$, or all pixels with intensity greater than equal to two standard deviations above the mean. . x = my_image[:,:,0] #take all elements of the first sheet/leaf of the array. mask = (x &gt;= x.mean() + 2*x.std()) . plt.imshow(mask) plt.axis(&#39;off&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; We can then get the coordinates in the array simply: . coords = np.argwhere(x == mask) coords . array([[ 0, 0], [ 0, 1], [ 0, 2], ..., [223, 221], [223, 222], [223, 223]]) . print(coords.size) . 20656 . If we knew a priori that a particular type of retinopathy was characterized by abnormal foveal locations, and we had a sufficient train/test/validation dataset, we could reduce the size of our dataset significantly with such an engineered feature. . print(f&#39;If we only require foveal coordinates, our dataset may be reduced by {round(100-(coords.size/my_image.size)*100,2)}% !&#39;) . If we only require foveal coordinates, our dataset may be reduced by 86.28% ! . In Closing... . I hope this example was helpful in showing how we can quickly prepare complex data types for computational purposes. I will return to abstract bioimaging processing in future posts. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/tensors.html",
            "relUrl": "/jupyter/2020/12/31/tensors.html",
            "date": " ‚Ä¢ Dec 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Originally from a small town in northern California, I completed my B.A. in Biological Sciences with a minor in Mathematical Finance at the University of Southern California. . I then spent about a year in consulting and briefly worked making wine. I also devoted a considerable amount of time and effort on the law school admissions test (LSAT) before deciding the profession was not for me. Instead, I chose to return toward technical topics and decided to pursue the master‚Äôs program in computational biology at Carnegie Mellon University. . Now, I‚Äôll be working as a Data Scientist at Encoded Therapeutics beginning June, 2021. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://simonlevine.github.io/simonsays/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://simonlevine.github.io/simonsays/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}