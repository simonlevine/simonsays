{
  
    
        "post0": {
            "title": "One GiGANtic Leap...",
            "content": "Introduction . Generative Adversarial Networks are more than a novel deep learning architecture. They are powerful game-theoretic constructs with potential to change much of our understanding of what is &quot;real&quot;. If what is real is useful insofar as what we perceive is useful, then we may be in touble. The advent of GANs in the early part of this century has distrupted the way reality is made. Deep networks are becoming immensely powerful at learning class-posterior distributions $P(Y|X=x)$. However, generative models are doing something much trickier: learning $P(X,Y)$. GANs use a discriminator of $P(Y|X)$ to accomplish this. Thus, adversarial nets are a balancing act to implement and train, but provide unparalleled results. On the one hand, this could seriousy destroy media (Microsoft&#39;s MSN recently replaced human authors with generative machines), social stability, and our politics (think DeepFakes). However, there is reason to be hopeful. . One realm I&#39;m particularly excited to see GANs making an entrance is in the biosciences. GANs (and really any high-fidelity generative model) may hold the promise to generate sythetic datasets to build better supervised learners on features such as genomes, proteomes, and bioimages. This could help bring machine learning closer to solving few-shot and zero-shot problems in medicine. . Another example is for the purposes of simulation and modeling. Tons of assumptions must be made during modeling (for, say, a pandemic) that may not prove realistic. On the other hand, epidemiological datasets may be limited in scope and depth, such that sparse or missing features from a data collection stage could prove vital if not for their low frequencies. Simulating patient risk pools for disease-fighting or drug design via a GAN is one potential method to overcome this, as we can sample from $P(Y,X=x_{minority})$ via a model trained to discriminate perfectly on $P(Y|X)$. . Bioengineering is another intriguing space, whereby in-silico experimentation meets the lab bench through simulating data like yeast genomes. There&#39;s even the potential to build a life factory through generating realistic but novel oligomers and augmenting existing genomes (possible given how splicing is currently solved). . In any event, we should know just how to 1) Train a GAN, and 2) Use it to augment an existing workflow. In this post, I will begin by using the COVID-19 cell atlas as a source of features, and $X$ will be the single-cell read counts. That is, single-cell profiling of COVID-19 patients will provide the raw data to build a generative adversarial network. We will begin by training a discriminator $f$ to learn a classification $f(X) = y$, or $P(Y|X)$. Then, the generative model will begin to simulate $X,y$ pairs, and be trained to accurately generate $P(X,Y)$. . This can be and is used to generate realistic data, or samples on $P(X|Y=y)$ (such as realistic read-counts of single-cell data given patient phenotypes). We can use this trained generative model to shore up minority classes to improve classifier performance (another $f(X) = y$, though on held-out test data), or simply to provide more data distributed in the same fashion as $X,y$. I will simply generate realistic fake data for now. . Data . I will use scanpy to load in a toy PBMC (peripheral blood mononuclear cell) dataset into a PyTorch Dataset object. Features will be normalized single-cell read counts, a correlate of gene expression. Labels will be cell types identified from Louvain clusters. . import sklearn.model_selection from sklearn.preprocessing import LabelEncoder import scanpy as sc data = sc.datasets.pbmc3k_processed() train, val = sklearn.model_selection.train_test_split(data,test_size=.25) le = LabelEncoder().fit(train.obs.louvain) train.y=le.transform(train.obs.louvain) val.y=le.transform(val.obs.louvain) . from torch.utils.data import Dataset,DataLoader import torch from matplotlib import pyplot as plt import matplotlib as mpl class ToySingleCellDataset(Dataset): def __init__(self,anndata): self.anndata = anndata self.X = self.anndata.X self.y = self.anndata.y self.setup_run = False def __len__(self): return len(self.anndata) def __getitem__(self,idx): assert self.setup_run==True, &#39;Run .setup()&#39; return self.X[idx], self.y[idx] def setup(self): self.X = torch.FloatTensor(self.X) self.y = torch.LongTensor(self.y) print(f&#39;Data loaded. X is of shape {self.X.shape}, with y of shape {self.y.shape}&#39;) self.setup_run=True def plot_umap(self): mpl.rcParams[&#39;figure.dpi&#39;]= 150 plt.style.use(&#39;dark_background&#39;) plt.scatter(self.anndata.obsm[&#39;X_umap&#39;][:,0],self.anndata.obsm[&#39;X_umap&#39;][:,1], c=self.y, alpha=0.20, cmap = &#39;Pastel1&#39;,s=20) plt.xlabel(&#39;Compenent 1&#39;) plt.ylabel(&#39;Compenent 2&#39;) plt.title(&#39;UMAP, Colored by Cell Type&#39;) . train_dataset, val_dataset = ToySingleCellDataset(train), ToySingleCellDataset(val) train_dataset.plot_umap() . Model Architecture . Probabilistic Intuition . A symmetric view: . i) A generator (in the data-faker view) samples from $P(X|Y=y)$, the distribution of the feature over each class or label. . Note that $P(X,Y) = P(X|Y)P(Y) implies P(X|Y) = P(X,Y)/P(Y)$ | . ii) A discriminator samples on $P(Y|X=x)$, the distribution of the labels over each feature. . Note that $P(X,Y) = P(Y|X)P(X) implies P(Y|X) = P(X,Y)/P(X)$ | . This is because we are provided many instances of $x,y$, where the label space is finite. . Assuming the labels are discrete (i.e., in the case of a classifier and not a regressor), we can always sum to marginalize out the labels from the joint distribution to produce the feature prior $P(x) = sum_y P(X,Y=y)$. . Likewise, for a continuous feature $X$, we can always integrate to marginalize out the features and produce the class prior $P(y) = int_x P(Y,X=x)$. . Then, either the &quot;generator&quot; $P(X|Y)$ or the &quot;discriminator&quot; $P(Y|X)$ can be derived by the definition of conditional probability and Bayes&#39; rule (see the above bullets). . Implementation . We integrate this construction by using a GAN. Classically, this implies that we train a discriminator $f(X) = y$ on $ vec{x}, vec{y}$ and subsequently use this . The Generative Adversarial Network in PyTorch . import os from argparse import ArgumentParser from collections import OrderedDict import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms from torch.utils.data import DataLoader, random_split from torchvision.datasets import MNIST import pytorch_lightning as pl . import sklearn.model_selection from sklearn.preprocessing import LabelEncoder import scanpy as sc class SingleCellDataModule(pl.LightningDataModule): def __init__(self, data_dir: str = &#39;./data&#39;, batch_size: int = 64, num_workers: int = 8): super().__init__() self.data_dir = data_dir self.batch_size = batch_size self.num_workers = num_workers # self.dims is returned when you call dm.size() # Setting default dims here because we know them. # Could optionally be assigned dynamically in dm.setup() # self.dims = (1, 28, 28) # self.num_classes = 10 def setup(self, stage=None): # Assign train/val datasets for use in dataloaders. No test stage in our case. data = sc.datasets.pbmc3k_processed() train, val = sklearn.model_selection.train_test_split(data,test_size=.25) le = LabelEncoder().fit(train.obs.louvain) train.y=le.transform(train.obs.louvain) val.y=le.transform(val.obs.louvain) self.train_dataset, self.val_dataset = ToySingleCellDataset(train), ToySingleCellDataset(val) self.train_dataset.setup() self.val_dataset.setup() self.num_classes = len(self.train_dataset.y.unique()) def train_dataloader(self): return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers) def val_dataloader(self): return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers) . class Generator(nn.Module): def __init__(self, latent_dim, sc_instance_shape): super().__init__() self.sc_instance_shape = sc_instance_shape def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(latent_dim, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(sc_instance_shape))), nn.Tanh() ) def forward(self, z): sc_instance = self.model(z) sc_instance = sc_instance.view(sc_instance.size(0), *self.sc_instance_shape) return sc_instance . class Discriminator(nn.Module): def __init__(self, sc_instance_shape): super().__init__() self.model = nn.Sequential( nn.Linear(int(np.prod(sc_instance_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, 1), nn.Sigmoid(), ) def forward(self, sc_instance): sc_instance_flat = sc_instance.view(sc_instance.size(0), -1) validity = self.model(sc_instance_flat) return validity . class SingleCellGAN(pl.LightningModule): def __init__( self, data_shape, latent_dim: int = 100, lr: float = 0.0002, b1: float = 0.5, b2: float = 0.999, batch_size: int = 64, **kwargs ): super().__init__() self.save_hyperparameters() # networks # data_shape = (1838,) self.generator = Generator(latent_dim=self.hparams.latent_dim, sc_instance_shape=data_shape) self.discriminator = Discriminator(sc_instance_shape=data_shape) self.validation_z = torch.randn(8, self.hparams.latent_dim) self.example_input_array = torch.zeros(2, self.hparams.latent_dim) def forward(self, z): return self.generator(z) def adversarial_loss(self, y_hat, y): return F.binary_cross_entropy(y_hat, y) def training_step(self, batch, batch_idx, optimizer_idx): sc_instances, _ = batch # sample noise z = torch.randn(sc_instances.shape[0], self.hparams.latent_dim) z = z.type_as(sc_instances) # train generator if optimizer_idx == 0: # # generate images # self.generated_sc_instances = self(z) # # log sampled images # sample_instances = self.generated_sc_instances[:6] # grid = torchvision.utils.make_grid(sample_sc_instances) # self.logger.experiment.add_image(&#39;generated_images&#39;, grid, 0) # ground truth result (ie: all fake) # put on GPU because we created this tensor inside training_loop valid = torch.ones(sc_instances.size(0), 1) valid = valid.type_as(sc_instances) # adversarial loss is binary cross-entropy g_loss = self.adversarial_loss(self.discriminator(self(z)), valid) tqdm_dict = {&#39;g_loss&#39;: g_loss} output = OrderedDict({ &#39;loss&#39;: g_loss, &#39;progress_bar&#39;: tqdm_dict, &#39;log&#39;: tqdm_dict }) return output # train discriminator if optimizer_idx == 1: # Measure discriminator&#39;s ability to classify real from generated samples # how well can it label as real? valid = torch.ones(sc_instances.size(0), 1) valid = valid.type_as(sc_instances) real_loss = self.adversarial_loss(self.discriminator(sc_instances), valid) # how well can it label as fake? fake = torch.zeros(sc_instances.size(0), 1) fake = fake.type_as(sc_instances) fake_loss = self.adversarial_loss( self.discriminator(self(z).detach()), fake) # discriminator loss is the average of these d_loss = (real_loss + fake_loss) / 2 tqdm_dict = {&#39;d_loss&#39;: d_loss} output = OrderedDict({ &#39;loss&#39;: d_loss, &#39;progress_bar&#39;: tqdm_dict, &#39;log&#39;: tqdm_dict }) return output def configure_optimizers(self): lr = self.hparams.lr b1 = self.hparams.b1 b2 = self.hparams.b2 opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2)) opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2)) return [opt_g, opt_d], [] def on_epoch_end(self): z = self.validation_z.type_as(self.generator.model[0].weight) # log sampled images sample_sc_instances = self(z) grid = torchvision.utils.make_grid(sample_sc_instances) self.logger.experiment.add_image(&#39;generated_sc_instances&#39;, grid, self.current_epoch) . dm = SingleCellDataModule() dm.setup() model = SingleCellGAN(tuple(next(iter(dm.train_dataset))[0].shape)) trainer = pl.Trainer(gpus=1, max_epochs=25, fast_dev_run=False) trainer.fit(model, dm) . /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You have set progress_bar_refresh_rate &lt; 20 on Google Colab. This may crash. Consider using progress_bar_refresh_rate &gt;= 20 in Trainer. warnings.warn(*args, **kwargs) GPU available: True, used: True TPU available: None, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . Data loaded. X is of shape torch.Size([1978, 1838]), with y of shape torch.Size([1978]) Data loaded. X is of shape torch.Size([660, 1838]), with y of shape torch.Size([660]) . /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop warnings.warn(*args, **kwargs) | Name | Type | Params | In sizes | Out sizes -- 0 | generator | Generator | 2.6 M | [2, 100] | [2, 1838] 1 | discriminator | Discriminator | 1.1 M | ? | ? -- 3.7 M Trainable params 0 Non-trainable params 3.7 M Total params /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The {progress_bar:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0 Please use self.log(...) inside the lightningModule instead. # log on a step or aggregate epoch metric to the logger and/or progress bar # (inside LightningModule) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) warnings.warn(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0 Please use self.log(...) inside the lightningModule instead. # log on a step or aggregate epoch metric to the logger and/or progress bar # (inside LightningModule) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) warnings.warn(*args, **kwargs) . . 1 . Conclusion . We&#39;ve trained a GAN! Now, the model can be saved and the Generator object can be put into production as a single-cell data synthesizer. I hope you&#39;ve seen how powerful this framework can be and I look forward to seeing what generative models can do for the biosciences. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/17/_01_16_generate_me-(1).html",
            "relUrl": "/jupyter/2021/01/17/_01_16_generate_me-(1).html",
            "date": " • Jan 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Kaokore Facial Verification",
            "content": ". Overview . The recent Kaokore (https://arxiv.org/abs/2002.08595) facial dataset presents some exciting creative opportunities for machine learning on these images. We have nearly 9,000 faces, each associated with a male or female gender label as well as a social status label (one of Noble, Warrior, Incarnation, or Commoner). . Approach . I&#39;ve built a quick ResNet18 pipeline. No training here and this is totally unsupervised (for now!). Rather, I simply embed each painting into a vector $ vec{v}$ of cardinality 1,000. Then, these vectors are fitted to a k-nearest-neighbors ($k=5$ for now) estimator from scikit-learn. Finally, I embed my some human faces (including my own) as queries to this estimator, then retrieve the indices of the k-most-similar vectors. The results are pretty rough, and my plan is to fine-tune FaceNet or another more niche network on these data. Another possibility is a more recent self-supervised model trained with Contrastive Predictive Coding (CPC). . . %matplotlib inline from IPython.display import Image from IPython.display import display import joblib from torchvision.datasets import ImageFolder from torch.utils.data import DataLoader import torchvision.transforms as transforms import numpy as np import torchvision.models as models from tqdm.notebook import tqdm from sklearn.neighbors import NearestNeighbors from sklearn.externals import joblib import torch from torch.autograd import Variable from PIL import Image from torchvision.utils import make_grid from IPython.display import Image from IPython.display import display import torchvision DEVICE = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) . /usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+. warnings.warn(msg, category=FutureWarning) . !git clone https://github.com/rois-codh/kaokore . Cloning into &#39;kaokore&#39;... remote: Enumerating objects: 85, done. remote: Counting objects: 100% (85/85), done. remote: Compressing objects: 100% (75/75), done. remote: Total 85 (delta 33), reused 39 (delta 7), pack-reused 0 Unpacking objects: 100% (85/85), done. . !cd kaokore &amp;&amp; python3 download.py --dataset_version 1.2 . Downloading Kaokore version 1.2, saving to kaokore Downloading 8848 images using 16 threads 36% 3179/8848 [02:14&lt;01:51, 50.83it/s]Grayscale -&gt; RGB : kaokore/images_256/00003184.jpg 36% 3185/8848 [02:14&lt;01:54, 49.55it/s]Grayscale -&gt; RGB : kaokore/images_256/00003185.jpg Grayscale -&gt; RGB : kaokore/images_256/00003186.jpg Grayscale -&gt; RGB : kaokore/images_256/00003188.jpg Grayscale -&gt; RGB : kaokore/images_256/00003190.jpg Grayscale -&gt; RGB : kaokore/images_256/00003191.jpg Grayscale -&gt; RGB : kaokore/images_256/00003192.jpg 36% 3191/8848 [02:14&lt;02:04, 45.41it/s]Grayscale -&gt; RGB : kaokore/images_256/00003193.jpg Grayscale -&gt; RGB : kaokore/images_256/00003194.jpg Grayscale -&gt; RGB : kaokore/images_256/00003187.jpg Grayscale -&gt; RGB : kaokore/images_256/00003195.jpg Grayscale -&gt; RGB : kaokore/images_256/00003189.jpg 36% 3196/8848 [02:15&lt;02:05, 44.94it/s]Grayscale -&gt; RGB : kaokore/images_256/00003196.jpg Grayscale -&gt; RGB : kaokore/images_256/00003197.jpg Grayscale -&gt; RGB : kaokore/images_256/00003198.jpg Grayscale -&gt; RGB : kaokore/images_256/00003200.jpg 36% 3202/8848 [02:15&lt;02:07, 44.27it/s]Grayscale -&gt; RGB : kaokore/images_256/00003199.jpg 100% 8846/8848 [06:25&lt;00:00, 15.86it/s] 100% 8848/8848 [06:25&lt;00:00, 22.94it/s] . class Embedder: def __init__(self): self.image_embeddings=[] self.model = models.resnet18(pretrained=True).to(DEVICE) self.imsize=256 def load_images(self): self.dataset = ImageFolder(root=&#39;./kaokore&#39;,transform=torchvision.transforms.ToTensor()) self.loader = DataLoader(self.dataset) def embed_images(self): for x,_ in tqdm(self.loader): x = x.to(DEVICE) fx = self.model(x) self.image_embeddings.append(fx.detach().cpu().numpy().flatten()) def generate_estimator(self):#,model_fname): knn_estimator = NearestNeighbors(n_neighbors=5, metric=&quot;cosine&quot;) knn_estimator.fit(np.array(self.image_embeddings)) # save the model to disk ? # joblib.dump(knn_estimator, model_fname) return knn_estimator def embed_single_image(self,image_fpath): &quot;&quot;&quot;load image, returns cuda tensor&quot;&quot;&quot; loader = transforms.Compose([transforms.Scale(self.imsize), transforms.ToTensor()]) image = Image.open(image_fpath) image = loader(image).float() image = Variable(image, requires_grad=True) image = image.unsqueeze(0) #this is for VGG, may not be needed for ResNet image = image.to(DEVICE) embedding = self.model(image).detach().cpu().numpy().flatten() return embedding def query_knn(self, query_embedding): _, top_indices = self.estimator.kneighbors([query_embedding]) # find k nearest train neighbours top_indices=top_indices.flatten() return top_indices . # Fit kNN model on embedded images k = 5 image_embedder = Embedder() print(&#39;Loading images...&#39;) image_embedder.load_images() print(&#39;Embedding Kaokore paintings with a neural network...&#39;) image_embedder.embed_images() . Loading images... Embedding Kaokore paintings with a neural network... . print(f&#39;Fitting and saving {k}-nearest-neighbour model on image embeddings...&#39;) knn=image_embedder.generate_estimator() . Fitting and saving 5-nearest-neighbour model on image embeddings... . knn . NearestNeighbors(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;cosine&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0) . test1=&#39;/content/63212182948__89FADCD6-CDD0-4821-A046-9C3237E9732B.jpeg&#39; test2=&#39;/content/63212325784__F25E76D5-9E72-408A-9256-2BFE5A7E4895.jpeg&#39; test3=&#39;/content/test.jpg&#39; images_dataset = ImageFolder(root=&#39;./kaokore&#39;) . from PIL import Image import matplotlib.pyplot as plt results=[] test_img_fpaths = [test1,test2,test3] # inferrer = Inference(image_embedder) for img in test_img_fpaths: print(img) test_embedding= image_embedder.embed_single_image(img) _,top_k_indices = knn.kneighbors([test_embedding]) top_k_indices=top_k_indices.flatten() # top_k_indices = image_embedder.query_knn(test_embedding) print(f&#39;top {k} indices found were n n{top_k_indices}&#39;) print(&#39;Saving a grid of these images...&#39;) results.append(top_k_indices) . /content/63212182948__89FADCD6-CDD0-4821-A046-9C3237E9732B.jpeg top 5 indices found were [5891 6531 3822 7168 2401] Saving a grid of these images... /content/63212325784__F25E76D5-9E72-408A-9256-2BFE5A7E4895.jpeg . /usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead. &#34;please use transforms.Resize instead.&#34;) /usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead. &#34;please use transforms.Resize instead.&#34;) . top 5 indices found were [7182 6006 139 5121 4420] Saving a grid of these images... /content/test.jpg top 5 indices found were [6648 2944 1457 2391 6892] Saving a grid of these images... . /usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:280: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead. &#34;please use transforms.Resize instead.&#34;) . from IPython.display import display, Image img_list1 = [images_dataset[i][0] for i in results[0]] img_list2 = [images_dataset[i][0] for i in results[1]] img_list3 = [images_dataset[i][0] for i in results[2]] . from IPython.display import Image, display display(Image(filename=&#39;./images_kaokore/test.jpg&#39;,width=300)) . display(*img_list3) . Work in progress... . I hope to fine-tune a triplet-loss or self-supervised model on the paintings at some point. This should result in better matches. Stay tuned! .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/11/kaokore.html",
            "relUrl": "/jupyter/2021/01/11/kaokore.html",
            "date": " • Jan 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Everything but the Model",
            "content": "Background . Most of the attention with regard to machine learning goes to the models, with good reason. We are at the point where we can reasonably model any $X to y$ relationship we wish. Difficult classification and clustering problems are increasingly tractable with clever approaches in statistical and rules-based learning. Even if we have no clue how to begin to go about developing such an approach (or such an approach would be intractable / biased), we can train deep neural networks – Universal Approximators – to theoretically represent almost any $f(x)= y = Wx + b$ relationship given properly trained weights $W$. . Still, humans are likely to be of use for the foreseeable future, and one of the more useful domains is the preparation of dat for such modeling. This includes topics such as feature selection and feature extraction, and how we might select the best model based on a rigorous, generalizable evaluation metric. . In this post, I will outline some of these useful topics with the hope that you will be able to tackle most everything about machine learning but the machine itself. . Example Dataset . I will use the COVID-19 Cell Atlas&#39; nasal immunodefiency swab dataset (https://www.sanger.ac.uk/group/vento-tormo-group/) for examples to follow. This is a recent dataset of real patients. I will use the scanpy package to load in the data and take a pandas dataframe for examples for feed into scikit-learn. . For the response variable, I will use Vasoactive agents required during hospitalization, a proxy for severity of symptoms and infection. . CellType log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Lab number Donor Id Age Sex Race ... Pre-existing Hypertension Pre-existing immunocompromised condition Smoking SARS-CoV-2 PCR SARS-CoV-2 Ab Symptomatic Admitted to hospital Highest level of respiratory support Vasoactive agents required during hospitalization 28-day death . GW1_AAACGGGAGCTAGTCT-1 Secretary epithelium | 14.909096 | 5687 | 0.042059 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AAAGTAGTCCTAGGGC-1 Secretary epithelium KRT5 | 13.611947 | 3967 | 0.097771 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AACACGTCAGCGTCCA-1 Ciliated epithelium | 9.366322 | 513 | 0.036419 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AACCATGAGAATCTCC-1 Secretary epithelium | 15.217731 | 6260 | 0.053720 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . GW1_AACCATGCATCCTTGC-1 Ciliated epithelium | 9.134426 | 439 | 0.016043 | 0 | CV19-1-S3.2A | GWAS_1 | 18 | F | White | ... | No | Yes | Never or unknown | Positive | NaN | Yes | Yes | Mechanical ventilation with intubation | Yes | No | . 5 rows × 26 columns . X.columns . Index([&#39;CellType&#39;, &#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;, &#39;MT_fraction&#39;, &#39;Viral Molecules&#39;, &#39;Lab number&#39;, &#39;Donor Id&#39;, &#39;Age&#39;, &#39;Sex&#39;, &#39;Race&#39;, &#39;Ethnicity&#39;, &#39;BMI&#39;, &#39;Pre-existing heart disease&#39;, &#39;Pre-existing lung disease&#39;, &#39;Pre-existing kidney disease&#39;, &#39;Pre-existing diabetes&#39;, &#39;Pre-existing Hypertension&#39;, &#39;Pre-existing immunocompromised condition&#39;, &#39;Smoking&#39;, &#39;SARS-CoV-2 PCR&#39;, &#39;SARS-CoV-2 Ab&#39;, &#39;Symptomatic&#39;, &#39;Admitted to hospital&#39;, &#39;Highest level of respiratory support&#39;, &#39;28-day death&#39;], dtype=&#39;object&#39;) . This is a good toy dataset. We have a mixture of categorical, continuous-valued, integer-valued, string-valued, and others, as well as a clean binary Yes or No response variable. . Wrangling . A few short operations will make life easier later on. . Dealing with NaN (Missing Values) . You can see in the SARS-CoV-2 Ab column that we have NaN values. Although classifier implementations may have built-in accommodations, it may best best to deal with these values in a way we can fully control. . print(X[&#39;SARS-CoV-2 Ab&#39;]) . GW1_AAACGGGAGCTAGTCT-1 NaN GW1_AAAGTAGTCCTAGGGC-1 NaN GW1_AACACGTCAGCGTCCA-1 NaN GW1_AACCATGAGAATCTCC-1 NaN GW1_AACCATGCATCCTTGC-1 NaN ... GW13_TTTCCTCCAAGCCTAT-1 NaN GW13_TTTCCTCCAAGTCTGT-1 NaN GW13_TTTGTCAAGCCCAATT-1 NaN GW13_TTTGTCAGTAGGACAC-1 NaN GW13_TTTGTCATCGTGTAGT-1 NaN Name: SARS-CoV-2 Ab, Length: 4936, dtype: category Categories (3, object): [&#39;Not done&#39; &lt; &#39;Negative&#39; &lt; &#39;Positive&#39;] . I suspect this is just an indicator variable to show whether or not the patient recieved an antibody test, we can look at unique values to be sure. . X[&#39;SARS-CoV-2 Ab&#39;].unique() . [NaN] Categories (0, object): [] . It is. In this case, let&#39;s just drop the column rather. It is likely uninformative with regard to the reponse variable y = Vasoactive agents required during hospitalization. . X = X.drop(columns=[&#39;SARS-CoV-2 Ab&#39;]) print(X.shape,dataset.obs.shape, sep=&#39; n&#39;) . (4936, 24) (4936, 26) . As expected. . Dealing with Categorial Features . We will need to encode nominal and/or ordinal features to a one-hot representation. We can easily exclude numerical-valued columns from this process. We should also binarize the $y$ labels. . numer_cols = list(X._get_numeric_data().columns) cat_cols = list(set(X.columns) - set(numer_cols)) print(f&#39;numerical columns: n{numer_cols} n ncategorical columns: n {cat_cols}&#39;) . numerical columns: [&#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;, &#39;MT_fraction&#39;, &#39;Viral Molecules&#39;] categorical columns: [&#39;Race&#39;, &#39;Lab number&#39;, &#39;Pre-existing immunocompromised condition&#39;, &#39;Ethnicity&#39;, &#39;28-day death&#39;, &#39;Admitted to hospital&#39;, &#39;Pre-existing diabetes&#39;, &#39;BMI&#39;, &#39;Pre-existing Hypertension&#39;, &#39;Donor Id&#39;, &#39;Pre-existing kidney disease&#39;, &#39;Age&#39;, &#39;Symptomatic&#39;, &#39;Pre-existing heart disease&#39;, &#39;Pre-existing lung disease&#39;, &#39;Smoking&#39;, &#39;Sex&#39;, &#39;Highest level of respiratory support&#39;, &#39;CellType&#39;, &#39;SARS-CoV-2 PCR&#39;] . With the exception of Age (should be numeric) and BMI (should be ordinal), this looks fine (since we have a BMI range, we should use an ordinal encoder in this case). The rest are simply categorical. . numer_cols.append(&#39;Age&#39;) cat_cols.remove(&#39;Age&#39;) print(f&#39;numerical columns: n{numer_cols} n ncategorical columns: n {cat_cols}&#39;) . numerical columns: [&#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;, &#39;MT_fraction&#39;, &#39;Viral Molecules&#39;, &#39;Age&#39;] categorical columns: [&#39;Race&#39;, &#39;Lab number&#39;, &#39;Pre-existing immunocompromised condition&#39;, &#39;Ethnicity&#39;, &#39;28-day death&#39;, &#39;Admitted to hospital&#39;, &#39;Pre-existing diabetes&#39;, &#39;BMI&#39;, &#39;Pre-existing Hypertension&#39;, &#39;Donor Id&#39;, &#39;Pre-existing kidney disease&#39;, &#39;Symptomatic&#39;, &#39;Pre-existing heart disease&#39;, &#39;Pre-existing lung disease&#39;, &#39;Smoking&#39;, &#39;Sex&#39;, &#39;Highest level of respiratory support&#39;, &#39;CellType&#39;, &#39;SARS-CoV-2 PCR&#39;] . ord_cols = [&#39;BMI&#39;] cat_cols = [i for i in cat_cols if i not in ord_cols] X[cat_cols].head(3) . Race Lab number Pre-existing immunocompromised condition Ethnicity 28-day death Admitted to hospital Pre-existing diabetes Pre-existing Hypertension Donor Id Pre-existing kidney disease Symptomatic Pre-existing heart disease Pre-existing lung disease Smoking Sex Highest level of respiratory support CellType SARS-CoV-2 PCR . GW1_AAACGGGAGCTAGTCT-1 White | CV19-1-S3.2A | Yes | Not Hispanic or Latino | No | Yes | No | No | GWAS_1 | No | Yes | No | No | Never or unknown | F | Mechanical ventilation with intubation | Secretary epithelium | Positive | . GW1_AAAGTAGTCCTAGGGC-1 White | CV19-1-S3.2A | Yes | Not Hispanic or Latino | No | Yes | No | No | GWAS_1 | No | Yes | No | No | Never or unknown | F | Mechanical ventilation with intubation | Secretary epithelium KRT5 | Positive | . GW1_AACACGTCAGCGTCCA-1 White | CV19-1-S3.2A | Yes | Not Hispanic or Latino | No | Yes | No | No | GWAS_1 | No | Yes | No | No | Never or unknown | F | Mechanical ventilation with intubation | Ciliated epithelium | Positive | . Now we can encode these properly using scikit-learn or a builtin pandas method (we will use the latter). While we could use an ordinal encoder, whereby one clas is mapped to an integer, we should actually use one-hot encoding as this is a continuous input, valid for scikit-learn estimators. Note that a NaN is treated as a distinct category, and note how Pandas will automatically ignore numeerical columns of X. We just need to watch out for the BMI variable since it should be ordinal. . We then can append the BMI column with ordinal information. . import pandas as pd X_cat = pd.get_dummies(X[cat_cols]) X_cat.head(2) . Race_White Race_Black Race_Asian Race_Other Lab number_CV19-1-S3.2A Lab number_CV19-11-S3.2A Lab number_CV19-12-S3.2A Lab number_CV19-13-S3.2A Pre-existing immunocompromised condition_No Pre-existing immunocompromised condition_Yes ... CellType_Secretary epithelium KRT5 CellType_Squamous epithelium 1 CellType_Squamous epithelium 2 CellType_Ciliated epithelium CellType_Neutrophil CellType_Erythrocytes CellType_Low quality CellType_filtered cells and doublets SARS-CoV-2 PCR_Positive SARS-CoV-2 PCR_Negative . GW1_AAACGGGAGCTAGTCT-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . GW1_AAAGTAGTCCTAGGGC-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 rows × 65 columns . X[&#39;BMI&#39;].unique() . [&#39;30.0-39.9 (obese)&#39;, &#39;25.0-29.9 (overweight)&#39;, &#39;Unknown&#39;] Categories (3, object): [&#39;25.0-29.9 (overweight)&#39; &lt; &#39;30.0-39.9 (obese)&#39; &lt; &#39;Unknown&#39;] . scale_mapper = {&#39;Unknown&#39;:1,&#39;25.0-29.9 (overweight)&#39;:2,&#39;30.0-39.9 (obese)&#39;:3} X_ord = X[&#39;BMI&#39;].replace(scale_mapper) X_ord . GW1_AAACGGGAGCTAGTCT-1 3 GW1_AAAGTAGTCCTAGGGC-1 3 GW1_AACACGTCAGCGTCCA-1 3 GW1_AACCATGAGAATCTCC-1 3 GW1_AACCATGCATCCTTGC-1 3 .. GW13_TTTCCTCCAAGCCTAT-1 1 GW13_TTTCCTCCAAGTCTGT-1 1 GW13_TTTGTCAAGCCCAATT-1 1 GW13_TTTGTCAGTAGGACAC-1 1 GW13_TTTGTCATCGTGTAGT-1 1 Name: BMI, Length: 4936, dtype: int64 . We now have X_cat and X_ord. All that&#39;s left is to binarize the response variable y. . from sklearn import preprocessing y = preprocessing.LabelEncoder().fit_transform(y) y . array([1, 1, 1, ..., 0, 0, 0]) . Feature Rescaling . Before we combine these encoded categorical columns, it may be prudent to rescale numerical features, especially if each feature is not on the same given scale. This may be done to remove the bias of certain features given downstream tasks. For instance, take the following features from our $X$: log2p1_RNAcount and nFeature_RNA. . X[[&#39;log2p1_RNAcount&#39;, &#39;nFeature_RNA&#39;]].head() . log2p1_RNAcount nFeature_RNA . GW1_AAACGGGAGCTAGTCT-1 14.909096 | 5687 | . GW1_AAAGTAGTCCTAGGGC-1 13.611947 | 3967 | . GW1_AACACGTCAGCGTCCA-1 9.366322 | 513 | . GW1_AACCATGAGAATCTCC-1 15.217731 | 6260 | . GW1_AACCATGCATCCTTGC-1 9.134426 | 439 | . In an extraction process (such as PCA, making use of covariance), or when using a classifier making use of Euclidean distance, the feature with the largest numerical range will be naturally more weighted. . x_1 = X[&#39;log2p1_RNAcount&#39;] x_2 = X[&#39;nFeature_RNA&#39;] range_1 = x_1.max()-x_1.min() range_2 = x_2.max()-x_2.min() print(range_1, range_2, sep=&#39; n&#39;) . 12.476429788752128 9651 . nFeature_RNA therefore has a much more significant bearing on an outcome contingent upon this range. . We therefore rescale in a few ways: . 1) with min/max rescaling $x_i&#39; = frac{x_i - min(x)}{max(x) - min(x)}$ | simple, preserves mean of dataset. Useful for image pixel intensity, for instance. | . | 2) with $z$-score normalization . $x_i&#39; = frac{x_i - bar{x}}{ sigma}$ = $ text{number of standard deviations from the mean}$ = $z text{-score}$ | standardizes features, typically with $ mu = 0, sigma^2 = 1$. This is a better choice than min/max for things like PCA since in that case we want to select components maximizing variance of the feature matrix, without getting caught up by the scale of that variance. | . | 3) with median and interquartile range rescaling (robust rescaling). . Remove the median value and scale according to interquartile range. | better choice if there are significant outliers. | . | . Let&#39;s go with 3) somewhat arbitrarily but also in the case of outliers. . from sklearn import preprocessing robust_scaler = preprocessing.RobustScaler() X_numer = robust_scaler.fit_transform(X[numer_cols]) X_numer . array([[ 2.10213024, 5.15834811, 0.35863219, 0. , -0.76190476], [ 1.52049401, 3.41525209, 2.21766008, 0. , -0.76190476], [-0.38322616, -0.08512795, 0.17041353, 0. , -0.76190476], ..., [ 1.54686863, 2.91867241, 0.05551372, 0. , 0. ], [-0.48147742, -0.28781353, 0.42904635, 0. , 0. ], [-0.26773767, -0.06080568, -0.07087445, 0. , 0. ]]) . Finally, we can recombine these processed data. . X[numer_cols] = X_numer X.head(2) . CellType log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Lab number Donor Id Age Sex Race ... Pre-existing kidney disease Pre-existing diabetes Pre-existing Hypertension Pre-existing immunocompromised condition Smoking SARS-CoV-2 PCR Symptomatic Admitted to hospital Highest level of respiratory support 28-day death . GW1_AAACGGGAGCTAGTCT-1 Secretary epithelium | 2.102130 | 5.158348 | 0.358632 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AAAGTAGTCCTAGGGC-1 Secretary epithelium KRT5 | 1.520494 | 3.415252 | 2.217660 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . 2 rows × 24 columns . X[&#39;BMI&#39;] = X_ord X[&#39;BMI&#39;] . GW1_AAACGGGAGCTAGTCT-1 3 GW1_AAAGTAGTCCTAGGGC-1 3 GW1_AACACGTCAGCGTCCA-1 3 GW1_AACCATGAGAATCTCC-1 3 GW1_AACCATGCATCCTTGC-1 3 .. GW13_TTTCCTCCAAGCCTAT-1 1 GW13_TTTCCTCCAAGTCTGT-1 1 GW13_TTTGTCAAGCCCAATT-1 1 GW13_TTTGTCAGTAGGACAC-1 1 GW13_TTTGTCATCGTGTAGT-1 1 Name: BMI, Length: 4936, dtype: int64 . X_concat = pd.concat([X_cat,X.drop(columns=cat_cols)],axis=1) X_concat . Race_White Race_Black Race_Asian Race_Other Lab number_CV19-1-S3.2A Lab number_CV19-11-S3.2A Lab number_CV19-12-S3.2A Lab number_CV19-13-S3.2A Pre-existing immunocompromised condition_No Pre-existing immunocompromised condition_Yes ... CellType_Low quality CellType_filtered cells and doublets SARS-CoV-2 PCR_Positive SARS-CoV-2 PCR_Negative log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age BMI . GW1_AAACGGGAGCTAGTCT-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 2.102130 | 5.158348 | 0.358632 | 0.0 | -0.761905 | 3 | . GW1_AAAGTAGTCCTAGGGC-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 1.520494 | 3.415252 | 2.217660 | 0.0 | -0.761905 | 3 | . GW1_AACACGTCAGCGTCCA-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | -0.383226 | -0.085128 | 0.170414 | 0.0 | -0.761905 | 3 | . GW1_AACCATGAGAATCTCC-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 2.240521 | 5.739042 | 0.747728 | 0.0 | -0.761905 | 3 | . GW1_AACCATGCATCCTTGC-1 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | -0.487207 | -0.160122 | -0.509505 | 0.0 | -0.761905 | 3 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . GW13_TTTCCTCCAAGCCTAT-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0.274377 | 0.276666 | -0.477400 | 0.0 | 0.000000 | 1 | . GW13_TTTCCTCCAAGTCTGT-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0.378640 | 0.409425 | -0.530748 | 0.0 | 0.000000 | 1 | . GW13_TTTGTCAAGCCCAATT-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 1.546869 | 2.918672 | 0.055514 | 0.0 | 0.000000 | 1 | . GW13_TTTGTCAGTAGGACAC-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | -0.481477 | -0.287814 | 0.429046 | 0.0 | 0.000000 | 1 | . GW13_TTTGTCATCGTGTAGT-1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | -0.267738 | -0.060806 | -0.070874 | 0.0 | 0.000000 | 1 | . 4936 rows × 71 columns . Feature Extraction . Feature extraction is the process of reducing dimensionality to find (important) latent features in a given feature set. This may be useful in the case of our dataset since our encoded feature size has swelled to 71! The motivation is also very philosophical. What is the self, if not the most important features that others learn to associate your identity with? Your identity in the world is one of useful features learned by others. We will see how we can extract and/or learn a representation of features such that a model operating on such a representation can generalize to as many of the original features as possible. . Extraction can be done in a variety of ways for a variety of use cases. As for why, let&#39;s say we have a dataset with a massive number of features, such that training a network to make use of all the features somewhat equally. We want to use as few (latent) features as possible to completely represent the original dataset. That is, we are not selecting features, but rather finding a new, smaller feature set derivative of the original. This may be a linear mapping, or a non-linear manifold-learning process. . Methods Outlined and a Simple Autoencoder . Feature learning can be accomplished via common dimensionality reduction methods such as PCA, UMAP (shown above on dataset), and tSNE (technically, a non-linear method of extracting features). Linear Discriminant Analysis (LDA) is a useful technique to maximize class separation when labels are known. I will mention that when in doubt, use UMAP. It&#39;s very effective, extensible, and can even be used as a quasi-clustering technique. . Another method of feature-representation is of course the autoencoder and variational autoencoder. The former works to reconstruct the feature set based upon an encoder-decoder neural network, trained with gradient descent to learn weights. We then discard the decoder layer and have are left with a function useful for identifying critical latent representations of the original features. . In the vanilla autoencoder&#39;s case, a single value for each encoding dimension is found (this is usually the final layer of a multilayer perceptron, or a linear head stacked upon a convolutional net or other flavor). In the variational case, the network&#39;s encoder learns a probability distribution for each latent attribute. The decoder then randomly samples from this space to generate an input vector for the decoder. This enforces a smooth representation of latent space, allowing for variational expectation maximization / inference. As far as feature learning goes, allows for latent features to be maximally disentangled, as an isotropic Gaussian (such that any single dimension / single Gaussian of this multivariate distribution over features is not covariant with any other). . I will show a simple autoencoder implementation below using the neat pytorch-lightning package. We will train and the reconstructed features will be compared to the original. . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) . import torch from torch import nn, optim from torch.utils.data import random_split, DataLoader,Dataset from torch.nn import functional as F from torchvision import transforms import pytorch_lightning as pl class CovidDataset(Dataset): def __init__(self,X,y): self.data = torch.FloatTensor(X.values.astype(&#39;float&#39;)) self.labels = torch.LongTensor(y) #not needed for autoencoder. def __len__(self): return len(self.data) def __getitem__(self,index): return self.data[index], self.labels[index] . CovidDataset(X_test,y_test)[1] . (tensor([ 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.4500, 0.7216, -0.4589, 0.0000, 0.0000, 1.0000]), tensor(0)) . class CovidDataModule(pl.LightningDataModule): def __init__(self,train_dataset,val_dataset): super().__init__() self.train_dataset = train_dataset self.val_dataset = val_dataset def train_dataloader(self): return DataLoader(self.train_dataset, batch_size=64) def val_dataloader(self): return DataLoader(self.val_dataset, batch_size=64) . covid_dm = CovidDataModule(CovidDataset(X_train,y_train), CovidDataset(X_test,y_test)) covid_dm . &lt;__main__.CovidDataModule at 0x7f9b1ee75ac0&gt; . import torch from torch import nn, optim from torch.utils.data import random_split, DataLoader from torch.nn import functional as F import torch.nn as nn from torchvision import transforms import pytorch_lightning as pl class CovidAutoencoder(pl.LightningModule): def __init__(self): super().__init__() self.encoder = nn.Sequential( nn.Linear(71,128), nn.ReLU(inplace=True), nn.Linear(128, 64), nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3) ) self.decoder = nn.Sequential( nn.Linear(3, 12), nn.ReLU(True), nn.Linear(12, 64), nn.ReLU(True), nn.Linear(64, 128), nn.ReLU(True), nn.Linear(128, 71, nn.Tanh()) ) def forward(self, x): x = self.encoder(x) x = self.decoder(x) return x def configure_optimizers(self): return torch.optim.Adam(self.parameters(), lr=1e-3) def training_step(self, batch, batch_idx): x, _ = batch #no need for labels fx = self(x) loss = F.mse_loss(x, fx) self.log(&#39;train_loss&#39;, loss) return loss def validation_step(self, batch, batch_idx): x, _ = batch #no need for labels fx = self(x) loss = F.mse_loss(x, fx) self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) . model = CovidAutoencoder() trainer = pl.Trainer(fast_dev_run=False,max_epochs=25) trainer.fit(model=model, datamodule=covid_dm) . GPU available: False, used: False GPU available: False, used: False TPU available: None, using: 0 TPU cores TPU available: None, using: 0 TPU cores | Name | Type | Params 0 | encoder | Sequential | 18.3 K 1 | decoder | Sequential | 18.4 K 36.6 K Trainable params 0 Non-trainable params 36.6 K Total params | Name | Type | Params 0 | encoder | Sequential | 18.3 K 1 | decoder | Sequential | 18.4 K 36.6 K Trainable params 0 Non-trainable params 36.6 K Total params Epoch 0: 67%|██████▋ | 52/78 [00:00&lt;00:00, 202.21it/s, loss=0.205, v_num=11, val_loss_epoch=0.479] Epoch 0: 100%|██████████| 78/78 [00:00&lt;00:00, 253.67it/s, loss=0.205, v_num=11, val_loss_epoch=0.194, val_loss_step=0.133] Epoch 1: 67%|██████▋ | 52/78 [00:00&lt;00:00, 203.16it/s, loss=0.161, v_num=11, val_loss_epoch=0.194, val_loss_step=0.133] Epoch 1: 100%|██████████| 78/78 [00:00&lt;00:00, 256.48it/s, loss=0.161, v_num=11, val_loss_epoch=0.138, val_loss_step=0.126] Epoch 2: 67%|██████▋ | 52/78 [00:00&lt;00:00, 196.89it/s, loss=0.0841, v_num=11, val_loss_epoch=0.138, val_loss_step=0.126] Epoch 2: 100%|██████████| 78/78 [00:00&lt;00:00, 248.57it/s, loss=0.0841, v_num=11, val_loss_epoch=0.0676, val_loss_step=0.0759] Epoch 3: 67%|██████▋ | 52/78 [00:00&lt;00:00, 198.54it/s, loss=0.0484, v_num=11, val_loss_epoch=0.0676, val_loss_step=0.0759] Epoch 3: 100%|██████████| 78/78 [00:00&lt;00:00, 251.11it/s, loss=0.0484, v_num=11, val_loss_epoch=0.0427, val_loss_step=0.0468] Epoch 4: 67%|██████▋ | 52/78 [00:00&lt;00:00, 195.16it/s, loss=0.041, v_num=11, val_loss_epoch=0.0427, val_loss_step=0.0468] Epoch 4: 100%|██████████| 78/78 [00:00&lt;00:00, 245.66it/s, loss=0.041, v_num=11, val_loss_epoch=0.0385, val_loss_step=0.0436] Epoch 5: 67%|██████▋ | 52/78 [00:00&lt;00:00, 192.33it/s, loss=0.0365, v_num=11, val_loss_epoch=0.0385, val_loss_step=0.0436] Epoch 5: 100%|██████████| 78/78 [00:00&lt;00:00, 243.62it/s, loss=0.0365, v_num=11, val_loss_epoch=0.0336, val_loss_step=0.0375] Epoch 6: 67%|██████▋ | 52/78 [00:00&lt;00:00, 203.55it/s, loss=0.0271, v_num=11, val_loss_epoch=0.0336, val_loss_step=0.0375] Epoch 6: 100%|██████████| 78/78 [00:00&lt;00:00, 255.54it/s, loss=0.0271, v_num=11, val_loss_epoch=0.0266, val_loss_step=0.0288] Epoch 7: 67%|██████▋ | 52/78 [00:00&lt;00:00, 195.35it/s, loss=0.0223, v_num=11, val_loss_epoch=0.0266, val_loss_step=0.0288] Epoch 7: 100%|██████████| 78/78 [00:00&lt;00:00, 244.57it/s, loss=0.0223, v_num=11, val_loss_epoch=0.0223, val_loss_step=0.026] Epoch 8: 67%|██████▋ | 52/78 [00:00&lt;00:00, 201.95it/s, loss=0.018, v_num=11, val_loss_epoch=0.0223, val_loss_step=0.026] Epoch 8: 100%|██████████| 78/78 [00:00&lt;00:00, 252.41it/s, loss=0.018, v_num=11, val_loss_epoch=0.0182, val_loss_step=0.0233] Epoch 9: 67%|██████▋ | 52/78 [00:00&lt;00:00, 206.98it/s, loss=0.016, v_num=11, val_loss_epoch=0.0182, val_loss_step=0.0233] Epoch 9: 100%|██████████| 78/78 [00:00&lt;00:00, 259.34it/s, loss=0.016, v_num=11, val_loss_epoch=0.0163, val_loss_step=0.021] Epoch 10: 67%|██████▋ | 52/78 [00:00&lt;00:00, 197.63it/s, loss=0.0144, v_num=11, val_loss_epoch=0.0163, val_loss_step=0.021] Epoch 10: 100%|██████████| 78/78 [00:00&lt;00:00, 247.62it/s, loss=0.0144, v_num=11, val_loss_epoch=0.0148, val_loss_step=0.0187] Epoch 11: 67%|██████▋ | 52/78 [00:00&lt;00:00, 184.67it/s, loss=0.013, v_num=11, val_loss_epoch=0.0148, val_loss_step=0.0187] Epoch 11: 100%|██████████| 78/78 [00:00&lt;00:00, 231.88it/s, loss=0.013, v_num=11, val_loss_epoch=0.0135, val_loss_step=0.0167] Epoch 12: 67%|██████▋ | 52/78 [00:00&lt;00:00, 200.98it/s, loss=0.0119, v_num=11, val_loss_epoch=0.0135, val_loss_step=0.0167] Epoch 12: 100%|██████████| 78/78 [00:00&lt;00:00, 245.95it/s, loss=0.0119, v_num=11, val_loss_epoch=0.0125, val_loss_step=0.015] Epoch 13: 67%|██████▋ | 52/78 [00:00&lt;00:00, 194.80it/s, loss=0.0111, v_num=11, val_loss_epoch=0.0125, val_loss_step=0.015] Epoch 13: 100%|██████████| 78/78 [00:00&lt;00:00, 245.03it/s, loss=0.0111, v_num=11, val_loss_epoch=0.0118, val_loss_step=0.0139] Epoch 14: 67%|██████▋ | 52/78 [00:00&lt;00:00, 193.38it/s, loss=0.0104, v_num=11, val_loss_epoch=0.0118, val_loss_step=0.0139] Epoch 14: 100%|██████████| 78/78 [00:00&lt;00:00, 242.07it/s, loss=0.0104, v_num=11, val_loss_epoch=0.0113, val_loss_step=0.0131] Epoch 15: 67%|██████▋ | 52/78 [00:00&lt;00:00, 183.31it/s, loss=0.00997, v_num=11, val_loss_epoch=0.0113, val_loss_step=0.0131] Epoch 15: 100%|██████████| 78/78 [00:00&lt;00:00, 232.88it/s, loss=0.00997, v_num=11, val_loss_epoch=0.0108, val_loss_step=0.0122] Epoch 16: 67%|██████▋ | 52/78 [00:00&lt;00:00, 193.03it/s, loss=0.00961, v_num=11, val_loss_epoch=0.0108, val_loss_step=0.0122] Epoch 16: 100%|██████████| 78/78 [00:00&lt;00:00, 241.64it/s, loss=0.00961, v_num=11, val_loss_epoch=0.0105, val_loss_step=0.0116] Epoch 17: 67%|██████▋ | 52/78 [00:00&lt;00:00, 191.10it/s, loss=0.00933, v_num=11, val_loss_epoch=0.0105, val_loss_step=0.0116] Epoch 17: 100%|██████████| 78/78 [00:00&lt;00:00, 241.13it/s, loss=0.00933, v_num=11, val_loss_epoch=0.0102, val_loss_step=0.0111] Epoch 18: 67%|██████▋ | 52/78 [00:00&lt;00:00, 190.33it/s, loss=0.00905, v_num=11, val_loss_epoch=0.0102, val_loss_step=0.0111] Epoch 18: 100%|██████████| 78/78 [00:00&lt;00:00, 238.68it/s, loss=0.00905, v_num=11, val_loss_epoch=0.00986, val_loss_step=0.0105] Epoch 19: 67%|██████▋ | 52/78 [00:00&lt;00:00, 192.96it/s, loss=0.00879, v_num=11, val_loss_epoch=0.00986, val_loss_step=0.0105] Epoch 19: 100%|██████████| 78/78 [00:00&lt;00:00, 243.82it/s, loss=0.00879, v_num=11, val_loss_epoch=0.00951, val_loss_step=0.01] Epoch 20: 67%|██████▋ | 52/78 [00:00&lt;00:00, 193.37it/s, loss=0.00859, v_num=11, val_loss_epoch=0.00951, val_loss_step=0.01] Epoch 20: 100%|██████████| 78/78 [00:00&lt;00:00, 244.96it/s, loss=0.00859, v_num=11, val_loss_epoch=0.00927, val_loss_step=0.00952] Epoch 21: 67%|██████▋ | 52/78 [00:00&lt;00:00, 179.89it/s, loss=0.00841, v_num=11, val_loss_epoch=0.00927, val_loss_step=0.00952] Epoch 21: 100%|██████████| 78/78 [00:00&lt;00:00, 227.89it/s, loss=0.00841, v_num=11, val_loss_epoch=0.00903, val_loss_step=0.00911] Epoch 22: 67%|██████▋ | 52/78 [00:00&lt;00:00, 190.09it/s, loss=0.00819, v_num=11, val_loss_epoch=0.00903, val_loss_step=0.00911] Epoch 22: 100%|██████████| 78/78 [00:00&lt;00:00, 239.86it/s, loss=0.00819, v_num=11, val_loss_epoch=0.00873, val_loss_step=0.00874] Epoch 23: 67%|██████▋ | 52/78 [00:00&lt;00:00, 186.86it/s, loss=0.00799, v_num=11, val_loss_epoch=0.00873, val_loss_step=0.00874] Epoch 23: 100%|██████████| 78/78 [00:00&lt;00:00, 237.02it/s, loss=0.00799, v_num=11, val_loss_epoch=0.00845, val_loss_step=0.00826] Epoch 24: 67%|██████▋ | 52/78 [00:00&lt;00:00, 192.72it/s, loss=0.00781, v_num=11, val_loss_epoch=0.00845, val_loss_step=0.00826] Epoch 24: 100%|██████████| 78/78 [00:00&lt;00:00, 242.28it/s, loss=0.00781, v_num=11, val_loss_epoch=0.00818, val_loss_step=0.00798] Epoch 24: 100%|██████████| 78/78 [00:00&lt;00:00, 236.67it/s, loss=0.00781, v_num=11, val_loss_epoch=0.00818, val_loss_step=0.00798] . 1 . In our case, mean square error is defined as $ frac{1}{n} sum_{i=1}^n( vec{x}-f( vec{x}))$. Thus, across the training dataset, we were able to train the network to reconstruct with a final loss (on a batch of 64 samples) of $ sim 0.008$, implying very low reconstructive loss despite only $3$ latent dimensions. . Feature Selection . Phew! Feature extraction, unsupervised learning, and data representation is its own discipline. We could spend days on that topic, and there are exciting developments along those lines – think state of the art language models and graphical embeddings. A future project I have in mind is actually strictly feature extraction, whereby a deep health embedding across time-series health data could improve clinical decisionmaking. . One critical aspect of the extraction process is that you do lose interpretability. In addition, you may not only lose linearity between $X to X_{embdedded}$ but also determinism. Some of these methods are effective, but indeed stochastic. This is of course on top of the &quot;black box&quot; problem of learning systems in general (just try inspecting even the simple autoencoder to parse its choices in reconstruction). . This is all to say that selection of existing features is a valuable skill to have. We can choose features as humans have provided them, rather than extracting features, and have stronger explainability. We can also use feature extraction / dimensionality reduction after this step. . Note that the following $X$ and $y$ do not correspond to processed/extracted data. . Variance Thresholding . We can select those features that are highly variant. That is, we will omit features (the numerical ones) that are not highly variant across samples. | That is, our new feature set is such that $ hat{ vec{x}} = x_i&gt;Var(x_i)$ such that $Var(x) = left( frac{1}{n} sum_{i=1}^n(x_i- mu)^2 right) forall x_i in hat{ vec{x}}$, our original feature vector. | Note: this is not a good idea to do when features are not in identical units, as is the case here. | Also, if a feature has previously been standardized to zero-mean and unit-variance, this technique is useless since we&#39;ve already asserted the variance. | . As such, the following is purely for illustrative purposes. . X_numer = X[numer_cols] X_numer.head(3) . log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age . GW1_AAACGGGAGCTAGTCT-1 14.909096 | 5687 | 0.042059 | 0 | 18 | . GW1_AAAGTAGTCCTAGGGC-1 13.611947 | 3967 | 0.097771 | 0 | 18 | . GW1_AACACGTCAGCGTCCA-1 9.366322 | 513 | 0.036419 | 0 | 18 | . from sklearn.feature_selection import VarianceThreshold thresholder = VarianceThreshold(threshold=0.50) thresholder.fit_transform(X_numer).shape . (4936, 3) . X[numer_cols].shape . (4936, 5) . We&#39;ve eliminated two features from our dataset. . Handling Highly Correlated Features . First, we must check for correlated features. If we don&#39;t we could be including redundant information and overfit to our dataset unnecessarily. . X.head(4) . CellType log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Lab number Donor Id Age Sex Race ... Pre-existing kidney disease Pre-existing diabetes Pre-existing Hypertension Pre-existing immunocompromised condition Smoking SARS-CoV-2 PCR Symptomatic Admitted to hospital Highest level of respiratory support 28-day death . GW1_AAACGGGAGCTAGTCT-1 Secretary epithelium | 2.102130 | 5.158348 | 0.358632 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AAAGTAGTCCTAGGGC-1 Secretary epithelium KRT5 | 1.520494 | 3.415252 | 2.217660 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACACGTCAGCGTCCA-1 Ciliated epithelium | -0.383226 | -0.085128 | 0.170414 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACCATGAGAATCTCC-1 Secretary epithelium | 2.240521 | 5.739042 | 0.747728 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . 4 rows × 24 columns . X_corr = X.corr().abs() X_corr . log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age BMI . log2p1_RNAcount 1.000000 | 0.908318 | 0.001661 | 0.107501 | 0.519020 | 0.035823 | . nFeature_RNA 0.908318 | 1.000000 | 0.086117 | 0.025669 | 0.443298 | 0.144945 | . MT_fraction 0.001661 | 0.086117 | 1.000000 | 0.021125 | 0.314239 | 0.180895 | . Viral Molecules 0.107501 | 0.025669 | 0.021125 | 1.000000 | 0.041918 | 0.028833 | . Age 0.519020 | 0.443298 | 0.314239 | 0.041918 | 1.000000 | 0.081834 | . BMI 0.035823 | 0.144945 | 0.180895 | 0.028833 | 0.081834 | 1.000000 | . Note that pandas considers strictly continuous-valued, numerical features. We can see that the diagonal, by definition, is strictly unit-valued. However, more interesting observations are apparent. For instance, nFeature_RNA is highly correlated to log2p1_RNAcount. This makes sense, as we would expect the number of features to increase with the single-cell RNA counts (and probably logarithmically, as each feature corresponds to many such counts). . We could choose to drop any features with high correlation, or simply . import numpy as np def upper_triangle(df:pd.DataFrame): return df.where(np.triu(np.ones(df.shape),k=1).astype(bool)) X_corr_upper = upper_triangle(X_corr) X_corr_upper . log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age BMI . log2p1_RNAcount NaN | 0.908318 | 0.001661 | 0.107501 | 0.519020 | 0.035823 | . nFeature_RNA NaN | NaN | 0.086117 | 0.025669 | 0.443298 | 0.144945 | . MT_fraction NaN | NaN | NaN | 0.021125 | 0.314239 | 0.180895 | . Viral Molecules NaN | NaN | NaN | NaN | 0.041918 | 0.028833 | . Age NaN | NaN | NaN | NaN | NaN | 0.081834 | . BMI NaN | NaN | NaN | NaN | NaN | NaN | . to_drop = [c for c in X_corr_upper if any(X_corr_upper[c]&gt;0.90)] to_drop . [&#39;nFeature_RNA&#39;] . X_uncorrelated = X.drop(columns=to_drop) X_uncorrelated.head() . CellType log2p1_RNAcount MT_fraction Viral Molecules Lab number Donor Id Age Sex Race Ethnicity ... Pre-existing kidney disease Pre-existing diabetes Pre-existing Hypertension Pre-existing immunocompromised condition Smoking SARS-CoV-2 PCR Symptomatic Admitted to hospital Highest level of respiratory support 28-day death . GW1_AAACGGGAGCTAGTCT-1 Secretary epithelium | 2.102130 | 0.358632 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AAAGTAGTCCTAGGGC-1 Secretary epithelium KRT5 | 1.520494 | 2.217660 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACACGTCAGCGTCCA-1 Ciliated epithelium | -0.383226 | 0.170414 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACCATGAGAATCTCC-1 Secretary epithelium | 2.240521 | 0.747728 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . GW1_AACCATGCATCCTTGC-1 Ciliated epithelium | -0.487207 | -0.509505 | 0.0 | CV19-1-S3.2A | GWAS_1 | -0.761905 | F | White | Not Hispanic or Latino | ... | No | No | No | Yes | Never or unknown | Positive | Yes | Yes | Mechanical ventilation with intubation | No | . 5 rows × 23 columns . As expected. . Removing Irrelevant Features for Supervised Learning . This is an analaous case to Linear Discriminant Analysis (LDA)1, in that we use the feature-target relationship to select only those features relevant to a target vector using a Chi-square ($ chi^2$) test for independence or an ANOVA (analysis of variance; for $ vec{x} in mathbb{R}$). In this fashion, we remove features that do not result in dignificant mean differences. This is not a classifier per se, but rather an intuitive method of eliminating features unlikely to be helpful in supervised settings. . . not to be confused with generative Latent Dirichlet Allocation, which allows sets of observations $ vec{y}$ to be explained by unobserved groups $ vec{x}_{unknown}$ that explain why some parts of the features of the data are similar).&#8617; . | Chi-Square Reduction of Categorical Features . We compute $ chi^2 = sum_i frac{(O_i-E_i)^2}{E^i}$. This is the sum of the number of class observations $O_i$ of a given categorical feature $i$ and the expected number $E_i$ if that feature was independent to the target vector. We then can select or omit feature $x_i$ based upon is $ chi^2$ value and the sklearn selector (a lower statistic is &quot;better&quot; in that the feature matches closely with what is expected, and as a result will be omitted). . from sklearn.feature_selection import chi2, SelectKBest X_cat.head(3) . 28-day death_No 28-day death_Yes Pre-existing diabetes_No Pre-existing diabetes_Yes Race_White Race_Black Race_Asian Race_Other Smoking_Never or unknown Smoking_Prior ... Lab number_CV19-1-S3.2A Lab number_CV19-11-S3.2A Lab number_CV19-12-S3.2A Lab number_CV19-13-S3.2A Sex_M Sex_F Pre-existing Hypertension_No Pre-existing Hypertension_Yes Pre-existing lung disease_No Pre-existing lung disease_Yes . GW1_AAACGGGAGCTAGTCT-1 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | . GW1_AAAGTAGTCCTAGGGC-1 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | . GW1_AACACGTCAGCGTCCA-1 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | . 3 rows × 65 columns . y . array([1, 1, 1, ..., 0, 0, 0]) . chi2_selector = SelectKBest(chi2,k=5) chi2_selector.fit_transform(X_cat,y) . array([[1, 0, 1, 0, 1], [1, 1, 1, 0, 1], [1, 0, 1, 0, 1], ..., [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0]], dtype=uint8) . ANOVA Reduction of Features . Now, for the continous-valued features, we can compute the ANOVA F-score between $X$ and $y$ and select the $k$-best similarly. Note that we could proceed with f_regression for continuous-valued features. . In any case, we use ANOVA, a generalization of the Student&#39;s $t$-test from $2$ to $n$ means. We compute this by providing features as a set of regressors to be tested in sequence, such that we compute the $F$ statistic where $F= frac{ sigma^2_X}{ sigma^2_y}$, the ratio of the sample variances of the features and the target. This is the same as a ratio of $ frac{ text{explained variance}}{ text{unexplained variance}}$. Under the null hypothesis, we expect the value of $F$ to have an $F$-distribution. In other words, we can find dependence of $y$ on $X$, and as such use those features going forward. . X_numer.head() . log2p1_RNAcount nFeature_RNA MT_fraction Viral Molecules Age . GW1_AAACGGGAGCTAGTCT-1 14.909096 | 5687 | 0.042059 | 0 | 18 | . GW1_AAAGTAGTCCTAGGGC-1 13.611947 | 3967 | 0.097771 | 0 | 18 | . GW1_AACACGTCAGCGTCCA-1 9.366322 | 513 | 0.036419 | 0 | 18 | . GW1_AACCATGAGAATCTCC-1 15.217731 | 6260 | 0.053720 | 0 | 18 | . GW1_AACCATGCATCCTTGC-1 9.134426 | 439 | 0.016043 | 0 | 18 | . from sklearn.feature_selection import f_classif fvalue_selector = SelectKBest(f_classif, k=2) fvalue_selector.fit_transform(X_numer,y) . array([[5687., 18.], [3967., 18.], [ 513., 18.], ..., [3477., 50.], [ 313., 50.], [ 537., 50.]]) . We&#39;ve selected nFeature_RNA and Age. . Eliminating Features with Recursion . A final method of feature selection can be accomplished with Recursive Feature Elimination and Cross Validation. RFECV works by iterating on a supervised task, discarding those features sequentially until performance on a given metric (such as accuracy) diminishes. This is perhaps most useful when a given model is decided upon early, but the number of features is not practical. This is also an excellent retrospective step of development, as we can then explain in retrospect those features that tend to inform outcomes or not (a big plus for explainability). . I will demonstrate with a very simple model: logistic regression. We are therefore fitting a hyperplane to the data and taking the sigmoid function of the dependent response variable&#39;s occurrence via the log-odds function (&quot;logit&quot;; $ mathcal{l} = log_b frac{p}{1-p}$) . from sklearn.feature_selection import RFECV from sklearn import linear_model from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X_concat,y, test_size=0.33, random_state=100) logit_clf = linear_model.LogisticRegression() logit_clf.fit(X_train,y_train) logit_clf.score(X_test,y_test) . 1.0 . OK, so I didn&#39;t expect the baseline to be 100% accurate. Nevertheless, we can prune features! One at a time, we will initiate logistic regression, scoring by $-MSE(X,y)$. . logit_clf = linear_model.LogisticRegression() rfecv = RFECV(estimator = logit_clf, step=1, scoring=&#39;neg_mean_squared_error&#39;) rfecv.fit(X_train,y_train) rfecv.transform(X_train) . array([[2.], [2.], [2.], ..., [3.], [3.], [1.]]) . rfecv.n_features_ . 1 . (rfecv.support_) . array([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]) . np.where(rfecv.support_ == True) . (array([70]),) . X_train.columns[70] . &#39;BMI&#39; . Wow! This is truly surprising. We can predict COVID symptom severity (remember, y = dataset.obs[&#39;Vasoactive agents required during hospitalization&#39;]) with BMI only! Trying without BMI.... . X_train_alt = X_train.drop(columns=[&#39;BMI&#39;]) logit_clf = linear_model.LogisticRegression() rfecv_alt = RFECV(estimator = logit_clf, step=1, scoring=&#39;neg_mean_squared_error&#39;) rfecv_alt.fit(X_train_alt,y_train) rfecv_alt.transform(X_train_alt) . array([[0.], [0.], [0.], ..., [1.], [1.], [0.]]) . X_train_alt.columns[np.where(rfecv_alt.support_ == True)] . Index([&#39;Donor Id_GWAS_1&#39;], dtype=&#39;object&#39;) . Fascinating. We can rely on this Genome-Wide Association Study marker. The only issue is that Donor_id is likely specific to the clinical outcome, and is probably not a great physiological feature. However, the recursive, cross-validated feature elimination worked as intended. Let&#39;s reorder the columns per the RFECV ranking... . rfecv.ranking_ . array([22, 62, 21, 61, 3, 13, 19, 14, 63, 46, 16, 11, 51, 45, 56, 58, 44, 50, 69, 17, 10, 2, 66, 9, 18, 8, 71, 55, 64, 65, 68, 43, 53, 60, 42, 48, 67, 12, 7, 49, 59, 57, 15, 20, 6, 52, 5, 54, 38, 36, 25, 32, 31, 33, 40, 29, 23, 37, 30, 27, 39, 35, 34, 47, 70, 28, 24, 26, 41, 4, 1]) . sorted(zip(X_train.columns,rfecv.ranking_)) . [(&#39;28-day death_No&#39;, 45), (&#39;28-day death_Yes&#39;, 56), (&#39;Admitted to hospital_No&#39;, 58), (&#39;Admitted to hospital_Yes&#39;, 44), (&#39;Age&#39;, 4), (&#39;BMI&#39;, 1), (&#39;CellType_Bcells 1&#39;, 38), (&#39;CellType_Bcells 2&#39;, 36), (&#39;CellType_Bcells 3&#39;, 25), (&#39;CellType_Ciliated epithelium&#39;, 30), (&#39;CellType_Erythrocytes&#39;, 39), (&#39;CellType_Low quality&#39;, 35), (&#39;CellType_Mononuclear phagocytes&#39;, 33), (&#39;CellType_Neutrophil&#39;, 27), (&#39;CellType_Secretary epithelium&#39;, 40), (&#39;CellType_Secretary epithelium KRT5&#39;, 29), (&#39;CellType_Squamous epithelium 1&#39;, 23), (&#39;CellType_Squamous epithelium 2&#39;, 37), (&#39;CellType_Tcells CD4&#39;, 32), (&#39;CellType_Tcells CD8&#39;, 31), (&#39;CellType_filtered cells and doublets&#39;, 34), (&#39;Donor Id_GWAS_1&#39;, 2), (&#39;Donor Id_GWAS_10&#39;, 66), (&#39;Donor Id_GWAS_11&#39;, 9), (&#39;Donor Id_GWAS_12&#39;, 18), (&#39;Donor Id_GWAS_13&#39;, 8), (&#39;Donor Id_GWAS_2&#39;, 71), (&#39;Donor Id_GWAS_3&#39;, 55), (&#39;Donor Id_GWAS_4&#39;, 64), (&#39;Donor Id_GWAS_5&#39;, 65), (&#39;Donor Id_GWAS_8&#39;, 68), (&#39;Ethnicity_Hispanic or Latino&#39;, 11), (&#39;Ethnicity_Not Hispanic or Latino&#39;, 16), (&#39;Ethnicity_Unknown or not documented&#39;, 51), (&#39;Highest level of respiratory support_Mechanical ventilation with intubation&#39;, 6), (&#39;Highest level of respiratory support_Non-invasive ventilation (CPAP, BiPAP, HFNC)&#39;, 52), (&#39;Highest level of respiratory support_None&#39;, 54), (&#39;Highest level of respiratory support_Supplemental O2&#39;, 5), (&#39;Lab number_CV19-1-S3.2A&#39;, 3), (&#39;Lab number_CV19-11-S3.2A&#39;, 13), (&#39;Lab number_CV19-12-S3.2A&#39;, 19), (&#39;Lab number_CV19-13-S3.2A&#39;, 14), (&#39;MT_fraction&#39;, 26), (&#39;Pre-existing Hypertension_No&#39;, 17), (&#39;Pre-existing Hypertension_Yes&#39;, 10), (&#39;Pre-existing diabetes_No&#39;, 50), (&#39;Pre-existing diabetes_Yes&#39;, 69), (&#39;Pre-existing heart disease_No&#39;, 48), (&#39;Pre-existing heart disease_Yes&#39;, 67), (&#39;Pre-existing immunocompromised condition_No&#39;, 63), (&#39;Pre-existing immunocompromised condition_Yes&#39;, 46), (&#39;Pre-existing kidney disease_No&#39;, 43), (&#39;Pre-existing kidney disease_Yes&#39;, 53), (&#39;Pre-existing lung disease_No&#39;, 12), (&#39;Pre-existing lung disease_Yes&#39;, 7), (&#39;Race_Asian&#39;, 21), (&#39;Race_Black&#39;, 62), (&#39;Race_Other&#39;, 61), (&#39;Race_White&#39;, 22), (&#39;SARS-CoV-2 PCR_Negative&#39;, 70), (&#39;SARS-CoV-2 PCR_Positive&#39;, 47), (&#39;Sex_F&#39;, 20), (&#39;Sex_M&#39;, 15), (&#39;Smoking_Current&#39;, 57), (&#39;Smoking_Never or unknown&#39;, 49), (&#39;Smoking_Prior&#39;, 59), (&#39;Symptomatic_No&#39;, 60), (&#39;Symptomatic_Yes&#39;, 42), (&#39;Viral Molecules&#39;, 41), (&#39;log2p1_RNAcount&#39;, 28), (&#39;nFeature_RNA&#39;, 24)] . Conclusion . This concludes discussion of feature-based analysis of everything but the model. In future posts, I will outline how you may efficiently perform model selection and optimization. For now, I hope this has helped in gaining a feel for performing the processing beyond reading in data but before jumping into true machine learning. This ties in nicely with discussions of far more advanced topics, such as regularization and modeling using deep learning. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/09/all-but-the-model.html",
            "relUrl": "/jupyter/2021/01/09/all-but-the-model.html",
            "date": " • Jan 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Probability for Computational Biologists",
            "content": "Introduction . This will be a comprehensive outline of topics in probability, including random variables, distributions, and rules of probability. My hope is that this will serve as a picture of (almost) everything you need may need to know regarding probability. . Topics such as statistical and deep learning leverage these concepts as assumed knowledge. It is therefore vital to have at least working knowledge of what is to come. . Counting . Multiplication Rule . Given an experiment with many potential outcomes, we can simply multiply the number of outcomes at each step for the overall number of possible outcomes. . $n_{total} = prod_i n_i$ . Sampling . Sampling $k$ items from a population of size $n$ results in the number of possibilities as follows: . Replacement (place each of the $k$ samples back into the population after choosing!) . Order-Preserving (we count each unique order in which we select the $k$ samples) $n^k$ | . | Order Doesn&#39;t Matter (we only care about the class membership of the $k samples) $ {n+k-1} choose{k} $ | . | . | No Replacement . Order-Preserving $ frac{n!}{(n-k)!}$ | A permutations problem. | . | Order Doesn&#39;t Matter $n choose k$ = $ frac{n!}{k!(n-k)!}$ | A combinations problem; use the binomial coefficient | . | . | . Na&#239;ve Probability . If all outcomes in a given event space are equally likely, $P_{naive}(X=x_i) = frac{ text{outcomes favorable to } x_i}{ text{number of outcomes}}$ . | This is intuitive. The probability of heads given a strictly fair coin (i.e., $X sim Bernoulli(p=.50)$) over two trials is $ frac{1}{2}$. . | . Conditional Probability . Independence . Independent Events . Two events $A,B$ are independent if the outcome of one event has no bearing on the other. In other words, knowing the outcome of $B$ gives no information about the potential outcome of $A$: . For independent events $A,B$, $P(A,B) = P(A)P(B)$ | $P(A|B) = P(A)$ | $P(B|A) = P(B)$ | . | . Conditional Independence . Two events $A,B$ are conditionally independent given another event outcome $C$ if: $$P(A,B|C) = P(A|C)P(B|C)$$ That is, we can tease apart the probability of a given event if they share a relevant background variable. . As an example, take the problem of three genetic mutations, $i,j$ and $k$. Let $i$ and $j$ be tightly correlated in a dataset and assume the probability distributions of each event is known. At first glance, we might think we could never model the two mutations independently. We may even assume $i$ causes $j$ in some fashion, or vice versa. . However, if we discovered that $k$ was a definitely a mutation appearing in an upstream promoter region impacting both sites corresponding to $i$ and $j$, we could then show conditional independence between $i$ and $j$ given the mutation $k$. Suddenly, our assumptions change and we may be more inclined to target $k$ as an event outcome worthy of attention. . Unions, Intersects, Complements . Set theory can be useful in the realm of probabilty. At the end of the day, we use probabilistic models to try and understand real events. This is the case with both discrete and continuous outcomes. . De Morgan&#39;s Laws . De Morgan&#39;s Laws offer versatility in logical reasoning, proofs, set theory, and other areas of intrigue. One thing to note is that generally, we frequently use $AND$ and $OR$ operators in probability. Note the following: . $ lnot(A lor B)= lnot A land lnot B $ | $ lnot(A land B) = lnot A lor lnot B $ | . Joint, Marginal, and Conditional Probability . Joint Probability . $P(A,B)$ . Note, $P(A,B) = P(A)P(B|A)$ | We can tease apart the distributions by conditioning on the portion of the event space occupied by $B$ where $A$ also has a bearing. . | Note also how $P(A,B)=P(B,A) = P(B)P(A|B)$. While consistency is important, this is just a matter of our choice of labels on the events. . | . | . Marginal (Unconditional / Prior) Probability . $P(A)$ | . Conditional Probability . $P(A|B) = frac{P(A,B)}{P(B)}$ | The Probability of A given B is equal to the Probability of A and B over the (prior) Probability of B . | Note: We can easily see how Bayes&#39; Rule follows. Given that we can &quot;flip&quot; the order of the joint probability expression, what is the right side equivalent to? . $P(A|B) = frac{P(A,B)}{P(B)} to P(A|B)P(B) = P(A,B) = P(B|A)P(A)$ | $ implies P(A,B) = frac{P(B|A)P(A)}{P(B)}$, or Bayes&#39; Rule! | . | . Conditional Probability is Probability . $P(A|B)$ is a probability function like any other for a fixed $B$. Any theorem applicable to probability is relevant for conditional probability. | . Chain Rule for Probability . Note we can disentangle a joint probability by use of the &quot;chain&quot; rule, an extension of operations on two-event probabilities. | $P(A,B,C) = P(A|B,C)P(B,C) = P(A|B,C)P(B|C)P(C)$. | This is exaclty the same as calling the joint event space $B,C = D$ and writing $P(A,D) = P(A|D)P(D)$. | . Law of Total Probability . For an event $A$ and disjoint sample partitions $B_1,...,B_n$, we can always marginalize out irrelevant event spaces. . $P(A) = P(A|B_1)P(B_1) + ... + P(A|B_n)P(P_n)$ | . Bayes&#39; Rule . Combining the definitions of conditional probility $P(A|B) = frac{P(A,B)}{P(B)}$ and joint probability $P(A,B) = P(A|B)P(B) = P(B|A)P(A)$, we can describe Bayes&#39; Rule: . $$P(A|B) = frac{P(B|A)P(A)}{P(B)}$$ . For 3 events $A,B,C$, we can write $$P(A|B,C) = frac{P(A,B,C)}{P(B,C)} = frac{P(B,C|A)P(A)}{P(B,C)}$$ . We can also use the chain rule to our liking: . $$P(A|B,C) = frac{P(A,B,C)}{P(B,C)} = frac{P(A|B,C)P(B|C)P(C)}{P(B|C)P(C)} = P(A|B,C)$$ . Note that it may be useful to commute these terms depending on the circumstance. . Random Variables and their Distributions . A random variable can take on a number of values according to a mathematical function. This may be thought of as the probability of a given outcome of an experiment in a global sense. . Probability Mass Functions (PMF) and Probability Distribution Functions (PDF) . Cumulative Distribution Functions (CDF) . Survival Functions . Independence of Random Variables . Expected Values and Indicators . Expected Values and Linearity . Expected Values . Mean, expectation, or first moment . Linearity . $ text{Distribution} implies text{Mean}$ . Conditional Expected Value . Indicator Random Variables . Indicator RVs . Distribution of an Indicator RV . Fundamental Bridge of an Indicator RV . Variance and Standard Deviation (w.r.t. Expectation) . Continuous Random Variables, Law of the Unconscious Statistician (LOTUS), and the Universaility of Uniform (UoU) . Continuous Random Variables . Probability of a CRV in a Given Interval . The Probability Density Function of a CRV . Expected Values of CRVs versus DRVs . Law of the Unconscious Statistician (LOTUS) . $g(RV_i) = RV_j$: The function of a random variable is itself a random variable. . I.e., one need only know the PMF/PDF of $X$ to find the PMF/PDF of $g(x)$. | . Universality of Uniform / Probability Integral Transform . Substitution of any $X_{cts}$ into its cumulative distribution function $F_X(x) = P(X leq x)$ yields $U(0,1)$. | Let $Y=F_X(X)$. Then, $F_Y(y) = P(Y leq y) = P(F_X(X) leq y) = P(X leq F^{-1}(y)) = F_X(F_X^{-1}(y)) = y$ for $Y sim U(0,1)$ and $X$ is some continous random variable with CDF $F_X$. . I.e. $F_X(X_{cts}) = int_{- infty}^{x}f(t)dt = int_{- infty}^{x}P(X leq t)dt = X sim U(0,1)$. | . | . Now, in Python. . Moments and Moment-Generating Functions . Moments . Moment-Generating Functions . Joint Probability Density Functions (PDFs) and Comulative Distribution Functions (CDFs) . Joint Distributions . Conditional Distributions . Bayes&#39; Rule for Discrete RVs . Bayes&#39; Rule for Continuous RVs . Marginal Distributions . Discrete Case: Marginal PMF from Joint PMF | Continous Case: Marginal PDF from Joint PDF | . Independence of Random Variables . Multivariate LOTUS . Further Topics . We will now take a look at topics that are more practical and/or obscure, such as relevant distributions and Markov Models . Covariance and Transformations . Covariance and Correlation . Covariance and Independence . Covariance and Variance . Properties of Covariance . Correlation: Location and Space-Invariant . Transformations . Single Variable . | Two Variables . | . Convolutions . Convolution Integral . Relevance to &quot;Convolutional&quot; Neural Networks . Poisson Processes . Law of Large Numbers . Central Limit Theorem . Markov Chains . Markov Property . States . Transition Matrix . Chain Properties . Stationary DIstributions . Some Continuous Distribitions . Normal . Exponential . Gamma . Beta . Chi-Square . Some Discrete Distribitions . Sampling: Varying Number of Trials and Replacement . Bernoulli . Binomial . Geometric . First-Success . Negative Binomial . Hypergeometric . Poisson . Some Multivariate Distributions . Multinomial . Multivariate Uniform . Multivariate Normal . A Note on Mixture Models . EM and Mixture Models . In Case You Missed It: Special Cases of Distributions . - . Important Inequalities . Cauchy-Shwarz . Markov . Chebyshev . Jensen . Background: Formulas . Geometric Series . Exponential Function . Gamma and Beta Integrals . Euler&#39;s Approximation for a Harmonic Sum . Stirling&#39;s Approximation for Factorials .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/05/probability.html",
            "relUrl": "/jupyter/2021/01/05/probability.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Magic in Numpy",
            "content": "Numpy Basics . We will now go over some basic approaches on a seemingly simple matrix for illustrative purposes. Hopefully some of the efficient and useful properties of Numpy become apparent. . A favorite work by a favorite artist, Dürer&#39;s Melencolia I (1514) includes sophisticated use of mathematical allegory, particularly in the top-right corner. It turns out Dürer actually makes many interesting points through his art, something you wouldn&#39;t expect from his messiah complex. . . . The matrix is thus: . import numpy as np X = np.array([[16, 3, 2, 13], [5, 10, 11, 8], [9, 6, 7, 12], [4, 15, 14, 1]]) print(X) . [[16 3 2 13] [ 5 10 11 8] [ 9 6 7 12] [ 4 15 14 1]] . type(X) . numpy.ndarray . Magic Squares . This matrix is purported to be a magic square. We must fit the following constraints: . 1) Magic 2) Square . Simple enough. Starting with the second condition, Numpy provides a number of methods. Though magic cubes and tesseracts are possible, we can begin with a square. Here&#39;s a simple function to detect if an array is square. . def is_square(M: np.ndarray) -&gt; bool: &#39;&#39;&#39; Arguments: M, a 2-d matrix Returns: a boolean, True if square &#39;&#39;&#39; assert M.ndim == 2 return True if M.shape[0] == M.shape[1] else False is_square(X) . True . Vectorized Summation: Magic . Now, the more involved condition. If a square is &quot;magic&quot;, the array exhibits the following properties1: . i) Each of the $n$ elements are of the set of distinct positive integers $[1,2,3,...,n^2]$, such that $n$ is the &quot;order&quot; of the square. Dürer thus presents a $4^{th}$ order magic square. . ii) The sum of the $n$ numbers about any horizontal, vertical or main diagnonal is the same number – the &quot;magic&quot; constant. It is known that such magic constants can be given by $ mathcal{M}(X_n) = frac{1}{n} sum_{k=1}^{n^2}k = frac{1}{2}n(n^2+1)$ . . Aside: iii) The complement to a magic square is derived from subtracting every number in a given magic square by $n^2 + 1$. . Back to Dürer. We can check each condition as follows. . . there are others, see https://faculty.etsu.edu/stephen/matrix-magic-squares.pdf)&#8617; . | def is_magic(M, verbose = True)-&gt;bool: #By constraints i) &amp; ii) assert M.shape[0] == M.shape[1], &#39;Not a square.&#39; n = M.shape[0] assert np.array_equal(np.sort(M.flatten()), np.arange(n**2) + 1), &#39;Expected elements from [1,2,...,n^2]&#39; column_sums = np.sum(M,axis=0) #Note that summing across axis 0 actually returns column-wise sums, and vice-versa. row_sums = np.sum(M, axis=1) diagonal_sums = np.array([np.trace(M),np.trace(np.fliplr(M))]).astype(int) magic_num_sum = np.unique(np.concatenate( (column_sums,row_sums,diagonal_sums) )) if len(magic_num_sum) == 1: if verbose: print(f&#39;Magic number is {magic_num_sum} with order {n}.&#39;) return True . np.fliplr(X).diagonal().sum() == np.flipud(X).diagonal().sum() . True . np.trace(X) == np.diagonal(X).sum() . True . is_magic(X) . Magic number is [34] with order 4. . True . X . array([[16, 3, 2, 13], [ 5, 10, 11, 8], [ 9, 6, 7, 12], [ 4, 15, 14, 1]]) . Fast Indexing: Gnomon Magic . Dürer&#39;s square is actually a Gnomonic Magic Square – that is, each non-overlapping root subsquare bordering the four sides of the square ($2 times 2$ subsquare), as well as the central subsquare, sums to the magic constant of the overarching square. . The Gnomon is the portion of the sundial casting a shadow. In a way we also cast a magic projection on subarrays of our main magic square. . We can verify this easily – in Numpy, arrays can be efficiently split with minimal logic, rather than looping over each element and hard-indexing. . a,b,c,d = [quadrant for sub_x in np.split(X,2, axis = 0) for quadrant in np.split(sub_x,2, axis = 1)] n = X.shape[0] n_subsquare = np.sqrt(n).astype(int) start = n//2 - (n_subsquare // 2) end = n//2 + (n_subsquare // 2) e = X[start:end,start:end] sections = [a, b, c, d, e] sections . [array([[16, 3], [ 5, 10]]), array([[ 2, 13], [11, 8]]), array([[ 9, 6], [ 4, 15]]), array([[ 7, 12], [14, 1]]), array([[10, 11], [ 6, 7]])] . print(set([sum(s.flatten()) for s in sections])) . {34} . All quadrants sum to the magic number of 34. As such, we have verified the deliberate style of Dürer. . Linear Algebra . We will now move onto some essential linear algebra operations on the magic square. . X . array([[16, 3, 2, 13], [ 5, 10, 11, 8], [ 9, 6, 7, 12], [ 4, 15, 14, 1]]) . Rank . The rank of a matrix is the number of its linearly independent columns. That is, the dimensionality of the vector space spanned by a matrix&#39;s columns (or rows) is given by its rank, such that the span is the smallest linear subspace containing a set of vectors describing the matrix. . We can obtain the span of all linear combinations of some vectors $ vec{u}, vec{v}$ by computing $s vec{u} + t vec{v}$ for some scalar constants $s,t$. The dimensionality of the span of the row or column vectors of a matrix thus yields the row or column rank. . We will proceed using Numpy, which proceeds using singular value decompositon (SVD): . rank = np.linalg.matrix_rank(X) rank . 3 . Thus we have a rank-deficient matrix, since $3 &lt; 4$. 4 is the column-dimensionality of the Magic Square matrix but the columns only span 3 dimensions. Note that $rank(M) leq min (m,n)$ for an $m times n$ matrix $M$. . Note how Numpy uses the property that the rank is equal to the number of nonzero singular values as follows: . u,s,vh = np.linalg.svd(X) s . array([3.40000000e+01, 1.78885438e+01, 4.47213595e+00, 6.25921042e-16]) . We have 4 nonzero singular values, but the final value is extremely small. Numpy therefore considers this zero as the default tolerance is computed as M.max()*max(M.shape)*eps. . eps = np.finfo(float).eps tol = X.max()*max(X.shape)*eps tol . 1.4210854715202004e-14 . rank == len(s[s&gt;tol]) . True . rank . 3 . Determinant . The determinant is a useful encoding of the linear transformation described by a particular $n times n$ matrix. In geometric terms, it is analagous to the volume scaling factor of the linear transformation described by the matrix. . In other words, this is the volume of the parallelepiped (a rhomboid prism; a cube is to a square as a parallelepiped is to a parallelogram) spanned by the vectors (row or column) of a matrix. The sign of the determinant of a matrix denotes whether or not the orientation of a vector space is preserved by the transformation described by the matrix. . Two simple examples, then Dürer&#39;s: . A = np.array([[0,-1],[1,0]]) B = np.array([[-2,0],[0,2]]) . $A$ describes a 90-degree counterclockwise (↪️) rotation. . $B$ describes a scaling by a factor of $2$ as well as a reflection about the $y$ axis. . print(np.linalg.det(A), np.linalg.det(B), sep=&#39; n&#39;) . 1.0 -4.0 . A simple rotation does not change &quot;volume&quot; nor orientation. A scaling on $x,y$ and a reflection about the $y$ axis is encoded in the determinant, however: the &quot;volume&quot; is changed in total by a factor of $4$ and the sign is negative, indicating a change in the orientation of previous vector space. . These are simple enough to compute by hand, but even a $4 times 4$ dimensional matrix as provided by Dürer is more onerous. Thankfully, Numpy works well: . X = np.array([[16, 3, 2, 13], [5, 10, 11, 8], [9, 6, 7, 12], [4, 15, 14, 1]]) np.linalg.det(X) . 1.449507180950607e-12 . An interesting observation: Dürer does not provide a pandiagonal magic square, as the determinant of this order-4 magic square is near to, but $not$, zero. In other words, if the broken diagonals (for instance, $16,11,12,15$, or $3,8,7,4$) summed to the magic number, the determinant would be zero [^1]. . Eigenvectors and Eigenvalues . We solve the characteristic equation of a matrix $M$ to find eigenvalues $ vec{ lambda}$. That is, we solve $|M - lambda I| = 0$ where $I$ is the identity matrix ($I_{ij} = 1 s.t i=j, 0 s.t. i not = j$) of identical dimensionality to $M$. . In the case of Dürer&#39;s magic square, we simply subtract a value $ lambda$ from each element on the main diagonal and set the resulting matrix&#39;s determinant equal to zero. . We can quickly avoid rote work using Numpy: . eigenvals, eigenvects = np.linalg.eig(X) eigenvects . array([[ 5.00000000e-01, 8.16496581e-01, 2.23606798e-01, -4.08248290e-01], [ 5.00000000e-01, -4.08248290e-01, -6.70820393e-01, 0.00000000e+00], [ 5.00000000e-01, -1.76752662e-16, 6.70820393e-01, -4.08248290e-01], [ 5.00000000e-01, -4.08248290e-01, -2.23606798e-01, 8.16496581e-01]]) . eigenvals . array([ 3.40000000e+01, 8.00000000e+00, 4.84818517e-17, -8.00000000e+00]) . Note the interesting property of magic squares: the principal (largest) eigenvalue of a magic square composed of positive elements is its magic constant! Of further note, but not applicable here, is the observation that if a magic square has some negative elements, then its magic constant is one of its eigenvalues.[^1] . To show the first point, consider that $[1,1,...,1]^T$ is an eigenvector of a matrix $M$ if every row sums to the same value $k$. This can be shown by computing $|M- lambda I| = 0$ for a matrix $M$ where all rows sum to the same constant $k$. Substituting values and simplifying, $k$ is an eigenvalue. The same holds for columns that sum to the same constant. . Now, we want to show that the entries in the vector $Mv$ are equal to $kv$, where $k$ is both the magic number and an eigenvalue, and $v$ is an eigenvector of $M$. Recall that the sum of all elements in an $n times n$ magic square $M^*$ is, by construction, equal to $ frac{n(n^2+1)}{2}$. Thus, since a magic square $M^*$ indeed does have each row sum to $k$, we have that $Av = kv$ for $[1,1,...,1]^T$. . This gives us another way to find the magic number of a magic square. . def get_magic_number(M): if is_magic(M, verbose= False) and is_square(M): return np.linalg.eig(X)[0].round(1).astype(int)[0] get_magic_number(X) . 34 . Conclusion . This concludes discussion (for now) of magic squares, essential Numpy, and some linear-algebraic approaches to simple matrices. From here, I hope to move to much more complex topics involving far more abstract data types and approaches to manipulation. Nonetheless, foundations will always be important and most likely present under the hood. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/04/linear-algebra.html",
            "relUrl": "/jupyter/2021/01/04/linear-algebra.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "(simple) Autoencoders ≡ PCA",
            "content": "Background . Shocker: The simplest autoencoder is actually the same as PCA. We will also see how VAEs perform expectation maximization. Autoencoders learn a meaningful representation of a signal. Using an encoder/decoder pair, autoencoders work by reconstructing a latent encoded representation of an original signal. By minimizing the loss between the original signal and the decoded latent representation, an encoder network can be trained to parse an instance of a dataset for its most meaningful features. . Sound familiar? That is precisely the goal of computing the principal components analysis (PCA) of a matrix. It turns out that autoencoders, by construction are exactly PCA. . The Simplest Autoencoder . Let a neural network be defined with a single hidden unit $W^T sigma(f(W vec{X}))$, with a linear activation function $ sigma$ (for this example). Let the weights on the encoder layer be denoted $W$. . We thus define a decoder to use weights $W^T$, and final outputs of the network should converge to a reconstruction of the original features $ vec{X}$ once properly trained. That is, our network should produce $ hat{ vec{X}}$ from $ vec{X} to vec{y} = WX to z = sigma(f(y))$ and finally $ hat{ vec{X}} = W^Tz$. . If we train by minimizing the $L_2$ divergence between $( vec{X}, hat{ vec{X}})$, we have an autoencoder, but we also learn the principle components of $ vec{X}$: . $$ hat{x} = w^Twx; div( hat{x},x) = |x- hat{x} |^2 = |x - w^TWx |^2 $$ $$ to via backprop to hat{w} = arg min_w E left[ | x-w^TWx |^2 right]$$ . This is equivalent to discovering a basis that spans the principle components of the input, as we discover the directions of maximum energy, equal to the variance of $X$ if $X$ is a zero-mean random variable. In other words, we find a linear mapping of the original features to the principle axis of the data. . Finally, if the learned decoder weight is not the same weight as the input weight (i.e., $U^T not = W^T$), we still learn a hidden representation $z$ that lies along the major axis of the input $X$. The minimum error direction here is by definition the pinciple eigen vector of $X$. . We could then find a useful component of $X$ (described perhaps by our training process or assumptions)by then projecting the eigen vector onto $X$. Again, if $U^T = W^T$, we arrive at the principle component(s) of $X$. . Of course, this is a roundabout method of obtaining principle components, but I hope it shows the rigorous grounding and versatility of perceptrons in learning representations of data. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2021/01/03/autoencoders.html",
            "relUrl": "/jupyter/2021/01/03/autoencoders.html",
            "date": " • Jan 3, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Tensors via Bioimaging",
            "content": "Background . 1.3 billion humans are able to use Microsoft Excel. Microsoft Excel is the modern clay tablet, an intuitive but ultimately limited instrument for the computational professional. After all, why should we be limited to two dimensions? Why should we pay for the privelege? Often, data is best represented as $n$-dimensional. . For instance, let&#39;s say you&#39;d like become a billionaire without too much effort. One way would be to totally automate clinical bioimage analysis at human-level fidelity using machine learning. . . A Diabetic retinopathy slide (https://www.kaggle.com/c/diabetic-retinopathy-detection). The condition is estimated to effect over 93 million people. . Whereas a highly skilled human could potentially spot abnormalities in the above retinopathy slide, a machine can do it better and much faster. Partially, this is because a machine views the below image somewhat naïvely as 150,528 dots, as we have a square RGB image with with 224 pixels per dimension. . Viewing the image not as an image but as a tensor (vector-valued matrices), or an $n$-dimensional generalization of a matrix, we can then move onto analyzing the image: segmentation, feature learning, classification (if labels are detected), and other interesting tasks that a human may or may not be able to do. . Numpy . We use Numpy for efficient vectorized tensor manipulation with the convenient abstraction of the Python programming language. Numpy fluency will carry a computational professional very far, and it will only begin to show limitations when deep learning and very large datasets are involved (though the syntax of major deep learning packages are very close to Numpy). . We can easily load the above png &quot;image&quot; into a numpy array using a number of packages. imageio is used below. . import imageio import numpy as np my_image = imageio.imread(&#39;10009_right.png&#39;) my_image . Array([[[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], ..., [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0], ..., [0, 0, 0], [0, 0, 0], [0, 0, 0]]], dtype=uint8) . Above we see the image loaded into memory as an integer-valued 3-dimensional matrix. Basic Numpy / Python fluency is assumed (slicing, etc.). . my_image.shape #note memory persistence in Jupyter. . (224, 224, 3) . my_image.size . 150528 . my_image.ndim . 3 . The $224 times 224 times 3$, $150,528$-element, $3$-dimensional array can be now be subject to a multitude of useful manipulations. . For instance, we can sparsify the matrix using scipy.sparse for 2-d matrices and sparse (!pip install sparse) for $n$-dimensional arrays (tensors). This may be useful for quick compression or storage of many such images: note the large number of zero values corresponding to &quot;black&quot; portions of the image. . import sparse sparse.COO(my_image) . Formatcoo | . Data Typeuint8 | . Shape(224, 224, 3) | . nnz120885 | . Density0.8030731823979592 | . Read-onlyTrue | . Size2.9M | . Storage ratio20.1 | . The sparse matrix in general is less memory intensive. . Another example of manipulation: we can quickly invert and flip the image with numerical rigor. We will use matplotlib to display the numpy array. . np.invert is np.bitwise_not . True . import matplotlib.pyplot as plt plt.imshow(my_image) plt.axis(&#39;off&#39;) plt.title(&#39;All Axes&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; plt.imshow(np.invert(my_image)) plt.axis(&#39;off&#39;) plt.title(&#39;Inverted (Bitwise NOT; twos-complement)&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Said another way... . 255 - my_image[100,100,:] == np.invert(my_image)[100,100,:] #we are in 256-bit color. . Array([ True, True, True]) . plt.imshow(np.fliplr(my_image)) plt.axis(&#39;off&#39;) plt.title(&#39;Flipped (LR)&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; This leads to some intuitive operations. For instance, if we were looking to feature-engineer such image data for use in a statistical learning classifier, we may hypothesize the location of the fovea (bright central spot) as a useful aspect of the retinopathy image. . I&#39;m betting we can segment these features with a single line of numpy. For this and subsequent examples, let&#39;s take a sample of the first sheet (zeroth index) of the image for simplicity such that we have a 2-d array (pretend we read-in a greyscale image). . x = my_image[:,:,0] #take all elements of the first sheet/leaf of the array. plt.imshow(x) plt.axis(&#39;off&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Now for the single line. We will find the bright portion by the assumption that we can find it by taking $region = frac{ sum_i x_i}{N} * 2 * sigma( vec{x})$, or all pixels with intensity greater than equal to two standard deviations above the mean. . x = my_image[:,:,0] #take all elements of the first sheet/leaf of the array. mask = (x &gt;= x.mean() + 2*x.std()) . plt.imshow(mask) plt.axis(&#39;off&#39;) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; We can then get the coordinates in the array simply: . coords = np.argwhere(x == mask) coords . array([[ 0, 0], [ 0, 1], [ 0, 2], ..., [223, 221], [223, 222], [223, 223]]) . print(coords.size) . 20656 . If we knew a priori that a particular type of retinopathy was characterized by abnormal foveal locations, and we had a sufficient train/test/validation dataset, we could reduce the size of our dataset significantly with such an engineered feature. . print(f&#39;If we only require foveal coordinates, our dataset may be reduced by {round(100-(coords.size/my_image.size)*100,2)}% !&#39;) . If we only require foveal coordinates, our dataset may be reduced by 86.28% ! . In Closing... . I hope this example was helpful in showing how we can quickly prepare complex data types for computational purposes. I will return to abstract bioimaging processing in future posts. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/tensors.html",
            "relUrl": "/jupyter/2020/12/31/tensors.html",
            "date": " • Dec 31, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "It's better to be evil than boring",
            "content": "Background . Healthcare is, apart from university tuition, one of the more rapidly inflating services on the market. Inconvenient as it might be, much of the reason behind the crushing costs of paying for healthcare has little to do with what you might expect. The price of the cancer drug crizotinib was actually developed on a shoestring, judging by the name. And yeah, it stings to think about how our copayments trickle their way into the Mercedes-AMG option sheet between some surgeon&#39;s sweaty thumbs, but there aren&#39;t that many surgeons. In reality, a large reason behind cost inflation in healthcare has nothing to do with the evil stuff. It has to do with the boring stuff. . In this post, I will look at a major chapter of the administrative nightmare of healthcare: manual medical coding. We will quickly see how a large cost center in administration, medical coding, will likely be totally automated in the near future. My hope is that you will be left wondering how this is still possibly done largely by hand, and perhaps more optimistic about the future of healthcare. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/boring.html",
            "relUrl": "/jupyter/2020/12/31/boring.html",
            "date": " • Dec 31, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "(simple) Autoencoders ≡ PCA",
            "content": "Background . Shocker: The simplest autoencoder is actually the same as PCA. We will also see how VAEs perform expectation maximization. Autoencoders learn a meaningful representation of a signal. Using an encoder/decoder pair, autoencoders work by reconstructing a latent encoded representation of an original signal. By minimizing the loss between the original signal and the decoded latent representation, an encoder network can be trained to parse an instance of a dataset for its most meaningful features. . Sound familiar? That is precisely the goal of computing the principal components analysis (PCA) of a matrix. It turns out that autoencoders, by construction are exactly PCA. . The Simplest Autoencoder . Let a neural network be defined with a single hidden unit $W^T sigma(f(W vec{X}))$, with a linear activation function $ sigma$ (for this example). Let the weights on the encoder layer be denoted $W$. . We thus define a decoder to use weights $W^T$, and final outputs of the network should converge to a reconstruction of the original features $ vec{X}$ once properly trained. That is, our network should produce $ hat{ vec{X}}$ from $ vec{X} to vec{y} = WX to z = sigma(f(y))$ and finally $ hat{ vec{X}} = W^Tz$. . If we train by minimizing the $L_2$ divergence between $( vec{X}, hat{ vec{X}})$, we have an autoencoder, but we also learn the principle components of $ vec{X}$: . $$ hat{x} = w^Twx; div( hat{x},x) = |x- hat{x} |^2 = |x - w^TWx |^2 $$ $$ to via backprop to hat{w} = arg min_w E left[ | x-w^TWx |^2 right]$$ . This is equivalent to discovering a basis that spans the principle components of the input, as we discover the directions of maximum energy, equal to the variance of $X$ if $X$ is a zero-mean random variable. In other words, we find a linear mapping of the original features to the principle axis of the data. . Finally, if the learned decoder weight is not the same weight as the input weight (i.e., $U^T not = W^T$), we still learn a hidden representation $z$ that lies along the major axis of the input $X$. The minimum error direction here is by definition the principle eigen vector of $X$. . We could then find a useful component of $X$ (described perhaps by our training process or assumptions)by then projecting the eigen vector onto $X$. Again, if $U^T = W^T$, we arrive at the principle component(s) of $X$. . Of course, this is a roundabout method of obtaining principle components, but I hope it shows the rigorous grounding and versatility of perceptrons in learning representations of data. .",
            "url": "https://simonlevine.github.io/simonsays/jupyter/2020/12/31/autoencoders.html",
            "relUrl": "/jupyter/2020/12/31/autoencoders.html",
            "date": " • Dec 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Originally from a small town in northern California, I completed my B.A. in biological sciences with a minor in mathematical finance at the University of Southern California. . I then spent about a year in consulting and briefly worked at a winery. I also devoted a considerable amount of time and effort on the law school admissions test (LSAT) but realized the profession was not for me. Instead, I chose to return toward technical topics and decided to pursue the master’s program in computational biology at Carnegie Mellon University. . Since then, I’ve worked toward mastering an understanding of statistical and deep learning methods as they relate to the biosciences. Some of my favorite coursework includes 02-710: Computational Genomics, and the infamous 11-785: Deep Learning. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://simonlevine.github.io/simonsays/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://simonlevine.github.io/simonsays/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}